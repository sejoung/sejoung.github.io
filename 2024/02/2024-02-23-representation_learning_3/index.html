<!DOCTYPE html><html lang="lang"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Representation Learning (Self-Supervised Learning)"><meta name="keywords" content="Representation Learning"><meta name="author" content="sejoung"><meta name="copyright" content="sejoung"><title>Representation Learning (Self-Supervised Learning) | 폭간의 기술블로그</title><link rel="shortcut icon" href="../../../my-favicon.ico"><link rel="stylesheet" href="../../../css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><script async src="https://www.googletagmanager.com/gtag/js?id=G-NVRTGLD8RZ"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-NVRTGLD8RZ');</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 5.4.2"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Representation-Learning-Self-Supervised-Learning"><span class="toc-number">1.</span> <span class="toc-text">Representation Learning (Self-Supervised Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pretext-tasks-%EA%B8%B0%EB%B0%98-self-supervised-learning"><span class="toc-number">1.1.</span> <span class="toc-text">Pretext tasks 기반 self-supervised learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Contrastive-Learning"><span class="toc-number">1.2.</span> <span class="toc-text">Contrastive Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Semi-Supervised-Learning-%EC%9D%B4%EB%9E%80"><span class="toc-number">1.3.</span> <span class="toc-text">Semi-Supervised Learning 이란</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%EC%B0%B8%EC%A1%B0"><span class="toc-number">2.</span> <span class="toc-text">참조</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://avatars0.githubusercontent.com/u/4936005?s=400&amp;u=a679b941fe377418e7e4efcf916c6a636d7178ee&amp;v=4"></div><div class="author-info__name text-center">sejoung</div><div class="author-info__description text-center">잘정리하자</div><div class="follow-button"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/sejoung">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="../../../archives"><span class="pull-left">Articles</span><span class="pull-right">793</span></a><a class="author-info-articles__tags article-meta" href="../../../tags"><span class="pull-left">Tags</span><span class="pull-right">824</span></a><a class="author-info-articles__categories article-meta" href="../../../categories"><span class="pull-left">Categories</span><span class="pull-right">75</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="external nofollow noopener noreferrer" href="https://blog.naver.com/sanaes">naverblog</a><a class="author-info-links__name text-center" target="_blank" rel="external nofollow noopener noreferrer" href="https://www.linkedin.com/in/sanaes/">linkedin</a><a class="author-info-links__name text-center" target="_blank" rel="external nofollow noopener noreferrer" href="https://www.slideshare.net/sejoung">slideshare</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Van_Gogh_-_Terrasse_des_Caf%C3%A9s_an_der_Place_du_Forum_in_Arles_am_Abend1.jpeg/1024px-Van_Gogh_-_Terrasse_des_Caf%C3%A9s_an_der_Place_du_Forum_in_Arles_am_Abend1.jpeg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="../../../index.html">폭간의 기술블로그</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">Representation Learning (Self-Supervised Learning)</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2024-02-23</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="../../../categories/Machine-Learning/">Machine Learning</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2024/02/2024-02-23-representation_learning_3/"></span></a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Representation-Learning-Self-Supervised-Learning"><a href="#Representation-Learning-Self-Supervised-Learning" class="headerlink" title="Representation Learning (Self-Supervised Learning)"></a>Representation Learning (Self-Supervised Learning)</h1><ul>
<li><p>학습 기법의 label에 따른 분류</p>
<ul>
<li>딥러닝 모델 학습을 위해서는 많은 레이블이 필요하다.</li>
<li>하지만, 비싼 레이블링 없이도 모델을 잘 학습시키는 것은 중요하다.</li>
</ul>
</li>
<li><p>Supervised learning의 단점</p>
<ul>
<li>“레이블” 데이터가 많아야 함</li>
<li>학습되지 않은 데이터가 튀어나오면 예측하기 어려움. (transfer가 잘 안됨)</li>
</ul>
</li>
<li><p>Reinforcement learning의 단점</p>
<ul>
<li>실패해도 다시 시도하면 되는 게임에선 가능</li>
<li>현실 세계에서는 실패 자체가 치명적일 수 있어서 적용하기 힘듬</li>
<li>Policy gradients로 학습하는 과정은 불안정함</li>
</ul>
</li>
<li><p>Self-supervised learning의 필요성</p>
<ul>
<li>주변 상황과 조건을 고려해 예측해야 함 (context prediction)</li>
<li>실패하기 전에 사고가 난다는 것을 예측해야 함</li>
<li>레이블 없는 많은 데이터를 안정적으로 학습해야함</li>
<li>모델이 semantics 를 파악하는 데, 굳이 classification 이라는 task를 줄 필요가 없음</li>
</ul>
</li>
</ul>
<h2 id="Pretext-tasks-기반-self-supervised-learning"><a href="#Pretext-tasks-기반-self-supervised-learning" class="headerlink" title="Pretext tasks 기반 self-supervised learning"></a>Pretext tasks 기반 self-supervised learning</h2><ul>
<li>Exemplar<ul>
<li>STL-10 데이터셋 사용</li>
<li>96x96 이미지 내에서 considerable 한 gradient 영역 근처를 32x32 로 crop</li>
<li>32x32 seed patch를 기준으로 data augmentation을 적용하여 추가 영상 생성</li>
<li>Seed image를 data augmentation 하여 이 데이터를 seed image로 prediction하도록 학습.</li>
<li>surrogate training classes &#x2F; training samples가 많아질 수록 성능이 향상한다</li>
<li>Surrogate labels 을 data augmentation을 통해 얻어서 네트워크를 학습하면 transfer했을 때 성능이 좋다.</li>
<li>Discrimina&#x2F;ve 한 features로서 image matching에 적용했을 때 SIFT보다 좋은 descriptors로서의 결과도 낸다. (marginal 하지만)</li>
<li>ImageNet과 같은 large-scale datasets에는 적용하기 어려움.</li>
</ul>
</li>
<li>Context Prediction<ul>
<li>Context predicXon 이라는 pretext task를 통해 exemplar의 단점을 해결하려 함.</li>
<li>3x3 개의 patches를 뜯어서 가운데를 기준으로 1~8 번 할당</li>
<li>가운데를 기준으로 선택한 patch가 몇 번째 패치인지 예측하도록 모델이 학습됨.</li>
<li>사람도 다소 예측하기 어려움. 하지만, 이를 machine에게 학습시키면, 이미지 전반의 representation을 배울 수 있다는 게 모티베이션.</li>
<li>이미지 패치나 위치는 일정하지 않고 약간의 위치 변화를 주어 샘플링 된다.</li>
<li>그럼에도, context prediction tasks로 pre-training 한 feature extractor를 사용하면 성능이 좋다.</li>
</ul>
</li>
<li>Jigsaw Puzzles<ul>
<li>직쏘 퍼즐을 풀게하는 pretext task</li>
<li>앞선 context prediction과 3x3 패치를 추출하지만, 임의의 permutation으로 셔플함.</li>
<li>하지만, 직쏘 퍼즐은 9!&#x3D;362,880 개(36만개)의 클래스를 배워야해서 이는 어렵다.</li>
<li>Permutation 수에 따른 결과: permutation의 수가 많아질수록 permutation간의 차이가 클수록(구분하기 쉬워질 수록) transfer 성능이 좋아진다.</li>
<li>직쏘 퍼즐은 좋은 pretext task이다.</li>
<li>이와 같은 context-free network (CFN)은 인간이 해내기 어려운 task를 학습함으 로써 더 좋은 deep learning 모델을 학습 하는 데에 성공하였다.</li>
<li>또한 label없이 학습하는 self-supervised learning task를 정의하여 human annotation의 비용을 줄였다.</li>
</ul>
</li>
<li>Count<ul>
<li>한 patch에 대한 object의 특징을 가장의 vector로 표현함.</li>
<li>ex.각 패치안에 코2개,눈4개, 머리 1개 등등..</li>
<li>각 패치의 특징 벡터의 합을 prediction하는 것은 원래 이미지의 특징 벡터의 합과 같다는 이론에서 intuition을 얻음.</li>
<li>AlexNet을 사용하여 패치별 특 징벡터를 출력하게 함.</li>
<li>원본 이미지를 downsampling 했을때 얻는 특징 벡터와,각패치 별로 넣었을때 나오는 특징 벡터의 합이 같도록 학습함</li>
<li>하지만,같은 이미지로만 학습하면,모든 feature vector를 0으로만 예측하는 trivial solution이 생김</li>
<li>Counting task는 representation learning 으로 좋은 task이다.</li>
<li>Feature vector를 잘 학습해서 오른쪽 사진과 같이 retrieval 에서도 효과적으로 embedding space를 만든다.</li>
<li>Visual primitives의 조합을 잘 만드는 것은 중요하다.</li>
</ul>
</li>
<li>Multi-Task<ul>
<li>2017년 당시에 주요 쓰였던 self-supervised learning 방법들을 동시에 multi-task learning으로 학습시키는 방법 제안</li>
<li>하나의 네트워크로, relative patch location (Context prediction) + colorization + exemplar + motion segmentation 의 pretext tasks를 동시에 수행함.</li>
<li>각 결과를 서로 다른 GPU머신을 학습해서 gradient를 축적한 다음에 한번에 네트워크 업데이트를 한다.</li>
<li>Self-supv pre-training이 fully-supervised pre-training을 넘을 수 있는 가능성 제시.</li>
</ul>
</li>
<li>Rotation Prediction (RotNet)<ul>
<li>이미지의 upright 4방향 rotation을 prediction 하는 pretext task</li>
<li>회전 이미지가 원본에서 몇도 회전했는 지를 예측하게 함.</li>
</ul>
</li>
<li>Rotation Prediction (Decouple-RotNet)<ul>
<li>Pretext task는 RotNet과 같음.</li>
<li>Rotation 은 ambiguous 할 수 있지 않나?</li>
<li>그러면 그 example에 대해서는 학습이 매우 어렵고 무의미함.</li>
<li>Rotation-agnostic example을 제거하자.</li>
</ul>
</li>
</ul>
<h2 id="Contrastive-Learning"><a href="#Contrastive-Learning" class="headerlink" title="Contrastive Learning"></a>Contrastive Learning</h2><ul>
<li>Non-Parametric Instance Discrimination<ul>
<li>Non-parametric Softmax classifier<ul>
<li>고정된 w가 아닌 feature vector v로 대체하였다. (instance 특성학습 위해)</li>
<li>L2 norm을 통해 v를 unit sphere에 고정</li>
<li>모든 instance가 class에 상관없이 unit sphere에 골고루 퍼져서 분포하도록 학습</li>
</ul>
</li>
<li>Noise-contrastive estimation (NCE)<ul>
<li>NCE form의 contrative loss 추가.</li>
</ul>
</li>
<li>Proximal regularization<ul>
<li>각 instance를 개별 클래스로 두고 학습하면 학습이 불안정해서, proximal regularization 추가함.</li>
</ul>
</li>
<li>Weighted k-NN classifier<ul>
<li>Test time 때 들어오는 sample은 k-NN features를 찾아서 classifying 한다.</li>
</ul>
</li>
<li>결론<ul>
<li>instance-level discrimination 을 통해 unsupervised feature learning을 하는 것은 피처 학습에 효과적이다.</li>
<li>이때 non-parametric soamax formulation이 오히려 학습에 도움이 된다.</li>
<li>Image classification (ImageNet, Places) 결과</li>
<li>Semi-supervised learning, object detection으로의 일반화</li>
</ul>
</li>
</ul>
</li>
<li>MoCo: Momentum Contrast<ul>
<li>Contrastive learning을 unsupervised representa&#x2F;on learning 에 좀 더 효과적으로 적용시켜 보자.</li>
<li>Pretext task: Positive &#x2F; negative pairing을 augmentation 을 통해 더 많이 만들기.</li>
<li>Encoder가 key representation의 consistency를 해쳐서 학습이 불안정해지는 것을 막기위해 momentum contrast 도입.</li>
<li>결론<ul>
<li>Self-supervised contrastive learning 으로 model pre-training 하면 성능이 좋다.</li>
<li>Pretext task로 instance discrimination task 는 다른 task로 바뀔 수 있다. (e.g., masked auto-encoding 등)</li>
<li>Backbone network도 CNN에서 바뀔 수 있다.</li>
</ul>
</li>
</ul>
</li>
<li>SimCLR <ul>
<li>MoCo와 비슷한 시기에 나온 Contrastive learning 기법.</li>
<li>성능이 좀 더 향상되었다.</li>
<li>Simple framework로도 contrastive visual representation learning 의 좋은 결과를 낸다.</li>
</ul>
</li>
<li>MoCo Version 2<ul>
<li>SimCLR의 design improvements를 MoCo에도 적용하여 성능을 향상시키겠다.</li>
</ul>
</li>
<li>SimCLR Version 2<ul>
<li>Semi-supervised learning 쪽 알고리즘의 장점까지 도입하여 3단계로 학습해 보자.<ul>
<li>Unsupervised&#x2F;Self-supervised pretraining : task-agnostic CNN model 학습</li>
<li>Supervised Fine-tuning : pretraining 이후에 fine-tuning을 한다</li>
<li>Dis5lla5on using unlabeled data : teacher model에서 얻은 pseudo-label로 student model tuning.</li>
</ul>
</li>
</ul>
</li>
<li>MaskFeat<ul>
<li>모티베이션: Pretext task로 masked input으로 HOG를 prediction해보자.</li>
<li>결과<ul>
<li>Masked feature predictio은 visual pre-training task로서 좋은 결과를 준다.</li>
<li>특히 비디오와 같은 프레임간의 방향성을 가진 task에서 transfer가 잘 된다.</li>
</ul>
</li>
</ul>
</li>
<li>BYOL<ul>
<li>Contrastive learning방법들은 negative를 잘 선택 해야한다.</li>
<li>결과: batch size나 image augmentaXon에 대해 simclr보다 덜 민감함</li>
</ul>
</li>
<li>Barlow Twins<ul>
<li>BYOL과 비슷하게 Siamese network 활용</li>
<li>하지만, 최종 predicthon에서 mini-batch features 의 cross-correlation matrix를 계산하고, diagonal 을 maximize함</li>
</ul>
</li>
<li>DINO: Vision Transformer + SSL<ul>
<li>DINO:self-distillation with no labels.</li>
<li>모티베이션:Self-supvViT로 학습한 모델은 이미 object의 segmentation을 어느정도 잡고 있다.</li>
</ul>
</li>
</ul>
<h2 id="Semi-Supervised-Learning-이란"><a href="#Semi-Supervised-Learning-이란" class="headerlink" title="Semi-Supervised Learning 이란"></a>Semi-Supervised Learning 이란</h2><ul>
<li><p>준지도학습</p>
</li>
<li><p>Label이 일부 샘플에만 주어진것. (Supervised + Unsupervised)</p>
</li>
<li><p>Labeling은 비용이 많이 든다.</p>
</li>
<li><p>Smoothness:x1,x2의 거리가 가까우면, y1,y2도 가까워야 한다</p>
</li>
<li><p>Low-density:decisionboundary가 data의density가 높은 곳을 지나지 않는다. (잘 분리 된다.)</p>
</li>
<li><p>Manifold:고차원의 입력이 저 차원에서 특정 manifold를 따라 놓이게 된다.</p>
</li>
<li><p>Clustering:데이터가 같은 클러스터에 속하면, 같은 클래스이다. (유사도에 따라 분류된다.)</p>
</li>
<li><p>Entropy Minimization</p>
<ul>
<li>가정: Decision boundary는 데이터의 밀도가 낮은곳에 형성 될 것이다.</li>
<li>Prediction을 좀 더 sharp하게 만들어서 entropy를 minimize함.</li>
<li>pseudo-label사용.(one-hot vector로 argmax하기 때문에)</li>
</ul>
</li>
<li><p>Proxy-Label Methods</p>
<ul>
<li>Unlabeled data point에 label을 달아주는기법.</li>
<li>Labeled data에 벗어나는 샘플은 제대로된 pseudo-label을 주기 어렵다.</li>
<li>그래도,labeled data에서의 interpolation 효과를 줌</li>
<li>Pseudo-label의 confidence가높은샘플만을 사용했을 때 성능이 더 높았다는 보고가 있음.</li>
</ul>
</li>
<li><p>Proxy-Label Methods: Label Propagation by Graph</p>
<ul>
<li>Graph-based semi-supervised learning의 목표: unlabeled data로 예측 성능을 높이는 게 목표가 아닌, unlabeled data의 label 을 추정하는 그 자체가 목표.</li>
</ul>
</li>
<li><p>Consistency Regularization</p>
<ul>
<li>Consistency regularization: unlabeleddata에 small perturbation을 주어도 예측에는 일관성이 있을 것이다.</li>
<li>즉, unlabeled data에 data augmentation 을 주어 class가 바뀌지 않을 정도의 변화를 주고, 원래 데이터와 같아지도록 unsupervised loss를 준다.</li>
</ul>
</li>
<li><p>Consistency Regularization: Temporal Ensemble</p>
<ul>
<li>Temporal Ensemble: 과거의 network evaluation을 ensemble prediction 처럼 합친다. (이유: pi model이 noisy 한 input에 의해 모델도 noisy해진다)</li>
</ul>
</li>
<li><p>Consistency Regularization: Mean Teacher</p>
<ul>
<li>Meanteacher: output prediction뿐아니라,model weight에 대해서도 temporal ensembling을 하자.</li>
</ul>
</li>
<li><p>Consistency Regularization: Virtual Adversarial Training</p>
<ul>
<li>Virtual Adversarial Training: unlabeled 입력 데이터에 augmentation을 줄때에 fixed augmentation이 아닌 adversarial 한 이미지 변형을 주자</li>
</ul>
</li>
<li><p>Consistency Regularization: UDA</p>
<ul>
<li>세가지 dataaugmentation 방식을 통해 unlabeled data를 augment함.<ul>
<li>AutoAugmentation(이미지분류): rule-basedRL augmentation찾는 기법.</li>
<li>Backtranslation(텍스트분류): 두개의 기계 번역 모델로 원본 텍스트와 유사한 다양한 텍스트를 augmentation 하여 얻는 기법.</li>
<li>TD-IDF word replacement(텍스트분류): 설명력이 낮은(TD-IDF 벡터에서 값이 낮은) 단어를 대체하고, 키워드 단어는 보존하는 기법.</li>
</ul>
</li>
</ul>
</li>
<li><p>MixMatch</p>
<ul>
<li>MixMatch:여러 semi-supervised learning기법을 합쳐높은 성능을 냄</li>
</ul>
</li>
<li><p>ReMixMatch</p>
<ul>
<li>ReMixMatch: MixMatch에서 strong augmentation의 장점을 살리기위해 augmentation anchoring 도입</li>
</ul>
</li>
<li><p>FixMatch</p>
<ul>
<li>ReMixMatch+UDA</li>
<li>결론:semi-supervised learning의 a few lines of code변경으로 높은 성능 향상을 달성했다.</li>
</ul>
</li>
</ul>
<h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr>
<ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://twitter.com/mldcmu/status/1046869963347283973">Yann Lecun (On True AI) twitter </a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://medium.com/syncedreview/yann-lecun-cake-analogy-2-0-a361da560dae">Yann LeCun Cake Analogy 2.0</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://iclr.cc/media/iclr-2021/Slides/3720.pdf"></a></li>
</ul>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="../../../tags/Representation-Learning/">Representation Learning</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="../2024-02-26-design_patterns_beauty_3/"><i class="fa fa-chevron-left">  </i><span>CHAPTER 3 설계 원칙</span></a></div><div class="next-post pull-right"><a href="../2024-02-22-representation_learning_2/"><span>Representation Learning 2</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'https://sejoung.github.io/2024/02/2024-02-23-representation_learning_3/';
  this.page.identifier = '2024/02/2024-02-23-representation_learning_3/';
  this.page.title = 'Representation Learning (Self-Supervised Learning)';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'kimsejoung' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-scr" src="https://kimsejoung.disqus.com/count.js" async></script></div></div><footer class="footer-bg" style="background-image: url(https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Van_Gogh_-_Terrasse_des_Caf%C3%A9s_an_der_Place_du_Forum_in_Arles_am_Abend1.jpeg/1024px-Van_Gogh_-_Terrasse_des_Caf%C3%A9s_an_der_Place_du_Forum_in_Arles_am_Abend1.jpeg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2024 By sejoung</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="external nofollow noopener noreferrer" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="../../../js/third-party/anime.min.js"></script><script src="../../../js/third-party/jquery.min.js"></script><script src="../../../js/third-party/jquery.fancybox.min.js"></script><script src="../../../js/third-party/velocity.min.js"></script><script src="../../../js/third-party/velocity.ui.min.js"></script><script src="../../../js/utils.js?version=1.7.0"></script><script src="../../../js/fancybox.js?version=1.7.0"></script><script src="../../../js/sidebar.js?version=1.7.0"></script><script src="../../../js/copy.js?version=1.7.0"></script><script src="../../../js/fireworks.js?version=1.7.0"></script><script src="../../../js/transition.js?version=1.7.0"></script><script src="../../../js/scroll.js?version=1.7.0"></script><script src="../../../js/head.js?version=1.7.0"></script><script src="../../../js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>