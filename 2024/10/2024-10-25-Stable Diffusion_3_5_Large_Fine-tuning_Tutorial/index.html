<!DOCTYPE html><html lang="lang"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Stable Diffusion 3.5 Large Fine-tuning Tutorial"><meta name="keywords" content="stable diffusion,LORA,Lora,훈련,학습,SD 3.5,Large,Fine-tuning,Tutorial"><meta name="author" content="sejoung"><meta name="copyright" content="sejoung"><title>Stable Diffusion 3.5 Large Fine-tuning Tutorial | 폭간의 기술블로그</title><link rel="shortcut icon" href="../../../my-favicon.ico"><link rel="stylesheet" href="../../../css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><script async src="https://www.googletagmanager.com/gtag/js?id=G-NVRTGLD8RZ"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-NVRTGLD8RZ');</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 5.4.2"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial"><span class="toc-number">1.</span> <span class="toc-text">Stable Diffusion 3.5 Large Fine-tuning Tutorial</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%EB%8C%80%EC%83%81-%EB%AF%B8%EC%84%B8-%EC%A1%B0%EC%A0%95%EC%97%90-%EB%8C%80%ED%95%9C-%EC%B5%9C%EC%86%8C%ED%95%9C%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EC%A7%80%EC%8B%9D%EC%9D%84-%EA%B0%96%EC%B6%98-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4-%EB%98%90%EB%8A%94-%EA%B8%B0%EC%88%A0-%EC%9D%B8%EB%A0%A5"><span class="toc-number">2.</span> <span class="toc-text">대상 : 미세 조정에 대한 최소한의 기본 지식을 갖춘 엔지니어 또는 기술 인력</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Tools"><span class="toc-number">2.1.</span> <span class="toc-text">Tools</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Environment-Setup"><span class="toc-number">2.2.</span> <span class="toc-text">Environment Setup</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Python-Dependencies"><span class="toc-number">2.2.1.</span> <span class="toc-text">Python Dependencies</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EC%9D%B4%EC%A0%9C-%ED%95%84%EC%9A%94%ED%95%9C-SimpleTuner-%EC%A2%85%EC%86%8D%EC%84%B1%EC%9D%B4-%EB%AA%A8%EB%91%90-%EC%84%A4%EC%B9%98%EB%90%98%EC%96%B4-%EC%9E%88%EC%96%B4%EC%95%BC-%ED%95%A9%EB%8B%88%EB%8B%A4"><span class="toc-number">2.3.</span> <span class="toc-text">이제 필요한 SimpleTuner 종속성이 모두 설치되어 있어야 합니다.</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Dependencies"><span class="toc-number">2.3.1.</span> <span class="toc-text">Model Dependencies</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Configuration-setup-high-level"><span class="toc-number">2.3.2.</span> <span class="toc-text">Configuration setup (high-level)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Configuration-setup-low-level"><span class="toc-number">2.3.3.</span> <span class="toc-text">Configuration setup (low-level)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dataloader"><span class="toc-number">2.3.4.</span> <span class="toc-text">Dataloader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-preparation"><span class="toc-number">2.3.5.</span> <span class="toc-text">Data preparation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Returning-to-the-custom-config"><span class="toc-number">2.4.</span> <span class="toc-text">Returning to the custom config</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-rate-x2F-steps"><span class="toc-number">2.4.1.</span> <span class="toc-text">Learning rate&#x2F;steps</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Steps-calculation"><span class="toc-number">2.4.2.</span> <span class="toc-text">Steps calculation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multiple-concepts"><span class="toc-number">2.4.3.</span> <span class="toc-text">Multiple concepts</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Custom-config-json-miscellaneous"><span class="toc-number">2.4.4.</span> <span class="toc-text">Custom config.json miscellaneous</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#What-happened-to-the-low-level-config-env"><span class="toc-number">2.4.5.</span> <span class="toc-text">What happened to the low-level config.env ?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EC%9D%B4%EB%8A%94-%EC%9C%84%EC%9D%98-config-json%EA%B3%BC-%EB%8F%99%EC%9D%BC%ED%95%9C-%EB%B2%84%EC%A0%84%EC%9D%B4%EC%A7%80%EB%A7%8C-env-%ED%98%95%EC%8B%9D%EC%9E%85%EB%8B%88%EB%8B%A4"><span class="toc-number">2.5.</span> <span class="toc-text">이는 위의 config.json과 동일한 버전이지만 .env 형식입니다.</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-process"><span class="toc-number">2.6.</span> <span class="toc-text">Training process</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Possible-accelerate-issues"><span class="toc-number">2.6.1.</span> <span class="toc-text">Possible accelerate issues</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Memory-usage"><span class="toc-number">2.6.2.</span> <span class="toc-text">Memory usage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Monitoring-the-training"><span class="toc-number">2.6.3.</span> <span class="toc-text">Monitoring the training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Observing-training-loss"><span class="toc-number">2.6.4.</span> <span class="toc-text">Observing training loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LoRA"><span class="toc-number">2.6.5.</span> <span class="toc-text">LoRA</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluating-the-results"><span class="toc-number">2.7.</span> <span class="toc-text">Evaluating the results</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#How-to-actually-get-the-LoRA-models-into-ComfyUI"><span class="toc-number">2.7.1.</span> <span class="toc-text">How to actually get the LoRA models into ComfyUI</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Determining-the-best-checkpoint"><span class="toc-number">2.7.2.</span> <span class="toc-text">Determining the best checkpoint</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-x2F-B-evaluation"><span class="toc-number">2.8.</span> <span class="toc-text">A&#x2F;B evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Improving-x2F-tuning-generations-with-APG-scaling"><span class="toc-number">2.8.1.</span> <span class="toc-text">Improving&#x2F;tuning generations with APG scaling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Before-and-after-comparison"><span class="toc-number">2.8.2.</span> <span class="toc-text">Before and after comparison</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Other-fine-tuning-tools-x2F-libraries-for-SD3-5"><span class="toc-number">2.9.</span> <span class="toc-text">Other fine-tuning tools&#x2F;libraries for SD3.5</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion-amp-Feedback"><span class="toc-number">2.10.</span> <span class="toc-text">Conclusion &amp; Feedback</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Two-cents-from-Dango"><span class="toc-number">2.11.</span> <span class="toc-text">Two cents from Dango</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Diving-into-SD3-5-Large-Architecture"><span class="toc-number">2.11.1.</span> <span class="toc-text">Diving into SD3.5 Large Architecture</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%EC%B0%B8%EC%A1%B0"><span class="toc-number">3.</span> <span class="toc-text">참조</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://avatars0.githubusercontent.com/u/4936005?s=400&amp;u=a679b941fe377418e7e4efcf916c6a636d7178ee&amp;v=4"></div><div class="author-info__name text-center">sejoung</div><div class="author-info__description text-center">잘정리하자</div><div class="follow-button"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/sejoung">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="../../../archives"><span class="pull-left">Articles</span><span class="pull-right">866</span></a><a class="author-info-articles__tags article-meta" href="../../../tags"><span class="pull-left">Tags</span><span class="pull-right">905</span></a><a class="author-info-articles__categories article-meta" href="../../../categories"><span class="pull-left">Categories</span><span class="pull-right">80</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="external nofollow noopener noreferrer" href="https://blog.naver.com/sanaes">naverblog</a><a class="author-info-links__name text-center" target="_blank" rel="external nofollow noopener noreferrer" href="https://www.linkedin.com/in/sanaes/">linkedin</a><a class="author-info-links__name text-center" target="_blank" rel="external nofollow noopener noreferrer" href="https://www.slideshare.net/sejoung">slideshare</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Van_Gogh_-_Terrasse_des_Caf%C3%A9s_an_der_Place_du_Forum_in_Arles_am_Abend1.jpeg/1024px-Van_Gogh_-_Terrasse_des_Caf%C3%A9s_an_der_Place_du_Forum_in_Arles_am_Abend1.jpeg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="../../../index.html">폭간의 기술블로그</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">Stable Diffusion 3.5 Large Fine-tuning Tutorial</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2024-10-25</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="../../../categories/Machine-Learning/">Machine Learning</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2024/10/2024-10-25-Stable Diffusion_3_5_Large_Fine-tuning_Tutorial/"></span></a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial"><a href="#Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial" class="headerlink" title="Stable Diffusion 3.5 Large Fine-tuning Tutorial"></a>Stable Diffusion 3.5 Large Fine-tuning Tutorial</h1><p>이글은 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://stabilityai.notion.site/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6">Stable Diffusion 3.5 Large Fine-tuning Tutorial</a><br>글을 번역한 글입니다 이미지는 따로 첨부 하지 않으며 필요하다고 생각하는 부분만 번역합니다</p>
<h1 id="대상-미세-조정에-대한-최소한의-기본-지식을-갖춘-엔지니어-또는-기술-인력"><a href="#대상-미세-조정에-대한-최소한의-기본-지식을-갖춘-엔지니어-또는-기술-인력" class="headerlink" title="대상 : 미세 조정에 대한 최소한의 기본 지식을 갖춘 엔지니어 또는 기술 인력"></a>대상 : 미세 조정에 대한 최소한의 기본 지식을 갖춘 엔지니어 또는 기술 인력</h1><p>목적: SD1.5&#x2F;SDXL과 Stable Diffusion 3 Medium&#x2F;Large(SD3.5M&#x2F;L) 미세 조정 간의 차이점을 이해하고 더 많은 사용자가 두 모델을 미세 조정할 수 있도록 합니다.</p>
<h2 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h2><p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner">SimpleTuner</a> toolkit</p>
<h2 id="Environment-Setup"><a href="#Environment-Setup" class="headerlink" title="Environment Setup"></a>Environment Setup</h2><p>환경 설정은 여전히 이전과 비슷하지만, 이전 게시물 이후 SimpleTuner의 구성에는 <strong>많은</strong> 변경이 있었습니다.<br>가능한 한 이 작업을 간소화하려고 노력하겠지만 이전 <code>config.env</code> 파일과 새로운 <code>config.env</code> 및 <code>config.json</code>을 모두 사용하여 실험했습니다.<br><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main">여기</a>에 지정된 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/configure.py">configure.py</a> 방법을 사용했습니다. &#x2F;documentation&#x2F;quickstart&#x2F;SD3.md)를 참조하여 결과 파일이 무엇을 제공하는지 확인하세요.</p>
<p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://emojipedia.org/warning">**⚠️</a>** Just a note of warning, if you’d like to use your <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.notion.so/17f90df74bce4c62a295849f0dc8fb7e?pvs=21">old</a> <code>config.env</code> files, you’ll have to do some slight tweaking. I’ll cover it later in this <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.notion.so/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6?pvs=21">section</a>.</p>
<p>If you want to see the full list of options available, you can check the <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/OPTIONS.md#environment-configuration-variables">OPTIONS.MD</a> file.</p>
<ul>
<li>Sample <code>.json</code> generated with <code>configure.py</code> (used as a reference)</li>
</ul>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;config/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--aspect_bucket_rounding&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--disable_benchmark&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">256</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-sd35-large-fantasy-art-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel_area&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1024&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="string">&quot;42&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;35&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1024x1024&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="string">&quot;7.5&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;35&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1.05e-3&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;polynomial&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2400&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_torch_compile&quot;</span><span class="punctuation">:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>또한, <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/tree/release">릴리스 브랜치</a> 대신 <code>SimpleTuner</code>의 최신 메인 브랜치 중 하나를 사용하고 있습니다. 가능한 한 현재까지. 커밋 해시(2024년 10월 15일)는 다음과 같습니다.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">694784083c70bf81086bb3ceba86262b7b22757d</span><br></pre></td></tr></table></figure>

<h3 id="Python-Dependencies"><a href="#Python-Dependencies" class="headerlink" title="Python Dependencies"></a>Python Dependencies</h3><p>종속성을 설치하려면 저장소 페이지에서 SD3용 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/SD3.md">빠른 시작 가이드</a>를 따르세요. 여기에서도 살펴보고 대체 설치 방법도 추가하겠습니다. <code>SimpleTuner</code>(<code>12.4+</code>)와 일치하는 <code>CUDA</code> 버전이 있는 경우 종속성 설치가 매우 간단할 수 있지만 <code>CUDA</code>의 이전 버전을 사용하는 경우 조금 더 복잡해질 수 있습니다.</p>
<p>우선 저장소를 <code>git clone</code>합니다.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/bghira/SimpleTuner.git</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd SimpleTuner</span><br></pre></td></tr></table></figure>


<p>마지막으로 위에서 언급한 커밋 해시를 확인하세요. 디버깅을 하고 싶다면 계속해서 분기를 생성해 보겠습니다(‘base_branch’라는 이름, 자유롭게 이름을 바꾸세요).</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b base_branch 694784083c70bf81086bb3ceba86262b7b22757d</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch</span><br></pre></td></tr></table></figure>

<p>새 지점에 있으면 Python 가상 환경을 만들 차례입니다. 종속성을 설치할 때 <code>python 3.11</code>을 사용하는 것이 좋습니다.</p>
<p>각각 다음 명령을 사용하여 <code>OS</code> 및 <code>CUDA</code> 환경을 확인할 수 있습니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">uname</span> -a</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc --version</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python --version</span><br></pre></td></tr></table></figure>


<p><code>SimpleTuner</code> 디렉터리의 루트에 이 명령을 사용하여 <code>virtualenv</code>를 만듭니다.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv .venv</span><br></pre></td></tr></table></figure>

<p>Activate it with:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source .venv/bin/activate</span><br></pre></td></tr></table></figure>

<p>완료되면 <code>poetry</code>(<code>pip</code> 또는 <code>uv</code>와 유사한 종속성 관리자)를 설치합니다.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U poetry pip</span><br></pre></td></tr></table></figure>

<p><code>bghira</code>는 안전을 위해 이 명령을 실행할 것을 권장합니다:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poetry config virtualenvs.create false</span><br></pre></td></tr></table></figure>

<p>저는 <code>Linux</code>를 사용하고 있으므로 다음 단계는 다음 명령을 사용하여 모든 종속성을 설치하는 것입니다.</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poetry install</span><br></pre></td></tr></table></figure>


<p>그러나 SD3.5 Large는 <code>diffusers</code>의 특정 커밋에 따라 달라집니다(아마도 최신 버전도 작동할 것입니다). 이 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/huggingface/diffusers/commit/e2d037bbf1388fdc172458bed7a8a58b34fc6f84">커밋</a> 이상이 포함된 버전을 사용하고 있는지 확인하세요.</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">e2d037bbf1388fdc172458bed7a8a58b34fc6f84</span><br></pre></td></tr></table></figure>


<p>이는 ‘bghira’로 변경될 수 있으며 그의 팀은 SimpleTuner 저장소를 매우 빠르게 업데이트합니다. 올바른 버전의 <code>diffusers</code>를 사용하고 있는지 확인하려면 <code>SimpleTuner</code> 디렉터리의 <code>pyproject.toml</code> 파일을 변경하여 올바른 커밋을 사용하세요.</p>
<figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[tool.poetry]</span></span><br><span class="line"><span class="attr">name</span> = <span class="string">&quot;simpletuner&quot;</span></span><br><span class="line"><span class="attr">version</span> = <span class="string">&quot;1.1.0&quot;</span></span><br><span class="line"><span class="attr">description</span> = <span class="string">&quot;Stable Diffusion 2.x and XL tuner.&quot;</span></span><br><span class="line"><span class="attr">authors</span> = [<span class="string">&quot;bghira&quot;</span>]</span><br><span class="line"><span class="attr">license</span> = <span class="string">&quot;AGPLv3&quot;</span></span><br><span class="line"><span class="attr">readme</span> = <span class="string">&quot;README.md&quot;</span></span><br><span class="line"><span class="attr">package-mode</span> = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="section">[tool.poetry.dependencies]</span></span><br><span class="line"><span class="attr">python</span> = <span class="string">&quot;&gt;=3.10,&lt;3.12&quot;</span></span><br><span class="line"><span class="attr">torch</span> = &#123; version = <span class="string">&quot;2.4.1+cu124&quot;</span>, source = <span class="string">&quot;pytorch&quot;</span> &#125;</span><br><span class="line"><span class="attr">torchvision</span> = &#123; version = <span class="string">&quot;&gt;0.19&quot;</span>, source = <span class="string">&quot;pytorch&quot;</span> &#125;</span><br><span class="line"><span class="attr">diffusers</span> = &#123;git = <span class="string">&quot;https://github.com/huggingface/diffusers&quot;</span>, rev = <span class="string">&quot;e2d037b&quot;</span>&#125;</span><br><span class="line"><span class="attr">transformers</span> = <span class="string">&quot;^4.45.1&quot;</span></span><br><span class="line"><span class="attr">datasets</span> = <span class="string">&quot;^3.0.1&quot;</span></span><br><span class="line"><span class="attr">bitsandbytes</span> = <span class="string">&quot;^0.44.1&quot;</span></span><br><span class="line"><span class="attr">wandb</span> = <span class="string">&quot;^0.18.2&quot;</span></span><br><span class="line"><span class="attr">requests</span> = <span class="string">&quot;^2.32.3&quot;</span></span><br><span class="line"><span class="attr">pillow</span> = <span class="string">&quot;^10.4.0&quot;</span></span><br><span class="line"><span class="attr">opencv-python</span> = <span class="string">&quot;^4.10.0.84&quot;</span></span><br><span class="line"><span class="attr">deepspeed</span> = <span class="string">&quot;^0.15.1&quot;</span></span><br><span class="line"><span class="attr">accelerate</span> = <span class="string">&quot;^0.34.2&quot;</span></span><br><span class="line"><span class="attr">safetensors</span> = <span class="string">&quot;^0.4.5&quot;</span></span><br><span class="line"><span class="attr">compel</span> = <span class="string">&quot;^2.0.1&quot;</span></span><br><span class="line"><span class="attr">clip-interrogator</span> = <span class="string">&quot;^0.6.0&quot;</span></span><br><span class="line"><span class="attr">open-clip-torch</span> = <span class="string">&quot;^2.26.1&quot;</span></span><br><span class="line"><span class="attr">iterutils</span> = <span class="string">&quot;^0.1.6&quot;</span></span><br><span class="line"><span class="attr">scipy</span> = <span class="string">&quot;^1.11.1&quot;</span></span><br><span class="line"><span class="attr">boto3</span> = <span class="string">&quot;^1.35.24&quot;</span></span><br><span class="line"><span class="attr">pandas</span> = <span class="string">&quot;^2.2.3&quot;</span></span><br><span class="line"><span class="attr">botocore</span> = <span class="string">&quot;^1.35.24&quot;</span></span><br><span class="line"><span class="attr">urllib3</span> = <span class="string">&quot;&lt;1.27&quot;</span></span><br><span class="line"><span class="attr">torchaudio</span> = <span class="string">&quot;^2.4.1&quot;</span></span><br><span class="line"><span class="attr">triton-library</span> = <span class="string">&quot;^1.0.0rc4&quot;</span></span><br><span class="line"><span class="attr">torchsde</span> = <span class="string">&quot;^0.2.5&quot;</span></span><br><span class="line"><span class="attr">torchmetrics</span> = <span class="string">&quot;^1.1.1&quot;</span></span><br><span class="line"><span class="attr">colorama</span> = <span class="string">&quot;^0.4.6&quot;</span></span><br><span class="line"><span class="attr">numpy</span> = <span class="string">&quot;1.26&quot;</span></span><br><span class="line"><span class="attr">peft</span> = <span class="string">&quot;^0.12.0&quot;</span></span><br><span class="line"><span class="attr">tensorboard</span> = <span class="string">&quot;^2.17.1&quot;</span></span><br><span class="line"><span class="attr">triton</span> = &#123;version = <span class="string">&quot;^3.0.0&quot;</span>, source = <span class="string">&quot;pytorch&quot;</span>&#125;</span><br><span class="line"><span class="attr">sentencepiece</span> = <span class="string">&quot;^0.2.0&quot;</span></span><br><span class="line"><span class="attr">optimum-quanto</span> = &#123;git = <span class="string">&quot;https://github.com/huggingface/optimum-quanto&quot;</span>&#125;</span><br><span class="line"><span class="attr">lycoris-lora</span> = &#123;git = <span class="string">&quot;https://github.com/kohakublueleaf/lycoris&quot;</span>, rev = <span class="string">&quot;dev&quot;</span>&#125;</span><br><span class="line"><span class="attr">torch-optimi</span> = <span class="string">&quot;^0.2.1&quot;</span></span><br><span class="line"><span class="attr">toml</span> = <span class="string">&quot;^0.10.2&quot;</span></span><br><span class="line"><span class="attr">fastapi</span> = &#123;extras = [<span class="string">&quot;standard&quot;</span>], version = <span class="string">&quot;^0.115.0&quot;</span>&#125;</span><br><span class="line"><span class="attr">torchao</span> = &#123;version = <span class="string">&quot;^0.5.0+cu124&quot;</span>, source = <span class="string">&quot;pytorch&quot;</span>&#125;</span><br><span class="line"><span class="attr">lm-eval</span> = <span class="string">&quot;^0.4.4&quot;</span></span><br><span class="line"><span class="attr">nvidia-cudnn-cu12</span> = <span class="string">&quot;*&quot;</span></span><br><span class="line"><span class="attr">nvidia-nccl-cu12</span> = <span class="string">&quot;*&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">[build-system]</span></span><br><span class="line"><span class="attr">requires</span> = [<span class="string">&quot;poetry-core&quot;</span>, <span class="string">&quot;setuptools&quot;</span>, <span class="string">&quot;wheel&quot;</span>, <span class="string">&quot;torch&quot;</span>]</span><br><span class="line"><span class="attr">build-backend</span> = <span class="string">&quot;poetry.core.masonry.api&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[tool.poetry.source]]</span></span><br><span class="line"><span class="attr">priority</span> = <span class="string">&quot;supplemental&quot;</span></span><br><span class="line"><span class="attr">name</span> = <span class="string">&quot;pytorch&quot;</span></span><br><span class="line"><span class="attr">url</span> = <span class="string">&quot;https://download.pytorch.org/whl/cu124&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>변경 사항은 다음과 같습니다.</p>
<p>Old</p>
<figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">diffusers</span> = &#123;git = <span class="string">&quot;https://github.com/huggingface/diffusers&quot;</span>, rev = <span class="string">&quot;quantization-config&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>New</p>
<figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">diffusers</span> = &#123;git = <span class="string">&quot;https://github.com/huggingface/diffusers&quot;</span>, rev = <span class="string">&quot;e2d037b&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<h2 id="이제-필요한-SimpleTuner-종속성이-모두-설치되어-있어야-합니다"><a href="#이제-필요한-SimpleTuner-종속성이-모두-설치되어-있어야-합니다" class="headerlink" title="이제 필요한 SimpleTuner 종속성이 모두 설치되어 있어야 합니다."></a>이제 필요한 <code>SimpleTuner</code> 종속성이 모두 설치되어 있어야 합니다.</h2><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://emojipedia.org/police-car-light">**🚨</a>** 컴퓨터 환경에서 ‘CUDA 12.4’ 이상이 아닌 경우 ‘SimpleTuner’가 ‘CUDA 12.4’ 이상이라는 가정하에 작동하므로 CUDA 종속성 문제가 발생할 수 있습니다. 앞서 알아차리셨다면 저는 <code>CUDA 12.2</code>를 사용 중이었고 <code>poetry install</code> 문제가 발생했습니다.<ul>
<li><p>이 단락을 펼치고 <strong>대체</strong> 설치 지침을 보려면 ▷를 클릭하세요.</p>
<p>대신, 제가 한 일은 기본 <code>torch</code> 종속성을 먼저 설치한 다음 <code>pyproject.toml</code>의 나머지 종속성을 포함하는 <code>requirements.txt</code> 파일을 만드는 것이었습니다. 그런 다음 해당 텍스트 파일에 <code>pip install</code>을 실행했습니다.</p>
<p><code>poetry install</code>을 먼저 시도하고 문제가 발생했다면 기존 <code>virtualenv</code>를 제거하고 다시 설치하는 것이 좋습니다.</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">rm</span> -rf .venv</span><br></pre></td></tr></table></figure>

  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv .venv</span><br></pre></td></tr></table></figure>

  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> .venv/bin/activate</span><br></pre></td></tr></table></figure>

<p>  이제 ‘CUDA’ 버전에 따라 먼저 토치 종속성을 설치하세요. CUDA 12.1은 내 환경인 ‘CUDA 12.2’에 비해 낮은 버전이므로 나에게 적합합니다.</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure>

<p>‘cu121’이 추가된 것을 볼 수 있습니다. 이는 ‘CUDA’ 버전을 지정합니다. <code>CUDA</code> 버전에 맞게 변경하세요.<br>그런 다음 <code>torchao</code>를 설치합니다.</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchao --extra-index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure>

<p>이제 <code>SimpleTuner</code> 디렉터리 루트에 <code>requirements.txt</code> 파일을 만듭니다.</p>
<ul>
<li><p><code>requirements.txt</code></p>
  <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">diffusers @ git+https://github.com/huggingface/diffusers.git@e2d037b</span><br><span class="line">transformers==4.45.1</span><br><span class="line">datasets==3.0.1</span><br><span class="line">bitsandbytes==0.44.1</span><br><span class="line">wandb==0.18.2</span><br><span class="line">requests==2.32.3</span><br><span class="line">pillow==10.4.0</span><br><span class="line">opencv-python==4.10.0.84</span><br><span class="line">deepspeed==0.15.1</span><br><span class="line">accelerate==0.34.2</span><br><span class="line">safetensors==0.4.5</span><br><span class="line">compel==2.0.1</span><br><span class="line">clip-interrogator==0.6.0</span><br><span class="line">open-clip-torch==2.26.1</span><br><span class="line">iterutils==0.1.6</span><br><span class="line">scipy==1.11.1</span><br><span class="line">boto3==1.35.24</span><br><span class="line">pandas==2.2.3</span><br><span class="line">botocore==1.35.24</span><br><span class="line">urllib3&lt;1.27</span><br><span class="line">triton-library==1.0.0rc2</span><br><span class="line">torchsde==0.2.5</span><br><span class="line">torchmetrics==1.1.1</span><br><span class="line">colorama==0.4.6</span><br><span class="line">numpy==1.26</span><br><span class="line">peft==0.12.0</span><br><span class="line">tensorboard==2.17.1</span><br><span class="line">triton==3.0.0</span><br><span class="line">sentencepiece==0.2.0</span><br><span class="line">optimum-quanto @ git+https://github.com/huggingface/optimum-quanto.git</span><br><span class="line">lycoris-lora @ git+https://github.com/kohakublueleaf/lycoris.git@dev</span><br><span class="line">torch-optimi==0.2.1</span><br><span class="line">toml==0.10.2</span><br><span class="line">fastapi[standard]==0.115.0</span><br><span class="line">lm-eval==0.4.4</span><br></pre></td></tr></table></figure></li>
</ul>
<p>  완료되면 종속성을 설치하십시오.</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<p> 이제 필요한 모든 종속성이 설치되어 있어야 합니다.</p>
</li>
</ul>
</li>
</ul>
<h3 id="Model-Dependencies"><a href="#Model-Dependencies" class="headerlink" title="Model Dependencies"></a>Model Dependencies</h3><p>이번에는 기본 체크포인트와 디퓨저가 &#96;stabilityai&#x2F;stable-diffusion-‘이라는 Hugging Face <a target="_blank" rel="external nofollow noopener noreferrer" href="https://huggingface.co/stabilityai/stable-diffusion-3.5-large">저장소</a>에 모두 잘 패키지되어 있습니다. </p>
<p><code>MODEL_NAME</code>(<code>config.env</code>를 사용하는 경우) 또는 <code>--pretrained_model_name_or_path</code>(<code>config.json</code>을 사용하는 경우)를 <code>stabilityai/stable-diffusion-3.5-large</code>로 설정하세요. <code>SimpleTuner</code>는 Hugging Face에서 모델을 가져와 홈 디렉토리의 <code>.cache</code> 디렉토리에 저장합니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/.cache/huggingface/hub </span><br></pre></td></tr></table></figure>

<p>모델 파일은 <code>~/.cache/huggingface/hub/models--stabilityai--stable-diffusion-3.5-large/snapshots/hash</code> 내에 다음과 같이 표시됩니다.</p>
<h3 id="Configuration-setup-high-level"><a href="#Configuration-setup-high-level" class="headerlink" title="Configuration setup (high-level)"></a>Configuration setup (high-level)</h3><p>이전 버전의 ‘SimpleTuner’에서 오시는 경우 상위 수준 구성 파일 설정이 크게 변경되었습니다. 그러나 내부 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/OPTIONS.md#environment-configuration-variables">OPTIONS.MD</a>는 여전히 동일하게 유지됩니다.</p>
<p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://emojipedia.org/warning">**⚠️</a> 특히**, <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/">SD3 빠른 시작</a>만 따르면 됩니다. SD3.md) 구성 파일을 정확히 어떻게 설정해야 하는지 전체 그림을 얻지 못할 수도 있습니다. <code>SimpleTuner</code>의 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/INSTALL.md">INSTALL.MD</a> 파일은 구성 파일 시스템이 정확히 어떻게 작동하는지에 대한 전체 그림을 제공합니다.</p>
<p>더 진행하기 전에 실제로 훈련이 어떻게 시작되는지 알아보고 싶습니다. 빠른 시작에서는 다음을 사용하여 실행한다고 나와 있습니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash train.sh</span><br></pre></td></tr></table></figure>

<ul>
<li><p>기본 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://train.sh/">train.sh</a>가 여기에 제공됩니다.</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pull config from config.env</span></span><br><span class="line">[ -f <span class="string">&quot;config/config.env&quot;</span> ] &amp;&amp; <span class="built_in">source</span> config/config.env</span><br><span class="line"></span><br><span class="line"><span class="comment"># If the user has not provided VENV_PATH, we will assume $(pwd)/.venv</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;VENV_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="comment"># what if we have VIRTUAL_ENV? use that instead</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;VIRTUAL_ENV&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> VENV_PATH=<span class="string">&quot;<span class="variable">$&#123;VIRTUAL_ENV&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">export</span> VENV_PATH=<span class="string">&quot;<span class="subst">$(pwd)</span>/.venv&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;DISABLE_LD_OVERRIDE&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> NVJITLINK_PATH=<span class="string">&quot;<span class="subst">$(find <span class="string">&quot;<span class="variable">$&#123;VENV_PATH&#125;</span>&quot;</span> -name nvjitlink -type d)</span>/lib&quot;</span></span><br><span class="line">    <span class="comment"># if it&#x27;s not empty, we will add it to LD_LIBRARY_PATH at the front:</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;NVJITLINK_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">&quot;<span class="variable">$&#123;NVJITLINK_PATH&#125;</span>:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TOKENIZERS_PARALLELISM=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> PLATFORM</span><br><span class="line">PLATFORM=$(<span class="built_in">uname</span> -s)</span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$PLATFORM</span>&quot;</span> == <span class="string">&quot;Darwin&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> MIXED_PRECISION=<span class="string">&quot;no&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_EXTRA_ARGS&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_PROCESSES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Set custom env vars permanently in config/config.env:&quot;</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_NUM_PROCESSES not set, defaulting to 1.\n&quot;</span></span><br><span class="line">    TRAINING_NUM_PROCESSES=1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_MACHINES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_NUM_MACHINES not set, defaulting to 1.\n&quot;</span></span><br><span class="line">    TRAINING_NUM_MACHINES=1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;MIXED_PRECISION&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;MIXED_PRECISION not set, defaulting to bf16.\n&quot;</span></span><br><span class="line">    MIXED_PRECISION=bf16</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_DYNAMO_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_DYNAMO_BACKEND not set, defaulting to no.\n&quot;</span></span><br><span class="line">    TRAINING_DYNAMO_BACKEND=<span class="string">&quot;no&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;ENV&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;ENV not set, defaulting to default.\n&quot;</span></span><br><span class="line">    <span class="built_in">export</span> ENV=<span class="string">&quot;default&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">export</span> ENV_PATH=<span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$ENV</span>&quot;</span> != <span class="string">&quot;default&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> ENV_PATH=<span class="string">&quot;<span class="variable">$&#123;ENV&#125;</span>/&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;CONFIG_TYPE&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;<span class="variable">$&#123;CONFIG_TYPE&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_PATH=<span class="string">&quot;config/<span class="variable">$&#123;ENV_PATH&#125;</span>config&quot;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.json&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.toml&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;toml&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.env&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Using <span class="variable">$&#123;CONFIG_BACKEND&#125;</span> backend: <span class="variable">$&#123;CONFIG_PATH&#125;</span>.<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Update dependencies</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;DISABLE_UPDATES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;Updating dependencies. Set DISABLE_UPDATES to prevent this.&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;pyproject.toml&quot;</span> ] &amp;&amp; [ -f <span class="string">&quot;poetry.lock&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        nvidia-smi 2&gt; /dev/null &amp;&amp; poetry install</span><br><span class="line">        <span class="built_in">uname</span> -s | grep -q Darwin &amp;&amp; poetry install -C install/apple</span><br><span class="line">        rocm-smi 2&gt; /dev/null &amp;&amp; poetry install -C install/rocm</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment"># Run the training script.</span></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_CONFIG_PATH=<span class="string">&quot;<span class="variable">$&#123;HOME&#125;</span>/.cache/huggingface/accelerate/default_config.yaml&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Using Accelerate config file: <span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span></span><br><span class="line">    accelerate launch --config_file=<span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> train.py</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Accelerate config file not found: <span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>. Using values from config.env.&quot;</span></span><br><span class="line">    accelerate launch <span class="variable">$&#123;ACCELERATE_EXTRA_ARGS&#125;</span> --mixed_precision=<span class="string">&quot;<span class="variable">$&#123;MIXED_PRECISION&#125;</span>&quot;</span> --num_processes=<span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_PROCESSES&#125;</span>&quot;</span> --num_machines=<span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_MACHINES&#125;</span>&quot;</span> --dynamo_backend=<span class="string">&quot;<span class="variable">$&#123;TRAINING_DYNAMO_BACKEND&#125;</span>&quot;</span> train.py</span><br><span class="line"></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure></li>
</ul>
<p>이것이 일반적인 흐름입니다.</p>
<p>처음에는 <code>SimpleTuner/config</code> 디렉토리에서 <code>config.env</code>를 소스로 사용합니다. 이는 <code>gpus</code> 수와 같은 중요한 설정이 포함된 상위 수준 <code>config.env</code>가 있고 보다 세부적인 설정이 포함된 <code>config.json</code> 또는 <code>config.env</code>와 같은 하위 수준 구성이 있기 때문에 혼란스럽습니다. 설정(예: <code>model_family</code>, <code>learning_rate</code> 등).</p>
<p>그러나 저장소를 <code>git clone</code>하면 <code>config.env</code> 파일이 표시되지 않습니다.</p>
<p>내 테스트에서는 실제로 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/INSTALL.md">INSTALL.MD</a>에 따라 상위 수준 <code>config.env</code>를 생성할 필요가 없습니다. , 하지만 <code>config</code> 폴더 내에서 폴더를 동적으로 전환하는 데 도움이 되므로 그렇게 하는 것이 좋습니다.</p>
<p><code>config</code> 디렉터리에 <code>config.env</code> 파일을 만듭니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim SimpleTuner/config/config.env</span><br></pre></td></tr></table></figure>

<ul>
<li><p>High-level <code>config.env</code></p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_NUM_PROCESSES=1</span><br><span class="line">TRAINING_NUM_MACHINES=1</span><br><span class="line">TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line">MIXED_PRECISION=<span class="string">&#x27;bf16&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line"><span class="built_in">export</span> ENV=<span class="string">&quot;default&quot;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash train.sh</span><br></pre></td></tr></table></figure>


<p><code>SimpleTuner</code>는 <code>ENV</code> 디렉토리 내에서 <code>config</code> 디렉토리인 <code>config.json</code>을 검색합니다. 그 이유는 마스터 <code>config.env</code> 파일에서 <code>ENV</code>가 <code>default</code>로 설정되어 있기 때문입니다. 이는 <code>SimpleTuner/config</code>를 의미합니다.</p>
<p>‘config.json’을 찾는 이유가 무엇인지 물어볼 수도 있습니다. 음, <a target="_blank" rel="external nofollow noopener noreferrer" href="http://train.sh/"><code>train.sh</code></a> 파일에서 이 코드 블록을 보면, <code>CONFIG_BACKEND</code>로 지정한 내용에 따라 이 파일을 찾는다는 것을 알 수 있습니다. 마스터 <code>config.env</code> 파일:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_PATH=<span class="string">&quot;config/<span class="variable">$&#123;ENV_PATH&#125;</span>config&quot;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.json&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.toml&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;toml&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.env&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Using <span class="variable">$&#123;CONFIG_BACKEND&#125;</span> backend: <span class="variable">$&#123;CONFIG_PATH&#125;</span>.<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>


<p><code>config.*</code>의 이름을 변경할 수 있는지 궁금하실 수도 있습니다. <code>config_fantasy_art_lora_01.*</code>를 사용할 수 있나요? <code>config_fantasy_art_full_01.*</code>은 어떻습니까?</p>
<p>안타깝게도 그럴 수 없는 것 같습니다. <code>train.sh</code> 파일에서 <code>config.*</code>의 이름을 변경하더라도 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/helpers/configuration/loader">loader.py</a> .py#L17) 구성 도우미의 코드는 기본적으로 다음 값으로 설정됩니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">default_config_paths = &#123;</span><br><span class="line">    <span class="string">&quot;json&quot;</span>: <span class="string">&quot;config.json&quot;</span>,</span><br><span class="line">    <span class="string">&quot;toml&quot;</span>: <span class="string">&quot;config.toml&quot;</span>,</span><br><span class="line">    <span class="string">&quot;env&quot;</span>: <span class="string">&quot;config.env&quot;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>따라서 세부 훈련 매개변수 설정으로 하위 수준 <code>config.*</code> 파일을 구별하고 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob">loader.py</a>를 수정하고 싶지 않은 경우 &#x2F;main&#x2F;helpers&#x2F;configuration&#x2F;loader.py#L17) 코드를 사용하는 경우 훈련에 해당하는 <code>SimpleTuner/config</code> 디렉토리 내에 폴더를 생성하는 것이 좋습니다. 나도 똑같이 할 것이다.</p>
<p><code>SimpleTuner/config</code> 안에 첫 번째 훈련을 위한 디렉토리를 생성해 보겠습니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> SimpleTuner/config/sd35_fantasy_art_lora</span><br></pre></td></tr></table></figure>

<p>이제 <code>SimpleTuner/config/config.env</code>에서 상위 수준 <code>config.env</code>를 다음과 같이 수정하겠습니다.</p>
<ul>
<li><p>High-level <code>config.env</code></p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_NUM_PROCESSES=1</span><br><span class="line">TRAINING_NUM_MACHINES=1</span><br><span class="line">TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line">MIXED_PRECISION=<span class="string">&#x27;bf16&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line"><span class="built_in">export</span> ENV=<span class="string">&quot;sd35_fantasy_art_lora&quot;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>훈련이 시작되면 먼저 <code>SimpleTuner/config/config.env</code>에서 마스터 <code>config.env</code>를 소싱한 다음 <code>SimpleTuner/config/sd35_fantasy_art_lora</code>에서 해당 <code>config.$&#123;CONFIG_BACKEND&#125;</code> 파일을 찾습니다. 이 경우 <code>config.json</code> 입니다.</p>
<p>이를 이해하면 다양한 모델에 대한 다양한 ‘config’ 학습 매개변수를 관리하는 것이 매우 쉬워지므로 학습 흐름이 명확해지기를 바랍니다.</p>
<p>이제 하위 수준 <code>config.*</code> 파일로 이동하겠습니다.</p>
<h3 id="Configuration-setup-low-level"><a href="#Configuration-setup-low-level" class="headerlink" title="Configuration setup (low-level)"></a>Configuration setup (low-level)</h3><p><code>SimpleTuner/config/</code> 디렉토리에는 <code>bghira</code>에서 제공하는 기본 <code>config.json.example</code>이 있습니다.</p>
<p>자세한 내용을 알고 싶지 않다면 내 맞춤 <code>config.json</code> 사용으로 건너뛰세요.</p>
<ul>
<li><p>맞춤형 SD3.5 대형 <code>LoRA</code> <code>config.json</code></p>
  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">1.05e-3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--instance_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4 &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful female blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--metadata_update_interval&quot;</span><span class="punctuation">:</span> <span class="number">65</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--vae_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_unwanted_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_problematic_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--training_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--inference_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--snr_gamma&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--enable_xformers_memory_efficient_attention&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--allow_tf32&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--use_ema&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--ema_decay&quot;</span><span class="punctuation">:</span> <span class="number">0.999</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_alpha&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>자세한 내용을 알고 싶다면 계속 읽어보세요.</p>
<p><code>SimpleTuner</code> 루트에 있는 <code>config</code> 파일을 <code>ENV</code> 디렉터리에 복사하여 시작할 수 있습니다. 이것이 내 명령이다.</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp config/config.<span class="property">json</span>.<span class="property">example</span> config/sd35_fantasy_art_lora/config.<span class="property">json</span></span><br></pre></td></tr></table></figure>

<p>일단 열면 <code>json</code> 파일은 다음과 같습니다:</p>
<ul>
<li><p><code>config.json.example</code></p>
  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;config/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--aspect_bucket_rounding&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;output/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lycoris&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lycoris_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;config/lycoris_config.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">10000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">500</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0.1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel_area&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">500</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1024x1024&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">3.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;20&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;A photo-realistic image of a cat&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1e-4&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;polynomial&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">100</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_torch_compile&quot;</span><span class="punctuation">:</span> <span class="string">&quot;false&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--disable_benchmark&quot;</span><span class="punctuation">:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>원하신다면 이 제품을 즉시 사용하실 수 있습니다. 그러나 제공된 <code>json</code>에는 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/OPTIONS.md#environment-configuration-variables">OPTIONS.MD</a>의 다른 매개변수가 많이 부족합니다. <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/configure.py">configure.py</a>를 사용하더라도 결국 다음과 같은 <code>config.json</code> 파일이 생성됩니다.</p>
<ul>
<li><p><code>configure.py</code>로 생성된 샘플 <code>.json</code>(참조로 사용됨)</p>
  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;config/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--aspect_bucket_rounding&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--disable_benchmark&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">256</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-sd35-large-fantasy-art-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3-medium-diffusers&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel_area&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1024&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="string">&quot;42&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;200&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1024x1024&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="string">&quot;7.5&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;35&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful female blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1.05e-3&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;polynomial&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2400&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_torch_compile&quot;</span><span class="punctuation">:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/configure.py">configure.py</a>는 <code>lora_rank</code>와 같은 일부 매개변수를 제한할 뿐만 아니라 유효성 검사 중에 부정적인 프롬프트(<code>validation_negative_prompt)를 생략합니다. </code>) 무엇보다도 먼저 아래 <code>config.json</code>을 복사하여 시작하는 것이 좋습니다.</p>
<ul>
<li><p>Custom SD3.5 Large <code>LoRA</code> <code>config.json</code></p>
  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">1.05e-3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--instance_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4 &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful female blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--metadata_update_interval&quot;</span><span class="punctuation">:</span> <span class="number">65</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--vae_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_unwanted_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_problematic_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--training_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--inference_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--snr_gamma&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--enable_xformers_memory_efficient_attention&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--allow_tf32&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--use_ema&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--ema_decay&quot;</span><span class="punctuation">:</span> <span class="number">0.999</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_alpha&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>뭔가 눈치채셨을 수도 있지만, 우리는 <strong>더 이상</strong> 이전 하위 수준 <code>config.env</code>의 이 매개변수를 사용하지 않습니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">export</span> STABLE_DIFFUSION_3=<span class="literal">true</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>대신 <code>&quot;--model_family&quot;</code> 매개변수로 대체되었습니다. 이것을 <code>sd3</code>으로 설정합니다:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;--model_family&quot;: &quot;sd3&quot;</span><br></pre></td></tr></table></figure>


<p>실제로, 낮은 수준 <code>config.env</code>는 <code>SimpleTuner</code>에 의해 더 이상 사용되지 않을 수 있습니다. 하지만 원하시면 그래도 사용하는 방법은 이 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.notion.so/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6?pvs">섹션</a>에서 보여드리겠습니다. &#x3D;21).</p>
<p>또한 이 매개변수가 제대로 설정되었는지 확인하세요. 그렇지 않으면 <code>HuggingFace</code>에서 모델을 가져올 수 없습니다.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;--pretrained_model_name_or_path&quot;: &quot;stabilityai/stable-diffusion-3.5-large&quot;</span><br></pre></td></tr></table></figure>

<p>이것이 작동하는지 확인하려면 ‘HuggingFace’ 계정에 여기 모델 카드 페이지에서 이 모델에 대한 액세스 권한이 부여되었는지 확인해야 합니다. <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/SD3.md">빠른 시작 가이드</a>의 지침을 따르면 됩니다.</p>
<p>다음 명령은 다음과 같습니다.</p>
<p><strong>필수</strong></p>
<p>모델을 다운로드하기 위한 접근 권한을 얻기 위한 것입니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli login</span><br></pre></td></tr></table></figure>

<p>나머지 설정을 다루기 전에 지금 ‘multidatabackend.json’ 파일을 설정하는 것이 좋습니다.</p>
<h3 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h3><p>관련 매개변수를 인간이 이해할 수 있는 어휘로 구문 분석하기 전에 데이터 부분인 <code>--data_backend_config</code> 및 <code>--output_dir</code>부터 시작하고 싶습니다. 이전 버전의 <code>SimpleTuner</code>에는 데이터를 처리하는 <code>multidatabackend.json</code> 파일이 있었습니다.</p>
<p>Excerpt from old code:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> BASE_DIR=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/&quot;</span></span><br><span class="line"><span class="built_in">export</span> DATALOADER_CONFIG=<span class="string">&quot;<span class="variable">$&#123;BASE_DIR&#125;</span>/multidatabackend.json&quot;</span></span><br><span class="line"><span class="built_in">export</span> OUTPUT_DIR=<span class="string">&quot;<span class="variable">$&#123;BASE_DIR&#125;</span>/models&quot;</span></span><br></pre></td></tr></table></figure>

<p>보시다시피 <code>BASE_DIR</code>이 선언된 다음 <code>DATALOADER_CONFIG</code>와 <code>OUTPUT_DIR</code>이 이를 확장합니다. <code>multidatabackend.json</code>은 <code>BASE_DIR</code> 내부에 생성된 파일입니다.</p>
<p>그러나 SimpleTuner의 기본 구성 폴더에는 ‘SimpleTuner&#x2F;config&#x2F;multidatabackend.json’ 파일이 있습니다. 개인 취향에 따라 ‘multidatabackend.json’ 파일을 원하는 곳에 모두 배치할 수 있지만, 모든 모델과 캐시를 한 곳에 보관하므로 이전 버전의 ‘SimpleTuner’ 구조를 보존하겠습니다.</p>
<p>따라서 <code>BASE_DIR</code> 역할을 할 폴더 위치를 생성하겠습니다. 따라서 <code>--data_backend_config</code>와 <code>--output_dir</code> 모두 이 경로를 활용합니다.</p>
<p>우리는 <code>json</code>을 사용하고 있으므로 하드코딩해야 합니다.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;--data_backend_config&quot;: &quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;,</span><br><span class="line"> &quot;--output_dir&quot;: &quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;,</span><br></pre></td></tr></table></figure>


<p>모든 모델은 <code>--output_dir</code>에 저장되며, 이 경우 하드 코딩된 <code>BASE_DIR/models</code>입니다.</p>
<p>다음은 내 사용자 정의 <code>multidatabackend.json</code>입니다.</p>
<ul>
<li><p>Custom <code>multidatabackend.json</code></p>
  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;fantasy_art_neo&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;local&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;crop&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;crop_aspect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;square&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;crop_style&quot;</span><span class="punctuation">:</span> <span class="string">&quot;center&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;resolution&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;maximum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;target_downsample_size&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;area&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;cache_dir_vae&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/vae/sd3/fantasy_art_neo&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;instance_data_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/datasets/SDXL/duplicate_shuffle_01&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;disabled&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;skip_file_discovery&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;caption_strategy&quot;</span><span class="punctuation">:</span> <span class="string">&quot;textfile&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;metadata_backend&quot;</span><span class="punctuation">:</span> <span class="string">&quot;json&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;repeats&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text-embeds&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;local&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;dataset_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text_embeds&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;default&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;cache_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/text/sd3/fantasy_art_neo&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;disabled&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;write_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">128</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>지정해야 하는 디렉터리는 세 개입니다.</p>
<ol>
<li><code>cache_dir_vae</code></li>
</ol>
<p>내 예제 파일에는 다음이 있습니다.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;cache_dir_vae&quot;: &quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/vae/sd3/fantasy_art_neo&quot;</span><br></pre></td></tr></table></figure>


<p>가독성과 명확성을 위해 기본 디렉터리 안에 ‘cache’ 폴더를 넣었습니다.</p>
<ol>
<li><code>instance_dir_vae</code></li>
</ol>
<p>여기에 이미지와 캡션이 포함된 데이터세트가 저장됩니다. 매우 간단합니다.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;instance_data_dir&quot;: &quot;/weka2/home-yeo/datasets/SDXL/duplicate_shuffle_01&quot;</span><br></pre></td></tr></table></figure>

<ol>
<li><code>cache_dir</code></li>
</ol>
<p>위와 동일합니다.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;cache_dir&quot;: &quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/text/sd3/fantasy_art_neo&quot;</span><br></pre></td></tr></table></figure>


<p>나머지 설정은 나에게 그다지 중요하지 않습니다. 저는 이미 이미지를 미리 잘라서 <code>&quot;crop&quot;: false</code>를 설정했습니다.</p>
<p>또한 이전에 다른 교육 리포지토리를 사용해 본 적이 있는지 여부에 따라 익숙할 수도 있고 익숙하지 않을 수도 있는 ‘반복’ 매개변수가 있습니다. 이 내용도 다음 섹션에서 다루겠습니다. 그래서 &#96;&#96;repeats”: 1’을 제가 직접 처리하는 것입니다.</p>
<h3 id="Data-preparation"><a href="#Data-preparation" class="headerlink" title="Data preparation"></a>Data preparation</h3><p>내 데이터 세트의 모든 이미지는 이미 다음 종횡비 및 해상도 중 하나로 미리 잘려져 있습니다.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    (1024, 1024), (1152, 896), (896, 1152), (1216, 832),</span><br><span class="line">    (832, 1216), (1344, 768), (768, 1344), (1472, 704)</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>이미지를 자동으로 미리 자르는 데 도움이 필요한 경우 이를 위해 제가 작성한 경량의 기본 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/kasukanra/autogen_local_LLM/blob/main/Detect_utils.py">스크립트</a>가 있습니다. 다음에 따라 최상의 작물을 찾습니다.</p>
<ol>
<li>이미지에 사람 얼굴이 포함되어 있나요? 그렇다면 이미지의 해당 영역을 중심으로 자르기를 수행합니다.</li>
<li>감지된 사람의 얼굴이 없으면 이미지에서 가장 흥미로운 영역을 감지하는 돌출 맵을 사용하여 자르기를 수행합니다. 그러면 해당 지역을 중심으로 가장 좋은 작물이 추출됩니다.</li>
</ol>
<p>어쨌든 내 기본 데이터 세트 구조는 다음과 같습니다(텍스트 파일은 캡션입니다).</p>
<p>내 캡션이 어떻게 보이는지에 대한 몇 가지 예는 다음과 같습니다.</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">k4s4, a close up portrait view of a young man with green eyes and short dark hair, looking at the viewer with a slight smile, visible ears, wearing a dark jacket, hair bangs, a green and orange background</span><br></pre></td></tr></table></figure>

<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">k4s4, a rear view of a woman wearing a red hood and faded skirt holding a staff in each hand and steering a small boat with small white wings and large white sail towards a city with tall structures, blue sky with white clouds, cropped</span><br></pre></td></tr></table></figure>


<p>자체 미세 조정 데이터 세트가 없다면 John이 그린 그림의 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://drive.google.com/file/d/1capT9kF-zCu2OiNVzm7VG5DQDaAQLl1Q/view?usp=sharing">이 데이터 세트</a>를 자유롭게 사용해 보세요. 가수 Sargent(WikiArt에서 다운로드하고 자동 캡션 있음) 또는 합성 픽셀 아트 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://drive.google.com/file/d/1tOyNsjR5i7ki5UkyxHhjjT_VVD8vK5WN/view?usp=drive_link">데이터세트</a>.</p>
<p>다양한 데이터 세트 크기의 여러 미세 조정된 ‘LoRA’ 모델의 결과를 보여줌으로써 내가 선택한 설정이 ‘LoRA’ 미세 조정을 위한 좋은 출발점이 될 만큼 충분히 일반화된다는 것을 보여줄 것입니다.</p>
<table>
<thead>
<tr>
<th><code>name</code></th>
<th><code>fantasy art</code></th>
<th><code>cinema photo</code></th>
<th><code>john singer sargent</code></th>
<th><code>underexposed photography</code></th>
<th><code>pixel art</code></th>
<th><code>ethnic paint</code></th>
</tr>
</thead>
<tbody><tr>
<td><code>number of images</code></td>
<td>476</td>
<td>460</td>
<td>460</td>
<td>96</td>
<td>82</td>
<td>68</td>
</tr>
<tr>
<td><code>number of repeats</code></td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
</tr>
</tbody></table>
<p><code>반복</code>은 이미지를 복제하고(선택적으로 회전하고, 색조&#x2F;채도 등을 변경하는 등) 캡션도 모델에 일반화하고 과적합을 방지하는 데 도움이 됩니다. <code>SimpleTuner</code>는 캡션 드롭아웃(지정된 시간 비율에 따라 캡션을 무작위로 삭제)을 지원하지만 현재로서는 셔플링 토큰(토큰은 캡션의 단어와 유사함)을 지원하지 않지만 kohya의 동작을 시뮬레이션할 수 있습니다. <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/kohya-ss/sd-scripts">sd-scripts</a> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/kohya-ss/sd-scripts/blob/25f961bc779bc79aef440813e3e8e92244ac5739/">토큰 섞기</a>할 수 있는 곳 docs&#x2F;config_README-en.md?plain&#x3D;1#L146) [유지]하는 동안(<a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/kohya-ss/sd-scripts/blob/25f961bc779bc79aef440813e3e8e92244ac5739/docs/config_README-en.md?plain=1">https://github.com/kohya-ss/sd-scripts/blob/25f961bc779bc79aef440813e3e8e92244ac5739/docs/config_README-en.md?plain=1</a> #L143) 시작 위치에 ‘n’개의 토큰이 있습니다. <strong>이렇게 하면 모델이 외부 토큰에 너무 집착하지 않도록 도와줍니다.</strong></p>
<p>해당 기능을 복제하려면 여기에 이미지를 복제하고 캡션을 조작하는 스크립트를 제공했습니다.</p>
<ul>
<li><p><code>duplicate_shuffle.py</code></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">duplicate_and_shuffle_dataset</span>(<span class="params">input_folder, output_folder, dataset_repeats, n_tokens_to_keep</span>):</span><br><span class="line">    <span class="comment"># Create output folder if it doesn&#x27;t exist</span></span><br><span class="line">    Path(output_folder).mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get all image files</span></span><br><span class="line">    image_files = [f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(input_folder) <span class="keyword">if</span> f.lower().endswith((<span class="string">&#x27;.png&#x27;</span>, <span class="string">&#x27;.jpg&#x27;</span>, <span class="string">&#x27;.jpeg&#x27;</span>))]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(dataset_repeats):</span><br><span class="line">        <span class="keyword">for</span> image_file <span class="keyword">in</span> image_files:</span><br><span class="line">            <span class="comment"># Get corresponding text file</span></span><br><span class="line">            text_file = os.path.splitext(image_file)[<span class="number">0</span>] + <span class="string">&#x27;.txt&#x27;</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.join(input_folder, text_file)):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Warning: No corresponding text file found for <span class="subst">&#123;image_file&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create new file names</span></span><br><span class="line">            new_image_file = <span class="string">f&quot;<span class="subst">&#123;os.path.splitext(image_file)[<span class="number">0</span>]&#125;</span>_<span class="subst">&#123;i+<span class="number">1</span>&#125;</span><span class="subst">&#123;os.path.splitext(image_file)[<span class="number">1</span>]&#125;</span>&quot;</span></span><br><span class="line">            new_text_file = <span class="string">f&quot;<span class="subst">&#123;os.path.splitext(text_file)[<span class="number">0</span>]&#125;</span>_<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>.txt&quot;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Copy image file</span></span><br><span class="line">            shutil.copy2(os.path.join(input_folder, image_file), os.path.join(output_folder, new_image_file))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Read, shuffle, and write text file</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(input_folder, text_file), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                content = f.read().strip()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Split tokens using comma or period as separator</span></span><br><span class="line">            tokens = re.split(<span class="string">r&#x27;[,.]&#x27;</span>, content)</span><br><span class="line">            tokens = [token.strip() <span class="keyword">for</span> token <span class="keyword">in</span> tokens <span class="keyword">if</span> token.strip()]  <span class="comment"># Remove empty tokens and strip whitespace</span></span><br><span class="line"></span><br><span class="line">            tokens_to_keep = tokens[:n_tokens_to_keep]</span><br><span class="line">            tokens_to_shuffle = tokens[n_tokens_to_keep:]</span><br><span class="line">            random.shuffle(tokens_to_shuffle)</span><br><span class="line"></span><br><span class="line">            new_content = <span class="string">&#x27;, &#x27;</span>.join(tokens_to_keep + tokens_to_shuffle)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(output_folder, new_text_file), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(new_content)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Dataset duplication and shuffling complete. Output saved to <span class="subst">&#123;output_folder&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage</span></span><br><span class="line">input_folder = <span class="string">&quot;/weka2/home-yeo/datasets/SDXL/full_dataset_neo&quot;</span></span><br><span class="line">output_folder = <span class="string">&quot;/weka2/home-yeo/datasets/SDXL/duplicate_shuffle_1&quot;</span></span><br><span class="line">dataset_repeats = <span class="number">5</span></span><br><span class="line">n_tokens_to_keep = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">duplicate_and_shuffle_dataset(input_folder, output_folder, dataset_repeats, n_tokens_to_keep)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>그렇게 하면 최종 데이터 세트는 아래 이미지와 비슷해집니다. 제가 사용한 설정으로는 5번의 ‘반복’이 허용되는 것 같았습니다.</p>
<h2 id="Returning-to-the-custom-config"><a href="#Returning-to-the-custom-config" class="headerlink" title="Returning to the custom config"></a>Returning to the custom config</h2><p>이제 사용자 정의 구성에서 이러한 특정 설정을 다루겠습니다.</p>
<h3 id="Learning-rate-x2F-steps"><a href="#Learning-rate-x2F-steps" class="headerlink" title="Learning rate&#x2F;steps"></a>Learning rate&#x2F;steps</h3><ul>
<li><p>Custom SD3.5 Large  <code>config.json</code> for LoRA training</p>
  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">1.05e-3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--instance_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4 &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--metadata_update_interval&quot;</span><span class="punctuation">:</span> <span class="number">65</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--vae_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_unwanted_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_problematic_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--training_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--inference_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--snr_gamma&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--enable_xformers_memory_efficient_attention&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--allow_tf32&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--use_ema&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--ema_decay&quot;</span><span class="punctuation">:</span> <span class="number">0.999</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_alpha&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>이제 사용자 정의 구성에서 이러한 설정을 다루겠습니다.</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">1.05e-3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="Steps-calculation"><a href="#Steps-calculation" class="headerlink" title="Steps calculation"></a>Steps calculation</h3><p>최대 훈련 단계는 간단한 수학 방정식을 기반으로 계산할 수 있습니다(<strong>단일 개념</strong>의 경우).</p>
<p>$$<br>\text{Max training steps} &#x3D; \left(\frac{\text{Number of samples} \times \text{Repeats}}{\text{Batch size}}\right) \times \text{Epochs}<br>$$</p>
<p>여기에는 네 가지 변수가 있습니다.</p>
<ul>
<li>배치 크기: 한 번의 반복으로 처리되는 샘플 수입니다.</li>
<li>샘플 수: 데이터 세트의 총 샘플 수입니다.</li>
<li>반복 횟수: 한 에포크 내에서 데이터 세트를 반복하는 횟수입니다.</li>
<li>Epochs: 전체 데이터세트가 처리되는 횟수입니다.</li>
</ul>
<p>‘fantasy art’ 데이터세트에는 ‘476’ 이미지가 있습니다. <code>multidatabackend.json</code>의 <code>5</code> 반복 위에 추가합니다. 나는 두 가지 이유로 <code>train_batch_size</code>를 <code>6</code>으로 선택했습니다:</p>
<ol>
<li>이 값을 사용하면 진행률 표시줄이 1~2초마다 업데이트되는 것을 볼 수 있습니다.</li>
<li>한 번의 반복으로 ‘6’개의 샘플을 취할 수 있을 만큼 충분히 크므로 훈련 과정에서 더 많은 일반화가 이루어지도록 합니다.</li>
</ol>
<p>30개 정도의 에포크를 원했다면 최종 계산은 다음과 같습니다.</p>
<p>$$<br>\text{Max training steps} &#x3D; \left(\frac{\text{476} \times \text{5}}{\text{6}}\right) \times \text{30}<br>$$</p>
<p>이는 대략 ‘11,900’ 단계와 같습니다.</p>
<p>괄호 안의 부분:</p>
<p>$$<br>\left(\frac{\text{476} \times \text{5}}{\text{6}}\right)<br>$$</p>
<p>는 에포크당 단계 수, 즉 ‘396’을 나타냅니다.</p>
<p>따라서 <code>CHECKPOINTING_STEPS</code>에 대해 이 값을 <code>400</code>으로 반올림했습니다.</p>
<p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://emojipedia.org/warning">**⚠️</a>** <code>MAX_NUM_STEPS</code>에 대해 <code>11,900</code>을 계산했지만 결국 <code>24,000</code>으로 설정했습니다. LoRA 훈련 샘플을 더 보고 싶었습니다. 따라서 원래 ‘11,900’ 이후의 모든 값은 내가 과도한 훈련을 했는지 여부에 대한 좋은 척도가 될 것입니다. 그래서 총 단계 <code>11,900</code> x <code>2</code> &#x3D; <code>23,800</code>을 두 배로 늘린 다음 반올림했습니다.</p>
<p><code>CHECKPOINTING_STEPS</code>는 모델 체크포인트를 저장하려는 빈도를 나타냅니다. ‘400’으로 설정하는 것은 제게는 한 시대에 꽤 가깝기 때문에 괜찮아 보였습니다.</p>
<p><code>CHECKPOINTING_LIMIT</code>은 이전 체크포인트를 덮어쓰기 전에 저장하려는 체크포인트 수입니다. 제 경우에는 체크포인트를 모두 유지하고 싶어서 ‘60’처럼 높은 숫자로 제한을 두었습니다.</p>
<h3 id="Multiple-concepts"><a href="#Multiple-concepts" class="headerlink" title="Multiple concepts"></a>Multiple concepts</h3><p>The above example is trained on a single concept with one unifying trigger word at the beginning: <code>k4s4</code>. However, if your dataset has multiple concepts&#x2F;trigger words, then your step calculation could be something like this so:</p>
<p><code>2</code> concepts <code>[a, b]</code></p>
<p>$$<br>\text{Max steps} &#x3D; \left(\frac{N_a \times R_a + N_b \times R_b}{\text{Batch size}}\right) \times \text{Epochs}<br>$$</p>
<p><code>i</code> concepts</p>
<p>$$<br>\text{Max steps} &#x3D; \left(\frac{\sum_{i \in C} N_i \times R_i}{\text{Batch size}}\right) \times \text{Epochs}<br>$$</p>
<p>마지막으로 학습률의 경우 ‘1.5e-3’으로 설정했습니다. 더 높을수록 기울기가 다음과 같이 폭발하기 때문입니다.</p>
<p>다른 관련 설정은 ‘LoRA’와 관련이 있습니다.</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_alpha&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>


<p>개인적으로는 좀 더 높은 ‘LoRA’ 랭크와 알파를 사용해 아주 만족스러운 결과를 얻었습니다. 내 YouTube <a target="_blank" rel="external nofollow noopener noreferrer" href="https://youtube.com/@kasukanra">채널</a>에서 ‘LoRA’ 순위를 높일수록 이미지 충실도가 어떻게 증가하는지에 대한 보다 정확한 경험적 분석을 보려면 최신 동영상을 시청할 수 있습니다. .</p>
<p>어쨌든 VRAM, 저장 용량 또는 그렇게 높아질 시간이 없다면 ‘256’ 또는 ‘128’과 같이 더 낮은 값을 선택할 수 있습니다.</p>
<p><code>lora_type</code>에 관해서는, 나는 시도되고 진실된 <code>standard</code>를 사용하겠습니다. ‘LoRA’의 ‘lycoris’ 유형에 대한 또 다른 옵션이 있지만 아직은 매우 실험적이며 잘 탐색되지 않았습니다. 나는 ‘lycoris’에 대해 직접 심층 분석했지만 만족스러운 결과를 얻을 수 있는 올바른 설정을 찾지 못했습니다.</p>
<h3 id="Custom-config-json-miscellaneous"><a href="#Custom-config-json-miscellaneous" class="headerlink" title="Custom config.json miscellaneous"></a>Custom <code>config.json</code> miscellaneous</h3><p>삶의 질을 위해 변경할 수 있는 몇 가지 추가 설정이 있습니다.</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p><code>&quot;--validation_prompt&quot;: &quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</code></p>
<p><code>&quot;--validation_guidance&quot;: 7.5</code><br><code>&quot;--validation_steps&quot;: 200</code><br><code>&quot;--validation_num_inference_steps&quot;: 30</code><br><code>&quot;--validation_negative_prompt&quot;: &quot;blurry, cropped, ugly&quot;</code></p>
<p><code>&quot;--lr_scheduler&quot;: &quot;cosine&quot;</code></p>
<p><code>&quot;--lr_warmup_steps&quot;: 2400</code></p>
<p>이것들은 매우 자명합니다:</p>
<p><code>&quot;--validation_prompt&quot;</code></p>
<p>검증 이미지를 생성하는 데 사용할 프롬프트입니다. 이것이 당신의 긍정적인 메시지입니다.</p>
<p><code>&quot;--validation_negative_prompt&quot;</code></p>
<p>부정적인 프롬프트.</p>
<p><code>&quot;--validation_guidance&quot;</code></p>
<p>Classifier free guidance (CFG) scale.</p>
<p><code>&quot;--validation_num_inference_steps&quot;</code></p>
<p>사용할 샘플링 단계 수입니다.</p>
<p><code>&quot;--validation_seed&quot;</code></p>
<p>검증 이미지 생성 시 시드 값입니다.</p>
<p><code>&quot;--lr_warmup_steps&quot;</code></p>
<p>‘SimpleTuner’는 설정하지 않을 경우 기본 워밍업을 전체 훈련 단계의 ‘10%’로 설정했는데, 이는 제가 자주 사용하는 값입니다. 그래서 (<code>24,000</code> * <code>0.1</code> &#x3D; <code>2,400</code>)에 하드코딩했습니다. 자유롭게 변경해 보세요.</p>
<p><code>&quot;--validation_steps&quot;</code></p>
<p>검증 이미지를 생성하려는 빈도는 <code>&quot;--validation_steps&quot;</code>로 설정됩니다. 저는 400의 1&#x2F;2인 200으로 설정했습니다(판타지 아트 예제 데이터세트에 대한 한 시대의 단계 수). 이는 에포크의 1&#x2F;2마다 검증 이미지를 생성한다는 의미입니다. 온전한 확인을 위해 최소한 반기점마다 검증 이미지를 생성하는 것이 좋습니다. 그렇지 않으면 최대한 빨리 오류를 포착하지 못할 수도 있습니다.</p>
<p>마지막으로 <code>&quot;--lr_scheduler&quot;</code>와 <code>&quot;--lr_warmup_steps&quot;</code>입니다.</p>
<p>저는 ‘코사인’ 스케줄러를 사용했습니다. 다음과 같은 모습입니다.</p>
<h3 id="What-happened-to-the-low-level-config-env"><a href="#What-happened-to-the-low-level-config-env" class="headerlink" title="What happened to the low-level config.env ?"></a>What happened to the low-level <code>config.env</code> ?</h3><p>앞서 언급했듯이 <code>SimpleTuner</code>는 낮은 수준의 <code>config.env</code> 형식에서 벗어나 사용 편의성을 위해 <code>json</code>을 선택하는 것으로 보입니다. 대부분의 다른 교육 리포지토리도 <code>json</code>을 사용합니다.</p>
<p>그러나 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/helpers/configuration/loader.py#L17">loader.py</a>의 코드를 기반으로 하위 수준 <code>config.env</code>는 계속 지원됩니다. . 또한 이전의 낮은 수준 <code>config.env</code> 파일이 이미 있는 <code>SimpleTuner</code>의 이전 사용자는 파일 형식을 전환하지 않고도 일부 매개변수를 조정하여 신속하게 속도를 얻을 수 있습니다(해당 [OPTIONS.MD](https &#x2F;&#x2F;github.com&#x2F;bghira&#x2F;SimpleTuner&#x2F;blob&#x2F;main&#x2F;OPTIONS.md#environment-configuration-variables)).</p>
<h2 id="이는-위의-config-json과-동일한-버전이지만-env-형식입니다"><a href="#이는-위의-config-json과-동일한-버전이지만-env-형식입니다" class="headerlink" title="이는 위의 config.json과 동일한 버전이지만 .env 형식입니다."></a>이는 위의 <code>config.json</code>과 동일한 버전이지만 <code>.env</code> 형식입니다.</h2><ul>
<li><p>Custom SD3.5 Large <code>LoRA</code> <code>config.env</code></p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> MODEL_TYPE=<span class="string">&#x27;lora&#x27;</span></span><br><span class="line"><span class="built_in">export</span> MODEL_FAMILY=<span class="string">&#x27;sd3&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONTROLNET=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> USE_DORA=<span class="literal">false</span></span><br><span class="line"><span class="comment"># Restart where we left off. Change this to &quot;checkpoint-1234&quot; to start from a specific checkpoint.</span></span><br><span class="line"><span class="built_in">export</span> RESUME_CHECKPOINT=<span class="string">&quot;latest&quot;</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_STEPS=400</span><br><span class="line"><span class="comment"># This is how many checkpoints we will keep. Two is safe, but three is safer.</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_LIMIT=60</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is decided as a relatively conservative &#x27;constant&#x27; learning rate.</span></span><br><span class="line"><span class="comment"># Adjust higher or lower depending on how burnt your model becomes.</span></span><br><span class="line"><span class="built_in">export</span> LEARNING_RATE=1.05e-3</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using a Huggingface Hub model:</span></span><br><span class="line"><span class="built_in">export</span> MODEL_NAME=<span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make DEBUG_EXTRA_ARGS empty to disable wandb.</span></span><br><span class="line"><span class="built_in">export</span> DEBUG_EXTRA_ARGS=<span class="string">&quot;--report_to=wandb&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_PROJECT_NAME=<span class="string">&quot;sd35-training&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_RUN_NAME=<span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Max number of steps OR epochs can be used. Not both.</span></span><br><span class="line"><span class="built_in">export</span> MAX_NUM_STEPS=24000</span><br><span class="line"><span class="built_in">export</span> NUM_EPOCHS=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># A convenient prefix for all of your training paths.</span></span><br><span class="line"><span class="built_in">export</span> DATALOADER_CONFIG=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span></span><br><span class="line"><span class="built_in">export</span> OUTPUT_DIR=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span></span><br><span class="line"><span class="comment"># Set this to &quot;true&quot; to push your model to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_TO_HUB=<span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="comment"># If PUSH_TO_HUB and PUSH_CHECKPOINTS are both enabled, every saved checkpoint will be pushed to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_CHECKPOINTS=<span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="comment"># This will be the model name for your final hub upload, eg. &quot;yourusername/yourmodelname&quot;</span></span><br><span class="line"><span class="comment"># It defaults to the wandb project name, but you can override this here.</span></span><br><span class="line"><span class="comment"># export HUB_MODEL_NAME=$TRACKER_PROJECT_NAME</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, images will be resized so their SMALLER EDGE is 1024 pixels, maintaining aspect ratio.</span></span><br><span class="line"><span class="comment"># Setting this value to 768px might result in more reasonable training data sizes for SDXL.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION=1024</span><br><span class="line"><span class="comment"># If you want to have the training data resized by pixel area (Megapixels) rather than edge length,</span></span><br><span class="line"><span class="comment">#  set this value to &quot;area&quot; instead of &quot;pixel&quot;, and uncomment the next RESOLUTION declaration.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION_TYPE=<span class="string">&quot;pixel&quot;</span></span><br><span class="line"><span class="comment">#export RESOLUTION=1          # 1.0 Megapixel training sizes</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;pixel&quot;, the minimum resolution specifies the smaller edge length, measured in pixels. Recommended: 1024.</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;area&quot;, the minimum resolution specifies the total image area, measured in megapixels. Recommended: 1.</span></span><br><span class="line"><span class="built_in">export</span> MINIMUM_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># How many decimals to round aspect buckets to.</span></span><br><span class="line"><span class="comment">#export ASPECT_BUCKET_ROUNDING=2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use this to append an instance prompt to each caption, used for adding trigger words.</span></span><br><span class="line"><span class="comment"># This has not been tested in SDXL.</span></span><br><span class="line"><span class="built_in">export</span> INSTANCE_PROMPT=<span class="string">&quot;k4s4 &quot;</span></span><br><span class="line"><span class="comment"># If you also supply a user prompt library or `--use_prompt_library`, this will be added to those lists.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_PROMPT=<span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE=7.5</span><br><span class="line"><span class="comment"># You&#x27;ll want to set this to 0.7 if you are training a terminal SNR model.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE_RESCALE=0.0</span><br><span class="line"><span class="comment"># How frequently we will save and run a pipeline for validations.</span></span><br><span class="line"><span class="comment"># export VALIDATION_STEPS=200</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_STEPS=70</span><br><span class="line"><span class="built_in">export</span> VALIDATION_NUM_INFERENCE_STEPS=30</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> VALIDATION_NEGATIVE_PROMPT=<span class="string">&quot;blurry, cropped, ugly&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_SEED=42</span><br><span class="line"><span class="built_in">export</span> VALIDATION_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adjust this for your GPU memory size. This, and resolution, are the biggest VRAM killers.</span></span><br><span class="line"><span class="built_in">export</span> TRAIN_BATCH_SIZE=6</span><br><span class="line"><span class="comment"># Accumulate your update gradient over many steps, to save VRAM while still having higher effective batch size:</span></span><br><span class="line"><span class="comment"># effective batch size = ($TRAIN_BATCH_SIZE * $GRADIENT_ACCUMULATION_STEPS).</span></span><br><span class="line"><span class="built_in">export</span> GRADIENT_ACCUMULATION_STEPS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use any standard scheduler type. constant, polynomial, constant_with_warmup</span></span><br><span class="line"><span class="built_in">export</span> LR_SCHEDULE=<span class="string">&quot;cosine&quot;</span></span><br><span class="line"><span class="comment"># A warmup period allows the model and the EMA weights more importantly to familiarise itself with the current quanta.</span></span><br><span class="line"><span class="comment"># For the cosine or sine type schedules, the warmup period defines the interval between peaks or valleys.</span></span><br><span class="line"><span class="comment"># Use a sine schedule to simulate a warmup period, or a Cosine period to simulate a polynomial start.</span></span><br><span class="line"><span class="comment"># export LR_WARMUP_STEPS=$((MAX_NUM_STEPS / 10))</span></span><br><span class="line"><span class="built_in">export</span> LR_WARMUP_STEPS=2400</span><br><span class="line"></span><br><span class="line"><span class="comment"># Caption dropout probability. Set to 0.1 for 10% of captions dropped out. Set to 0 to disable.</span></span><br><span class="line"><span class="comment"># You may wish to disable dropout if you want to limit your changes strictly to the prompts you show the model.</span></span><br><span class="line"><span class="comment"># You may wish to increase the rate of dropout if you want to more broadly adopt your changes across the model.</span></span><br><span class="line"><span class="built_in">export</span> CAPTION_DROPOUT_PROBABILITY=0</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> METADATA_UPDATE_INTERVAL=65</span><br><span class="line"><span class="built_in">export</span> VAE_BATCH_SIZE=12</span><br><span class="line"></span><br><span class="line"><span class="comment"># If this is set, any images that fail to open will be DELETED to avoid re-checking them every time.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_ERRORED_IMAGES=0</span><br><span class="line"><span class="comment"># If this is set, any images that are too small for the minimum resolution size will be DELETED.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_SMALL_IMAGES=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bytedance recommends these be set to &quot;trailing&quot; so that inference and training behave in a more congruent manner.</span></span><br><span class="line"><span class="comment"># To follow the original SDXL training strategy, use &quot;leading&quot; instead, though results are generally worse.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"><span class="built_in">export</span> INFERENCE_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing this option or unsetting it uses vanilla training. Setting it reweights the loss by the position of the timestep in the noise schedule.</span></span><br><span class="line"><span class="comment"># A value &quot;5&quot; is recommended by the researchers. A value of &quot;20&quot; is the least impact, and &quot;1&quot; is the most impact.</span></span><br><span class="line"><span class="built_in">export</span> MIN_SNR_GAMMA=5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set this to an explicit value of &quot;false&quot; to disable Xformers. Probably required for AMD users.</span></span><br><span class="line"><span class="built_in">export</span> USE_XFORMERS=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># There&#x27;s basically no reason to unset this. However, to disable it, use an explicit value of &quot;false&quot;.</span></span><br><span class="line"><span class="comment"># This will save a lot of memory consumption when enabled.</span></span><br><span class="line"><span class="built_in">export</span> USE_GRADIENT_CHECKPOINTING=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Options below here may require a bit more complicated configuration, so they are not simple variables.</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TF32 is great on Ampere or Ada, not sure about earlier generations.</span></span><br><span class="line"><span class="built_in">export</span> ALLOW_TF32=<span class="literal">true</span></span><br><span class="line"><span class="comment"># AdamW 8Bit is a robust and lightweight choice. Adafactor might reduce memory consumption, and Dadaptation is slow and experimental.</span></span><br><span class="line"><span class="comment"># AdamW is the default optimizer, but it uses a lot of memory and is slower than AdamW8Bit or Adafactor.</span></span><br><span class="line"><span class="comment"># Choices: adamw, adamw8bit, adafactor, dadaptation</span></span><br><span class="line"><span class="comment"># export OPTIMIZER=&quot;adamw_bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> OPTIMIZER=<span class="string">&quot;adamw_bf16&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># EMA is a strong regularisation method that uses a lot of extra VRAM to hold two copies of the weights.</span></span><br><span class="line"><span class="comment"># This is worthwhile on large training runs, but not so much for smaller training runs.</span></span><br><span class="line"><span class="built_in">export</span> USE_EMA=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> EMA_DECAY=0.999</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TRAINER_EXTRA_ARGS=<span class="string">&quot;--lora_rank=768 --lora_alpha=768&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reproducible training. Set to -1 to disable.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SEED=42</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mixed precision is the best. You honestly might need to YOLO it in fp16 mode for Google Colab type setups.</span></span><br><span class="line"><span class="built_in">export</span> MIXED_PRECISION=<span class="string">&quot;bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> PURE_BF16=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This has to be changed if you&#x27;re training with multiple GPUs.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_PROCESSES=1</span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_MACHINES=1</span><br><span class="line"><span class="built_in">export</span> ACCELERATE_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># With Pytorch 2.1, you might have pretty good luck here.</span></span><br><span class="line"><span class="comment"># If you&#x27;re using aspect bucketing however, each resolution change will recompile. Seriously, just don&#x27;t do it.</span></span><br><span class="line"><span class="comment"># Well, then again... Pytorch 2.2 has support for dynamic shapes. Why not?</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TOKENIZERS_PARALLELISM=<span class="literal">false</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://emojipedia.org/index-pointing-up">**☝️</a>** <code>LoRA</code> 순위&#x2F;알파는 <code>TRAINER_EXTRA_ARGS</code> 변수 내에서 변경될 수 있다는 점을 지적하고 싶습니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> TRAINER_EXTRA_ARGS=<span class="string">&quot;--lora_rank=768 --lora_alpha=768&quot;</span></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://emojipedia.org/warning">**⚠️</a>** <code>.env</code> 형식을 사용하기로 결정한 경우 인라인 주석, 참조 변수 또는 계산이 없는지 확인하세요. 이것은 새로운 <code>SimpleTuner</code> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/bghira/SimpleTuner/blob/main/helpers/configuration/env_file.py#L94">env 도우미</a>가 작동하는 방식이므로 모든 것을 하드 코딩해야 합니다. . ****예를 들어:</p>
<p><strong>Failure case 1 (inline comments):</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LEARNING_RATE=1.05e-3 <span class="comment">#@param &#123;type:&quot;number&quot;&#125;</span></span><br></pre></td></tr></table></figure>

<p><strong>Failure case 2 (reference variable with <code>TRAINER_EXTRA_ARGS</code>):</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> TRAINER_EXTRA_ARGS=<span class="string">&quot;<span class="variable">$&#123;TRAINER_EXTRA_ARGS&#125;</span> --offset_noise --noise_offset=0.02&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>Failure case 3 (calculations)</strong>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LR_WARMUP_STEPS=$((MAX_NUM_STEPS / <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<p>원하는 경우 위의 하위 수준 <code>config.env</code>를 기본 참조로 사용할 수 있습니다. 하위 수준 <code>env</code> 파일을 사용하기로 결정한 경우 상위 수준 <code>config.env</code>에서 <code>CONFIG_BACKEND</code>를 <code>env</code>로 변경하는 것을 잊지 마세요.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_NUM_PROCESSES=1</span><br><span class="line">TRAINING_NUM_MACHINES=1</span><br><span class="line">TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line">MIXED_PRECISION=<span class="string">&#x27;bf16&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line"><span class="built_in">export</span> ENV=<span class="string">&quot;sd35_fantasy_art_lora&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="Training-process"><a href="#Training-process" class="headerlink" title="Training process"></a>Training process</h2><p>마지막으로 훈련 과정을 시작할 수 있습니다. 참고용으로 필요한 모든 파일을 여기에 가져오겠습니다.</p>
<ul>
<li><p>High-level <code>config.env</code></p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_NUM_PROCESSES=1</span><br><span class="line">TRAINING_NUM_MACHINES=1</span><br><span class="line">TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line">MIXED_PRECISION=<span class="string">&#x27;bf16&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line"><span class="built_in">export</span> ENV=<span class="string">&quot;sd35_fantasy_art_lora&quot;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Custom SD3.5 Large <code>config.json</code> for LoRA training</p>
  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">1.05e-3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--instance_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4 &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--metadata_update_interval&quot;</span><span class="punctuation">:</span> <span class="number">65</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--vae_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_unwanted_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_problematic_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--training_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--inference_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--snr_gamma&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--enable_xformers_memory_efficient_attention&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--allow_tf32&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--use_ema&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--ema_decay&quot;</span><span class="punctuation">:</span> <span class="number">0.999</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_alpha&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Custom SD3.5 Large  <code>config.env</code> for LoRA training</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="built_in">export</span> MODEL_TYPE=<span class="string">&#x27;lora&#x27;</span></span><br><span class="line"><span class="built_in">export</span> MODEL_FAMILY=<span class="string">&#x27;sd3&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONTROLNET=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> USE_DORA=<span class="literal">false</span></span><br><span class="line"><span class="comment"># Restart where we left off. Change this to &quot;checkpoint-1234&quot; to start from a specific checkpoint.</span></span><br><span class="line"><span class="built_in">export</span> RESUME_CHECKPOINT=<span class="string">&quot;latest&quot;</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_STEPS=400</span><br><span class="line"><span class="comment"># This is how many checkpoints we will keep. Two is safe, but three is safer.</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_LIMIT=60</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is decided as a relatively conservative &#x27;constant&#x27; learning rate.</span></span><br><span class="line"><span class="comment"># Adjust higher or lower depending on how burnt your model becomes.</span></span><br><span class="line"><span class="built_in">export</span> LEARNING_RATE=1.05e-3</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using a Huggingface Hub model:</span></span><br><span class="line"><span class="built_in">export</span> MODEL_NAME=<span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make DEBUG_EXTRA_ARGS empty to disable wandb.</span></span><br><span class="line"><span class="built_in">export</span> DEBUG_EXTRA_ARGS=<span class="string">&quot;--report_to=wandb&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_PROJECT_NAME=<span class="string">&quot;sd35-training&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_RUN_NAME=<span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Max number of steps OR epochs can be used. Not both.</span></span><br><span class="line"><span class="built_in">export</span> MAX_NUM_STEPS=24000</span><br><span class="line"><span class="built_in">export</span> NUM_EPOCHS=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># A convenient prefix for all of your training paths.</span></span><br><span class="line"><span class="built_in">export</span> DATALOADER_CONFIG=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span></span><br><span class="line"><span class="built_in">export</span> OUTPUT_DIR=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span></span><br><span class="line"><span class="comment"># Set this to &quot;true&quot; to push your model to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_TO_HUB=<span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="comment"># If PUSH_TO_HUB and PUSH_CHECKPOINTS are both enabled, every saved checkpoint will be pushed to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_CHECKPOINTS=<span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="comment"># This will be the model name for your final hub upload, eg. &quot;yourusername/yourmodelname&quot;</span></span><br><span class="line"><span class="comment"># It defaults to the wandb project name, but you can override this here.</span></span><br><span class="line"><span class="comment"># export HUB_MODEL_NAME=$TRACKER_PROJECT_NAME</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, images will be resized so their SMALLER EDGE is 1024 pixels, maintaining aspect ratio.</span></span><br><span class="line"><span class="comment"># Setting this value to 768px might result in more reasonable training data sizes for SDXL.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION=1024</span><br><span class="line"><span class="comment"># If you want to have the training data resized by pixel area (Megapixels) rather than edge length,</span></span><br><span class="line"><span class="comment">#  set this value to &quot;area&quot; instead of &quot;pixel&quot;, and uncomment the next RESOLUTION declaration.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION_TYPE=<span class="string">&quot;pixel&quot;</span></span><br><span class="line"><span class="comment">#export RESOLUTION=1          # 1.0 Megapixel training sizes</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;pixel&quot;, the minimum resolution specifies the smaller edge length, measured in pixels. Recommended: 1024.</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;area&quot;, the minimum resolution specifies the total image area, measured in megapixels. Recommended: 1.</span></span><br><span class="line"><span class="built_in">export</span> MINIMUM_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># How many decimals to round aspect buckets to.</span></span><br><span class="line"><span class="comment">#export ASPECT_BUCKET_ROUNDING=2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use this to append an instance prompt to each caption, used for adding trigger words.</span></span><br><span class="line"><span class="comment"># This has not been tested in SDXL.</span></span><br><span class="line"><span class="built_in">export</span> INSTANCE_PROMPT=<span class="string">&quot;k4s4 &quot;</span></span><br><span class="line"><span class="comment"># If you also supply a user prompt library or `--use_prompt_library`, this will be added to those lists.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_PROMPT=<span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE=7.5</span><br><span class="line"><span class="comment"># You&#x27;ll want to set this to 0.7 if you are training a terminal SNR model.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE_RESCALE=0.0</span><br><span class="line"><span class="comment"># How frequently we will save and run a pipeline for validations.</span></span><br><span class="line"><span class="comment"># export VALIDATION_STEPS=200</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_STEPS=70</span><br><span class="line"><span class="built_in">export</span> VALIDATION_NUM_INFERENCE_STEPS=30</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> VALIDATION_NEGATIVE_PROMPT=<span class="string">&quot;blurry, cropped, ugly&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_SEED=42</span><br><span class="line"><span class="built_in">export</span> VALIDATION_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adjust this for your GPU memory size. This, and resolution, are the biggest VRAM killers.</span></span><br><span class="line"><span class="built_in">export</span> TRAIN_BATCH_SIZE=6</span><br><span class="line"><span class="comment"># Accumulate your update gradient over many steps, to save VRAM while still having higher effective batch size:</span></span><br><span class="line"><span class="comment"># effective batch size = ($TRAIN_BATCH_SIZE * $GRADIENT_ACCUMULATION_STEPS).</span></span><br><span class="line"><span class="built_in">export</span> GRADIENT_ACCUMULATION_STEPS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use any standard scheduler type. constant, polynomial, constant_with_warmup</span></span><br><span class="line"><span class="built_in">export</span> LR_SCHEDULE=<span class="string">&quot;cosine&quot;</span></span><br><span class="line"><span class="comment"># A warmup period allows the model and the EMA weights more importantly to familiarise itself with the current quanta.</span></span><br><span class="line"><span class="comment"># For the cosine or sine type schedules, the warmup period defines the interval between peaks or valleys.</span></span><br><span class="line"><span class="comment"># Use a sine schedule to simulate a warmup period, or a Cosine period to simulate a polynomial start.</span></span><br><span class="line"><span class="comment"># export LR_WARMUP_STEPS=$((MAX_NUM_STEPS / 10))</span></span><br><span class="line"><span class="built_in">export</span> LR_WARMUP_STEPS=2400</span><br><span class="line"></span><br><span class="line"><span class="comment"># Caption dropout probability. Set to 0.1 for 10% of captions dropped out. Set to 0 to disable.</span></span><br><span class="line"><span class="comment"># You may wish to disable dropout if you want to limit your changes strictly to the prompts you show the model.</span></span><br><span class="line"><span class="comment"># You may wish to increase the rate of dropout if you want to more broadly adopt your changes across the model.</span></span><br><span class="line"><span class="built_in">export</span> CAPTION_DROPOUT_PROBABILITY=0</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> METADATA_UPDATE_INTERVAL=65</span><br><span class="line"><span class="built_in">export</span> VAE_BATCH_SIZE=12</span><br><span class="line"></span><br><span class="line"><span class="comment"># If this is set, any images that fail to open will be DELETED to avoid re-checking them every time.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_ERRORED_IMAGES=0</span><br><span class="line"><span class="comment"># If this is set, any images that are too small for the minimum resolution size will be DELETED.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_SMALL_IMAGES=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bytedance recommends these be set to &quot;trailing&quot; so that inference and training behave in a more congruent manner.</span></span><br><span class="line"><span class="comment"># To follow the original SDXL training strategy, use &quot;leading&quot; instead, though results are generally worse.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"><span class="built_in">export</span> INFERENCE_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing this option or unsetting it uses vanilla training. Setting it reweights the loss by the position of the timestep in the noise schedule.</span></span><br><span class="line"><span class="comment"># A value &quot;5&quot; is recommended by the researchers. A value of &quot;20&quot; is the least impact, and &quot;1&quot; is the most impact.</span></span><br><span class="line"><span class="built_in">export</span> MIN_SNR_GAMMA=5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set this to an explicit value of &quot;false&quot; to disable Xformers. Probably required for AMD users.</span></span><br><span class="line"><span class="built_in">export</span> USE_XFORMERS=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># There&#x27;s basically no reason to unset this. However, to disable it, use an explicit value of &quot;false&quot;.</span></span><br><span class="line"><span class="comment"># This will save a lot of memory consumption when enabled.</span></span><br><span class="line"><span class="built_in">export</span> USE_GRADIENT_CHECKPOINTING=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Options below here may require a bit more complicated configuration, so they are not simple variables.</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TF32 is great on Ampere or Ada, not sure about earlier generations.</span></span><br><span class="line"><span class="built_in">export</span> ALLOW_TF32=<span class="literal">true</span></span><br><span class="line"><span class="comment"># AdamW 8Bit is a robust and lightweight choice. Adafactor might reduce memory consumption, and Dadaptation is slow and experimental.</span></span><br><span class="line"><span class="comment"># AdamW is the default optimizer, but it uses a lot of memory and is slower than AdamW8Bit or Adafactor.</span></span><br><span class="line"><span class="comment"># Choices: adamw, adamw8bit, adafactor, dadaptation</span></span><br><span class="line"><span class="comment"># export OPTIMIZER=&quot;adamw_bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> OPTIMIZER=<span class="string">&quot;adamw_bf16&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># EMA is a strong regularisation method that uses a lot of extra VRAM to hold two copies of the weights.</span></span><br><span class="line"><span class="comment"># This is worthwhile on large training runs, but not so much for smaller training runs.</span></span><br><span class="line"><span class="built_in">export</span> USE_EMA=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> EMA_DECAY=0.999</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TRAINER_EXTRA_ARGS=<span class="string">&quot;--lora_rank=768 --lora_alpha=768&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reproducible training. Set to -1 to disable.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SEED=42</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mixed precision is the best. You honestly might need to YOLO it in fp16 mode for Google Colab type setups.</span></span><br><span class="line"><span class="built_in">export</span> MIXED_PRECISION=<span class="string">&quot;bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> PURE_BF16=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This has to be changed if you&#x27;re training with multiple GPUs.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_PROCESSES=1</span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_MACHINES=1</span><br><span class="line"><span class="built_in">export</span> ACCELERATE_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># With Pytorch 2.1, you might have pretty good luck here.</span></span><br><span class="line"><span class="comment"># If you&#x27;re using aspect bucketing however, each resolution change will recompile. Seriously, just don&#x27;t do it.</span></span><br><span class="line"><span class="comment"># Well, then again... Pytorch 2.2 has support for dynamic shapes. Why not?</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TOKENIZERS_PARALLELISM=<span class="literal">false</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Default <a target="_blank" rel="external nofollow noopener noreferrer" href="http://train.sh/">train.sh</a></p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pull config from config.env</span></span><br><span class="line">[ -f <span class="string">&quot;config/config.env&quot;</span> ] &amp;&amp; <span class="built_in">source</span> config/config.env</span><br><span class="line"></span><br><span class="line"><span class="comment"># If the user has not provided VENV_PATH, we will assume $(pwd)/.venv</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;VENV_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="comment"># what if we have VIRTUAL_ENV? use that instead</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;VIRTUAL_ENV&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> VENV_PATH=<span class="string">&quot;<span class="variable">$&#123;VIRTUAL_ENV&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">export</span> VENV_PATH=<span class="string">&quot;<span class="subst">$(pwd)</span>/.venv&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;DISABLE_LD_OVERRIDE&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> NVJITLINK_PATH=<span class="string">&quot;<span class="subst">$(find <span class="string">&quot;<span class="variable">$&#123;VENV_PATH&#125;</span>&quot;</span> -name nvjitlink -type d)</span>/lib&quot;</span></span><br><span class="line">    <span class="comment"># if it&#x27;s not empty, we will add it to LD_LIBRARY_PATH at the front:</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;NVJITLINK_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">&quot;<span class="variable">$&#123;NVJITLINK_PATH&#125;</span>:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TOKENIZERS_PARALLELISM=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> PLATFORM</span><br><span class="line">PLATFORM=$(<span class="built_in">uname</span> -s)</span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$PLATFORM</span>&quot;</span> == <span class="string">&quot;Darwin&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> MIXED_PRECISION=<span class="string">&quot;no&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_EXTRA_ARGS&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_PROCESSES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Set custom env vars permanently in config/config.env:&quot;</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_NUM_PROCESSES not set, defaulting to 1.\n&quot;</span></span><br><span class="line">    TRAINING_NUM_PROCESSES=1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_MACHINES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_NUM_MACHINES not set, defaulting to 1.\n&quot;</span></span><br><span class="line">    TRAINING_NUM_MACHINES=1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;MIXED_PRECISION&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;MIXED_PRECISION not set, defaulting to bf16.\n&quot;</span></span><br><span class="line">    MIXED_PRECISION=bf16</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_DYNAMO_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_DYNAMO_BACKEND not set, defaulting to no.\n&quot;</span></span><br><span class="line">    TRAINING_DYNAMO_BACKEND=<span class="string">&quot;no&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;ENV&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;ENV not set, defaulting to default.\n&quot;</span></span><br><span class="line">    <span class="built_in">export</span> ENV=<span class="string">&quot;default&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">export</span> ENV_PATH=<span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$ENV</span>&quot;</span> != <span class="string">&quot;default&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> ENV_PATH=<span class="string">&quot;<span class="variable">$&#123;ENV&#125;</span>/&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;CONFIG_TYPE&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;<span class="variable">$&#123;CONFIG_TYPE&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_PATH=<span class="string">&quot;config/<span class="variable">$&#123;ENV_PATH&#125;</span>config&quot;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.json&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.toml&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;toml&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.env&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Using <span class="variable">$&#123;CONFIG_BACKEND&#125;</span> backend: <span class="variable">$&#123;CONFIG_PATH&#125;</span>.<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Update dependencies</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;DISABLE_UPDATES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;Updating dependencies. Set DISABLE_UPDATES to prevent this.&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;pyproject.toml&quot;</span> ] &amp;&amp; [ -f <span class="string">&quot;poetry.lock&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        nvidia-smi 2&gt; /dev/null &amp;&amp; poetry install</span><br><span class="line">        <span class="built_in">uname</span> -s | grep -q Darwin &amp;&amp; poetry install -C install/apple</span><br><span class="line">        rocm-smi 2&gt; /dev/null &amp;&amp; poetry install -C install/rocm</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment"># Run the training script.</span></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_CONFIG_PATH=<span class="string">&quot;<span class="variable">$&#123;HOME&#125;</span>/.cache/huggingface/accelerate/default_config.yaml&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Using Accelerate config file: <span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span></span><br><span class="line">    accelerate launch --config_file=<span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> train.py</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Accelerate config file not found: <span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>. Using values from config.env.&quot;</span></span><br><span class="line">    accelerate launch <span class="variable">$&#123;ACCELERATE_EXTRA_ARGS&#125;</span> --mixed_precision=<span class="string">&quot;<span class="variable">$&#123;MIXED_PRECISION&#125;</span>&quot;</span> --num_processes=<span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_PROCESSES&#125;</span>&quot;</span> --num_machines=<span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_MACHINES&#125;</span>&quot;</span> --dynamo_backend=<span class="string">&quot;<span class="variable">$&#123;TRAINING_DYNAMO_BACKEND&#125;</span>&quot;</span> train.py</span><br><span class="line"></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Possible-accelerate-issues"><a href="#Possible-accelerate-issues" class="headerlink" title="Possible accelerate issues"></a>Possible <code>accelerate</code> issues</h3><p>여기서는 훈련을 시작하는 데 방해가 될 수 있는 한 가지 작은 사항을 언급하고 싶습니다. 끝 부분에 있는 기본 <code>train.sh</code> 안에는 훈련을 실행하는 명령이 있습니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch --config_file=<span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> train.py</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run the training script.</span></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">&quot;$&#123;ACCELERATE_CONFIG_PATH&#125;&quot;</span> ]]; then</span><br><span class="line">    ACCELERATE_CONFIG_PATH=<span class="string">&quot;$&#123;HOME&#125;/.cache/huggingface/accelerate/default_config.yaml&quot;</span></span><br><span class="line">fi</span><br><span class="line"><span class="keyword">if</span> [ -f <span class="string">&quot;$&#123;ACCELERATE_CONFIG_PATH&#125;&quot;</span> ]; then</span><br><span class="line">    echo <span class="string">&quot;Using Accelerate config file: $&#123;ACCELERATE_CONFIG_PATH&#125;&quot;</span></span><br><span class="line">    accelerate launch --config_file=<span class="string">&quot;$&#123;ACCELERATE_CONFIG_PATH&#125;&quot;</span> train.py</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    echo <span class="string">&quot;Accelerate config file not found: $&#123;ACCELERATE_CONFIG_PATH&#125;. Using values from config.env.&quot;</span></span><br><span class="line">    accelerate launch $&#123;ACCELERATE_EXTRA_ARGS&#125; --mixed_precision=<span class="string">&quot;$&#123;MIXED_PRECISION&#125;&quot;</span> --num_processes=<span class="string">&quot;$&#123;TRAINING_NUM_PROCESSES&#125;&quot;</span> --num_machines=<span class="string">&quot;$&#123;TRAINING_NUM_MACHINES&#125;&quot;</span> --dynamo_backend=<span class="string">&quot;$&#123;TRAINING_DYNAMO_BACKEND&#125;&quot;</span> train.py</span><br><span class="line"></span><br><span class="line">fi</span><br></pre></td></tr></table></figure>


<p>이것이 처음으로 훈련 저장소를 설치하는 것이라면 아마도 오류 없이 실행될 것입니다. 그러나 다른 저장소에서 <code>accelerate</code>를 사용한 경우 <code>default_config.yaml</code>을 이미 구성했을 가능성이 높습니다. 훈련에서 오류가 발생하는 경우 일반 훈련을 위해 여기에 자체 <code>config.yaml</code>을 제공했습니다. 또한 ‘LoRA’ 교육이 아닌 완전한 미세 조정을 시도하려는 경우 ‘DeepSpeed’ ‘config.yaml’을 제공했습니다.</p>
<p>‘DeepSpeed’는 GPU VRAM이 충분하지 않을 때 내부의 특수 기술을 사용하여 최적화 상태, 그라데이션 및 기타 매개변수를 CPU 메모리(RAM)로 오프로드합니다. ‘80GB’ VRAM과 ‘128GB’ CPU RAM을 갖춘 단일 ‘H100’ GPU에서는 ‘SD3.5 Large’로 완전한 미세 조정을 수행할 수 있었습니다. VRAM이 부족하고 CPU RAM으로 오프로드해야 할 때마다 이 <code>config.yaml</code>을 사용할 수도 있습니다.</p>
<ul>
<li><p>Custom general use <code>base_config.yaml</code></p>
  <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">debug:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">&#x27;NO&#x27;</span></span><br><span class="line"><span class="attr">downcast_bf16:</span> <span class="string">&#x27;no&#x27;</span></span><br><span class="line"><span class="attr">enable_cpu_affinity:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">gpu_ids:</span> <span class="string">all</span></span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">bf16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">rdzv_backend:</span> <span class="string">static</span></span><br><span class="line"><span class="attr">same_network:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">tpu_env:</span> []</span><br><span class="line"><span class="attr">tpu_use_cluster:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">tpu_use_sudo:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Custom <code>DeepSpeed 2</code> <code>deepspeed_config.yaml</code></p>
  <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">debug:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">gradient_accumulation_steps:</span> <span class="number">8</span></span><br><span class="line">  <span class="attr">gradient_clipping:</span> <span class="number">1.0</span></span><br><span class="line">  <span class="attr">offload_optimizer_device:</span> <span class="string">cpu</span></span><br><span class="line">  <span class="attr">offload_param_device:</span> <span class="string">cpu</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">zero_stage:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">downcast_bf16:</span> <span class="string">&#x27;no&#x27;</span></span><br><span class="line"><span class="attr">enable_cpu_affinity:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">bf16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">rdzv_backend:</span> <span class="string">static</span></span><br><span class="line"><span class="attr">same_network:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">tpu_env:</span> []</span><br><span class="line"><span class="attr">tpu_use_cluster:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">tpu_use_sudo:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>이러한 <code>yaml</code> 파일을 배치할 위치를 선택할 때마다 <code>train.sh</code> 코드에서 해당 파일을 올바르게 참조해야 합니다. 예를 들어, 저는 <code>SimpleTuner</code> 디렉토리의 루트에 파일을 배치합니다. 따라서 코드의 ‘ACCELERATE_CONFIG_PATH’ 부분이 그에 따라 수정됩니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run the training script with base config.</span></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_CONFIG_PATH=<span class="string">&quot;<span class="variable">$&#123;HOME&#125;</span>/SimpleTuner/base_config.yaml&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<p>or</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run the training script with DeepSpeed config.</span></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_CONFIG_PATH=<span class="string">&quot;<span class="variable">$&#123;HOME&#125;</span>/SimpleTuner/deepspeed_config.yaml&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<p>결국 <code>DeepSpeed</code> 지원 훈련을 시도하게 된다면, 이에 따라 사용할 수 있는 하위 수준 <code>config.env</code> 샘플이 있습니다.</p>
<ul>
<li><p>Custom SD3.5 Large <code>full</code> fine-tune<code>config.json</code></p>
  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;full&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">100</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">100</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">5e-5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-full-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--instance_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4 &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">25</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--metadata_update_interval&quot;</span><span class="punctuation">:</span> <span class="number">65</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--vae_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_unwanted_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_problematic_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--training_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--inference_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--snr_gamma&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--enable_xformers_memory_efficient_attention&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--allow_tf32&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--use_ema&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--ema_decay&quot;</span><span class="punctuation">:</span> <span class="number">0.999</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Custom SD3.5 Large <code>full</code> fine-tune<code>config.env</code></p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="built_in">export</span> MODEL_TYPE=<span class="string">&#x27;full&#x27;</span></span><br><span class="line"><span class="built_in">export</span> MODEL_FAMILY=<span class="string">&#x27;sd3&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONTROLNET=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> USE_DORA=<span class="literal">false</span></span><br><span class="line"><span class="comment"># Restart where we left off. Change this to &quot;checkpoint-1234&quot; to start from a specific checkpoint.</span></span><br><span class="line"><span class="built_in">export</span> RESUME_CHECKPOINT=<span class="string">&quot;latest&quot;</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_STEPS=100</span><br><span class="line"><span class="comment"># This is how many checkpoints we will keep. Two is safe, but three is safer.</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_LIMIT=100</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is decided as a relatively conservative &#x27;constant&#x27; learning rate.</span></span><br><span class="line"><span class="comment"># Adjust higher or lower depending on how burnt your model becomes.</span></span><br><span class="line"><span class="built_in">export</span> LEARNING_RATE=5e-5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using a Huggingface Hub model:</span></span><br><span class="line"><span class="built_in">export</span> MODEL_NAME=<span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make DEBUG_EXTRA_ARGS empty to disable wandb.</span></span><br><span class="line"><span class="built_in">export</span> DEBUG_EXTRA_ARGS=<span class="string">&quot;--report_to=wandb&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_PROJECT_NAME=<span class="string">&quot;sd35-training&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_RUN_NAME=<span class="string">&quot;simpletuner-fantasy-art-full-01&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Max number of steps OR epochs can be used. Not both.</span></span><br><span class="line"><span class="built_in">export</span> MAX_NUM_STEPS=24000</span><br><span class="line"><span class="built_in">export</span> NUM_EPOCHS=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># A convenient prefix for all of your training paths.</span></span><br><span class="line"><span class="built_in">export</span> DATALOADER_CONFIG=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_full_L_01/datasets/multidatabackend.json&quot;</span></span><br><span class="line"><span class="built_in">export</span> OUTPUT_DIR=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_full_L_01/datasets/models&quot;</span></span><br><span class="line"><span class="comment"># Set this to &quot;true&quot; to push your model to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_TO_HUB=<span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="comment"># If PUSH_TO_HUB and PUSH_CHECKPOINTS are both enabled, every saved checkpoint will be pushed to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_CHECKPOINTS=<span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="comment"># This will be the model name for your final hub upload, eg. &quot;yourusername/yourmodelname&quot;</span></span><br><span class="line"><span class="comment"># It defaults to the wandb project name, but you can override this here.</span></span><br><span class="line"><span class="comment"># export HUB_MODEL_NAME=$TRACKER_PROJECT_NAME</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, images will be resized so their SMALLER EDGE is 1024 pixels, maintaining aspect ratio.</span></span><br><span class="line"><span class="comment"># Setting this value to 768px might result in more reasonable training data sizes for SDXL.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION=1024</span><br><span class="line"><span class="comment"># If you want to have the training data resized by pixel area (Megapixels) rather than edge length,</span></span><br><span class="line"><span class="comment">#  set this value to &quot;area&quot; instead of &quot;pixel&quot;, and uncomment the next RESOLUTION declaration.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION_TYPE=<span class="string">&quot;pixel&quot;</span></span><br><span class="line"><span class="comment">#export RESOLUTION=1          # 1.0 Megapixel training sizes</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;pixel&quot;, the minimum resolution specifies the smaller edge length, measured in pixels. Recommended: 1024.</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;area&quot;, the minimum resolution specifies the total image area, measured in megapixels. Recommended: 1.</span></span><br><span class="line"><span class="built_in">export</span> MINIMUM_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># How many decimals to round aspect buckets to.</span></span><br><span class="line"><span class="comment">#export ASPECT_BUCKET_ROUNDING=2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use this to append an instance prompt to each caption, used for adding trigger words.</span></span><br><span class="line"><span class="comment"># This has not been tested in SDXL.</span></span><br><span class="line"><span class="built_in">export</span> INSTANCE_PROMPT=<span class="string">&quot;k4s4 &quot;</span></span><br><span class="line"><span class="comment"># If you also supply a user prompt library or `--use_prompt_library`, this will be added to those lists.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_PROMPT=<span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE=7.5</span><br><span class="line"><span class="comment"># You&#x27;ll want to set this to 0.7 if you are training a terminal SNR model.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE_RESCALE=0.0</span><br><span class="line"><span class="comment"># How frequently we will save and run a pipeline for validations.</span></span><br><span class="line"><span class="comment"># export VALIDATION_STEPS=200</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_STEPS=25</span><br><span class="line"><span class="built_in">export</span> VALIDATION_NUM_INFERENCE_STEPS=30</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> VALIDATION_NEGATIVE_PROMPT=<span class="string">&quot;blurry, cropped, ugly&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_SEED=42</span><br><span class="line"><span class="built_in">export</span> VALIDATION_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adjust this for your GPU memory size. This, and resolution, are the biggest VRAM killers.</span></span><br><span class="line"><span class="built_in">export</span> TRAIN_BATCH_SIZE=6</span><br><span class="line"><span class="comment"># Accumulate your update gradient over many steps, to save VRAM while still having higher effective batch size:</span></span><br><span class="line"><span class="comment"># effective batch size = ($TRAIN_BATCH_SIZE * $GRADIENT_ACCUMULATION_STEPS).</span></span><br><span class="line"><span class="built_in">export</span> GRADIENT_ACCUMULATION_STEPS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use any standard scheduler type. constant, polynomial, constant_with_warmup</span></span><br><span class="line"><span class="built_in">export</span> LR_SCHEDULE=<span class="string">&quot;cosine&quot;</span></span><br><span class="line"><span class="comment"># A warmup period allows the model and the EMA weights more importantly to familiarise itself with the current quanta.</span></span><br><span class="line"><span class="comment"># For the cosine or sine type schedules, the warmup period defines the interval between peaks or valleys.</span></span><br><span class="line"><span class="comment"># Use a sine schedule to simulate a warmup period, or a Cosine period to simulate a polynomial start.</span></span><br><span class="line"><span class="comment"># export LR_WARMUP_STEPS=$((MAX_NUM_STEPS / 10))</span></span><br><span class="line"><span class="built_in">export</span> LR_WARMUP_STEPS=2400</span><br><span class="line"></span><br><span class="line"><span class="comment"># Caption dropout probability. Set to 0.1 for 10% of captions dropped out. Set to 0 to disable.</span></span><br><span class="line"><span class="comment"># You may wish to disable dropout if you want to limit your changes strictly to the prompts you show the model.</span></span><br><span class="line"><span class="comment"># You may wish to increase the rate of dropout if you want to more broadly adopt your changes across the model.</span></span><br><span class="line"><span class="built_in">export</span> CAPTION_DROPOUT_PROBABILITY=0</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> METADATA_UPDATE_INTERVAL=65</span><br><span class="line"><span class="built_in">export</span> VAE_BATCH_SIZE=12</span><br><span class="line"></span><br><span class="line"><span class="comment"># If this is set, any images that fail to open will be DELETED to avoid re-checking them every time.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_ERRORED_IMAGES=0</span><br><span class="line"><span class="comment"># If this is set, any images that are too small for the minimum resolution size will be DELETED.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_SMALL_IMAGES=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bytedance recommends these be set to &quot;trailing&quot; so that inference and training behave in a more congruent manner.</span></span><br><span class="line"><span class="comment"># To follow the original SDXL training strategy, use &quot;leading&quot; instead, though results are generally worse.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"><span class="built_in">export</span> INFERENCE_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing this option or unsetting it uses vanilla training. Setting it reweights the loss by the position of the timestep in the noise schedule.</span></span><br><span class="line"><span class="comment"># A value &quot;5&quot; is recommended by the researchers. A value of &quot;20&quot; is the least impact, and &quot;1&quot; is the most impact.</span></span><br><span class="line"><span class="built_in">export</span> MIN_SNR_GAMMA=5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set this to an explicit value of &quot;false&quot; to disable Xformers. Probably required for AMD users.</span></span><br><span class="line"><span class="built_in">export</span> USE_XFORMERS=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># There&#x27;s basically no reason to unset this. However, to disable it, use an explicit value of &quot;false&quot;.</span></span><br><span class="line"><span class="comment"># This will save a lot of memory consumption when enabled.</span></span><br><span class="line"><span class="built_in">export</span> USE_GRADIENT_CHECKPOINTING=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Options below here may require a bit more complicated configuration, so they are not simple variables.</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TF32 is great on Ampere or Ada, not sure about earlier generations.</span></span><br><span class="line"><span class="built_in">export</span> ALLOW_TF32=<span class="literal">true</span></span><br><span class="line"><span class="comment"># AdamW 8Bit is a robust and lightweight choice. Adafactor might reduce memory consumption, and Dadaptation is slow and experimental.</span></span><br><span class="line"><span class="comment"># AdamW is the default optimizer, but it uses a lot of memory and is slower than AdamW8Bit or Adafactor.</span></span><br><span class="line"><span class="comment"># Choices: adamw, adamw8bit, adafactor, dadaptation</span></span><br><span class="line"><span class="comment"># export OPTIMIZER=&quot;adamw_bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> OPTIMIZER=<span class="string">&quot;adamw_bf16&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># EMA is a strong regularisation method that uses a lot of extra VRAM to hold two copies of the weights.</span></span><br><span class="line"><span class="comment"># This is worthwhile on large training runs, but not so much for smaller training runs.</span></span><br><span class="line"><span class="built_in">export</span> USE_EMA=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> EMA_DECAY=0.999</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TRAINER_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reproducible training. Set to -1 to disable.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SEED=42</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mixed precision is the best. You honestly might need to YOLO it in fp16 mode for Google Colab type setups.</span></span><br><span class="line"><span class="built_in">export</span> MIXED_PRECISION=<span class="string">&quot;bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> PURE_BF16=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This has to be changed if you&#x27;re training with multiple GPUs.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_PROCESSES=1</span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_MACHINES=1</span><br><span class="line"><span class="built_in">export</span> ACCELERATE_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># With Pytorch 2.1, you might have pretty good luck here.</span></span><br><span class="line"><span class="comment"># If you&#x27;re using aspect bucketing however, each resolution change will recompile. Seriously, just don&#x27;t do it.</span></span><br><span class="line"><span class="comment"># Well, then again... Pytorch 2.2 has support for dynamic shapes. Why not?</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TOKENIZERS_PARALLELISM=<span class="literal">false</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>Changed parameters</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">	<span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;full&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">100</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">100</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">5e-5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-full-01&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>특히 학습률이 ‘5e-5’로 감소했습니다.</p>
<p>모든 것이 정상이면 계속해서 훈련을 시작하십시오.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash train.sh</span><br></pre></td></tr></table></figure>

<h3 id="Memory-usage"><a href="#Memory-usage" class="headerlink" title="Memory usage"></a>Memory usage</h3><p>텍스트 인코더를 훈련하지 않는 경우(우리는 그렇지 않습니다) ‘SimpleTuner’를 사용하면 약 ‘10.4GB’의 VRAM을 절약할 수 있습니다.</p>
<p>‘배치 크기’를 ‘6’으로 설정하고 ‘lora 순위&#x2F;알파’를 ‘768’로 설정하면 훈련에서 약 ‘32GB’의 VRAM을 소비합니다.</p>
<p>당연히 이는 소비자 ‘24GB’ VRAM GPU의 범위를 벗어납니다. 그래서 <code>batch size</code>를 <code>1</code>, <code>lora Rank/alpha</code>를 <code>128</code>로 사용하여 메모리 비용을 줄이려고 했습니다.</p>
<p>잠정적으로 VRAM 비용을 약 ‘19.65GB’ VRAM으로 낮출 수 있었습니다.</p>
<p>그러나 유효성 검사 프롬프트에 대한 추론을 실행하면 VRAM이 최대 ‘23.37GB’까지 급증합니다.</p>
<p>안전을 위해 ‘lora 순위&#x2F;알파’를 ‘64’로 더욱 줄여야 할 수도 있습니다. 그렇다면 훈련 중에 약 ‘18.83GB’의 VRAM을 소비하게 됩니다.</p>
<p>검증 추론 중에는 최대 약 ‘21.50GB’의 VRAM이 사용됩니다. 이 정도면 충분히 안전해 보입니다.</p>
<p>‘배치 크기’ ‘6’ 및 ‘lora 순위&#x2F;알파’ ‘768’의 더 높은 사양 교육을 사용하기로 결정한 경우 [위](https:&#x2F;&#x2F; <a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.notion.so/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6?pvs=21">www.notion.so/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6?pvs=21</a>) GPU VRAM이 부족하고 CPU RAM이 충분한 경우.</p>
<h3 id="Monitoring-the-training"><a href="#Monitoring-the-training" class="headerlink" title="Monitoring the training"></a>Monitoring the training</h3><p>훈련 과정에서 검증 이미지가 픽셀화되거나 검게 변하는 경우가 있을 수 있습니다. 이는 ‘1.05e-3’이라는 매우 공격적인 학습률을 사용하고 있기 때문입니다. 더 안전하게 플레이하고 싶다면 ‘9.5e-4’를 사용하면 픽셀화 문제가 거의 발생하지 않습니다. 그럼에도 불구하고 두 손실 곡선은 결국 훌륭하게 수렴했습니다.</p>
<p>하지만 우려사항을 해소하기 위해 어떤 모습일지 몇 가지 예를 보여드리고 싶습니다.</p>
<h3 id="Observing-training-loss"><a href="#Observing-training-loss" class="headerlink" title="Observing training loss"></a>Observing training loss</h3><h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a><code>LoRA</code></h3><p>판타지 아트 ‘LoRA’ 수련을 통해 얻은 피규어들입니다. 손실이 감소하고 있으며 아직 수렴되지 않았습니다. 그러나 확산 모델을 미세 조정한 경험이 있는 경우 손실 최소화는 미적 극대화와 거의 관련이 없습니다. 또한 높은 학습률을 사용하는 경우 손실 곡선의 최고점 근처에서 검증 이미지의 픽셀화 또는 품질 저하가 발생할 수 있음을 확인했습니다. 훈련이 모델 가중치가 만족스럽지 않은 학습 속도에 도달하면 이는 의미가 있습니다.</p>
<p>학습률이 높으면 열차 손실도 최고점에 달합니다.</p>
<h2 id="Evaluating-the-results"><a href="#Evaluating-the-results" class="headerlink" title="Evaluating the results"></a>Evaluating the results</h2><h3 id="How-to-actually-get-the-LoRA-models-into-ComfyUI"><a href="#How-to-actually-get-the-LoRA-models-into-ComfyUI" class="headerlink" title="How to actually get the LoRA models into ComfyUI"></a>How to actually get the LoRA models into ComfyUI</h3><p>이제 모델이 모두 훈련되었으므로 <code>ComfyUI</code>를 사용하여 테스트할 차례입니다. 그러나 SimpleTuner가 모델을 저장하는 방식으로 인해 ‘ComfyUI&#x2F;models&#x2F;loras’ 디렉터리로 가져오기가 약간 어렵습니다.</p>
<p>모델을 저장한 디렉터리로 이동하면 해당 형식이 이 형식인 것을 볼 수 있습니다.</p>
<p>각 디렉토리에서 원하는 파일은 <code>pytorch_lora_weights.safetensors</code> 파일입니다. 이러한 파일을 <code>ComfyUI</code>로 가져오는 프로세스를 간소화하기 위해 다음 스크립트를 작성했습니다.</p>
<ul>
<li><p><code>create_symlinks_lora.sh</code></p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Source directory where the models are stored</span></span><br><span class="line">SOURCE_DIR=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Target directory for symlinks</span></span><br><span class="line">TARGET_DIR=<span class="string">&quot;/weka2/home-yeo/ComfyUI/models/loras/sd35_large/fantasy_art&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ensure target directory exists or create it</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="string">&quot;<span class="variable">$&#123;TARGET_DIR&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over each checkpoint directory</span></span><br><span class="line"><span class="keyword">for</span> CHECKPOINT_DIR <span class="keyword">in</span> <span class="variable">$&#123;SOURCE_DIR&#125;</span>/checkpoint-*; <span class="keyword">do</span></span><br><span class="line">    <span class="comment"># Check if it&#x27;s indeed a directory</span></span><br><span class="line">    <span class="keyword">if</span> [ -d <span class="string">&quot;<span class="variable">$&#123;CHECKPOINT_DIR&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="comment"># Extract the checkpoint number from the directory name</span></span><br><span class="line">        CHECKPOINT_NAME=$(<span class="built_in">basename</span> <span class="variable">$&#123;CHECKPOINT_DIR&#125;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Define the source file path</span></span><br><span class="line">        SOURCE_FILE=<span class="string">&quot;<span class="variable">$&#123;CHECKPOINT_DIR&#125;</span>/pytorch_lora_weights.safetensors&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Define the symlink name with &#x27;lora&#x27; added before &#x27;safetensors&#x27;</span></span><br><span class="line">        LINK_NAME=<span class="string">&quot;<span class="variable">$&#123;TARGET_DIR&#125;</span>/<span class="variable">$&#123;CHECKPOINT_NAME&#125;</span>_lora.safetensors&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if the source file exists</span></span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;SOURCE_FILE&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="comment"># Create a symlink in the target directory</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;Creating symlink from <span class="variable">$&#123;SOURCE_FILE&#125;</span> to <span class="variable">$&#123;LINK_NAME&#125;</span>&quot;</span></span><br><span class="line">            <span class="built_in">ln</span> -s <span class="string">&quot;<span class="variable">$&#123;SOURCE_FILE&#125;</span>&quot;</span> <span class="string">&quot;<span class="variable">$&#123;LINK_NAME&#125;</span>&quot;</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;Symlink created for <span class="variable">$&#123;CHECKPOINT_NAME&#125;</span>&quot;</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;File not found: <span class="variable">$&#123;SOURCE_FILE&#125;</span>&quot;</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;Not a directory: <span class="variable">$&#123;CHECKPOINT_DIR&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Symlinking complete.&quot;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>위의 쉘 스크립트가 수행할 작업은 <code>SimpleTuner</code>에서 <code>SOURCE_DIR</code>을 반복한 다음 <em><strong>만</strong></em> <code>pytorch_lora_weights.safetensors</code> 파일을 <code>TARGET_DIR</code>에 심볼릭 링크하는 것입니다. 이 파일은 <code>ComfyUI 내부 디렉토리여야 합니다. /모델/로라스</code>. 파일을 추적하기 위해 파일 이름 안에 해당 체크포인트 번호가 포함되도록 이름도 변경했습니다.</p>
<h3 id="Determining-the-best-checkpoint"><a href="#Determining-the-best-checkpoint" class="headerlink" title="Determining the best checkpoint"></a>Determining the best checkpoint</h3><p>제가 사용하고 있는 기본적인 ‘SD3.5 Large’ 워크플로는 이것이었습니다.</p>
<p>가장 좋은 체크포인트를 결정하는 방법은 특정 프롬프트에 대해 x축에 체크포인트 번호를 표시하는 것입니다. 그래서 저는 다음과 같은 단일 스트립을 얻습니다.</p>
<p>판타지 아트 ‘LoRA’</p>
<p>Prompt</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a three fourth perspective waist up portrait view of a young woman with messy long blonde hair and light purple eyes, looking at viewer with a closed mouth smile, wearing tight black dress, a faded pink simple background during golden hour</span><br></pre></td></tr></table></figure>



<p>이를 위해 <code>ComfyUI</code> 워크플로의 <code>api</code> 버전에 로드되는 사용자 정의 스크립트를 사용합니다. 저장(API 형식) 버튼을 클릭하면 모든 워크플로우를 ‘API’ 형식으로 저장할 수 있습니다. 귀하가 사용할 수 있도록 이미 위 버전을 저장했습니다. ‘ComfyUI’ API 사용에 대한 더 심층적인 비디오 가이드를 원하시면 제가 작년에 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://youtu.be/WwsJ_QIgsG8">여기</a>를 만들었습니다.</p>
<p><code>ComfyUI</code>가 실행 중인지 확인한 후 아래 스크립트를 실행하세요. 또한 스크립트를 실행하는 동일한 위치에 <code>.env</code> 파일을 설정해야 합니다.</p>
<ul>
<li><p><code>API script</code></p>
<p>This is my custom <code>python</code> script:</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import json</span><br><span class="line">import random</span><br><span class="line">from urllib import request</span><br><span class="line">import datetime</span><br><span class="line">from PIL import Image, ImageDraw, ImageFont</span><br><span class="line">import time</span><br><span class="line">import re</span><br><span class="line">import urllib.error</span><br><span class="line"></span><br><span class="line">from dotenv import load_dotenv</span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"># Configuration</span><br><span class="line">api_workflow_dir = os.getenv(&quot;API_WORKFLOW_DIR&quot;)</span><br><span class="line">lora_dir = os.getenv(&quot;LORA_DIR&quot;)</span><br><span class="line"></span><br><span class="line">api_workflow_file = os.getenv(&quot;API_WORKFLOW_FILE&quot;)</span><br><span class="line">api_endpoint = os.getenv(&quot;API_ENDPOINT&quot;)</span><br><span class="line">image_output_dir = os.getenv(&quot;IMAGE_OUTPUT_DIR&quot;)</span><br><span class="line">font_ttf_path = os.getenv(&quot;FONT_TTF_PATH&quot;)</span><br><span class="line"></span><br><span class="line">comfyui_output_dir = os.getenv(&quot;COMFYUI_OUTPUT_DIR&quot;)</span><br><span class="line"></span><br><span class="line">api_endpoint = f&quot;http://&#123;api_endpoint&#125;/prompt&quot;</span><br><span class="line"></span><br><span class="line">workflow_file_path = os.path.join(api_workflow_dir, api_workflow_file)</span><br><span class="line">workflow = json.load(open(workflow_file_path))</span><br><span class="line"></span><br><span class="line">current_datetime = datetime.datetime.now().strftime(&quot;%Y-%m-%d_%H-%M-%S&quot;)</span><br><span class="line">relative_output_path = current_datetime</span><br><span class="line"></span><br><span class="line">directory_creation_timeout = 3000  # Timeout for directory creation in seconds</span><br><span class="line">image_generation_timeout = 30000  # Timeout for image generation in seconds</span><br><span class="line"></span><br><span class="line">def get_checkpoint_number(filename):</span><br><span class="line">    match = re.search(r&#x27;checkpoint-(\d+)&#x27;, filename)</span><br><span class="line">    if match:</span><br><span class="line">        return int(match.group(1))</span><br><span class="line">    match = re.search(r&#x27;/checkpoint-(\d+)/&#x27;, filename)</span><br><span class="line">    if match:</span><br><span class="line">        return int(match.group(1))</span><br><span class="line">    return None</span><br><span class="line"></span><br><span class="line">def get_most_recent_output_folder(base_dir):</span><br><span class="line">    folders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]</span><br><span class="line">    if not folders:</span><br><span class="line">        return None</span><br><span class="line">    return max(folders, key=lambda f: os.path.getctime(os.path.join(base_dir, f)))</span><br><span class="line"></span><br><span class="line">def process_loras(lora_dir, workflow):</span><br><span class="line">    print(f&quot;Scanning directory: &#123;lora_dir&#125;&quot;)</span><br><span class="line">    </span><br><span class="line">    # Extract the last two directories from LORA_DIR</span><br><span class="line">    lora_path_parts = lora_dir.split(&#x27;/&#x27;)</span><br><span class="line">    dynamic_lora_path = &#x27;/&#x27;.join(lora_path_parts[-2:])</span><br><span class="line">    </span><br><span class="line">    all_items = os.listdir(lora_dir)</span><br><span class="line">    </span><br><span class="line">    lora_items = [f for f in all_items if f.endswith(&#x27;_lora.safetensors&#x27;)]</span><br><span class="line">    </span><br><span class="line">    lora_items.sort(key=lambda x: int(x.split(&#x27;-&#x27;)[1].split(&#x27;_&#x27;)[0]))</span><br><span class="line">    </span><br><span class="line">    print(f&quot;Found items: &#123;lora_items&#125;&quot;)</span><br><span class="line">    </span><br><span class="line">    for item in lora_items:</span><br><span class="line">        checkpoint_num = item.split(&#x27;-&#x27;)[1].split(&#x27;_&#x27;)[0]</span><br><span class="line">        </span><br><span class="line">        print(f&quot;Processing: &#123;item&#125;&quot;)</span><br><span class="line"></span><br><span class="line">        # Update the LoRA loader node</span><br><span class="line">        lora_loader_node = workflow[&quot;276&quot;]</span><br><span class="line">        lora_loader_node[&quot;inputs&quot;][&quot;lora_name&quot;] = f&quot;&#123;dynamic_lora_path&#125;/&#123;item&#125;&quot;</span><br><span class="line"></span><br><span class="line">        save_image = workflow[&quot;314&quot;]</span><br><span class="line">        filename_prefix = f&quot;checkpoint-&#123;checkpoint_num&#125;&quot;</span><br><span class="line">        save_image[&quot;inputs&quot;][&quot;output_path&quot;] = relative_output_path</span><br><span class="line">        save_image[&quot;inputs&quot;][&quot;filename_prefix&quot;] = filename_prefix</span><br><span class="line"></span><br><span class="line">        success = queue_prompt(workflow)</span><br><span class="line">        if not success:</span><br><span class="line">            print(f&quot;Failed to queue prompt for checkpoint &#123;checkpoint_num&#125;&quot;)</span><br><span class="line">        else:</span><br><span class="line">            print(f&quot;Successfully queued prompt for checkpoint &#123;checkpoint_num&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    if not lora_items:</span><br><span class="line">        print(&quot;No LoRA files found in the directory.&quot;)</span><br><span class="line">    </span><br><span class="line">    return len(lora_items)</span><br><span class="line"></span><br><span class="line">def create_image_strip(lora_dir, image_folder, output_filename):</span><br><span class="line">    lora_files = [f for f in os.listdir(lora_dir) if f.endswith(&#x27;_lora.safetensors&#x27;)]</span><br><span class="line">    lora_files.sort(key=get_checkpoint_number)</span><br><span class="line">    checkpoints = [get_checkpoint_number(f) for f in lora_files if get_checkpoint_number(f) is not None]</span><br><span class="line"></span><br><span class="line">    images = []</span><br><span class="line">    for checkpoint in checkpoints:</span><br><span class="line">        filename = f&quot;checkpoint-&#123;checkpoint&#125;_0001.png&quot;</span><br><span class="line">        filepath = os.path.join(image_folder, filename)</span><br><span class="line">        if os.path.exists(filepath):</span><br><span class="line">            try:</span><br><span class="line">                img = Image.open(filepath)</span><br><span class="line">                images.append(img)</span><br><span class="line">            except IOError as e:</span><br><span class="line">                print(f&quot;Cannot open image: &#123;filepath&#125;&quot;)</span><br><span class="line">                print(f&quot;Error: &#123;e&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    if not images:</span><br><span class="line">        print(&quot;No valid images found.&quot;)</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    img_width, img_height = images[0].size</span><br><span class="line">    strip_width = img_width * len(images)</span><br><span class="line">    label_height = 50  # Space for labels</span><br><span class="line">    strip_height = img_height + label_height</span><br><span class="line"></span><br><span class="line">    strip_image = Image.new(&#x27;RGB&#x27;, (strip_width, strip_height), &#x27;white&#x27;)</span><br><span class="line">    draw = ImageDraw.Draw(strip_image)</span><br><span class="line">    font = ImageFont.truetype(font_ttf_path, 20)</span><br><span class="line"></span><br><span class="line">    for i, (img, checkpoint) in enumerate(zip(images, checkpoints)):</span><br><span class="line">        strip_image.paste(img, (i * img_width, label_height))</span><br><span class="line">        </span><br><span class="line">        label = f&quot;checkpoint-&#123;checkpoint&#125;&quot;</span><br><span class="line">        label_width = draw.textlength(label, font=font)</span><br><span class="line">        label_x = i * img_width + (img_width - label_width) // 2</span><br><span class="line">        draw.text((label_x, 10), label, fill=&quot;black&quot;, font=font)</span><br><span class="line"></span><br><span class="line">    strip_image.save(output_filename)</span><br><span class="line">    print(f&quot;Image strip saved to: &#123;output_filename&#125;&quot;)</span><br><span class="line"></span><br><span class="line">def queue_prompt(workflow):</span><br><span class="line">    p = &#123;&quot;prompt&quot;: workflow&#125;</span><br><span class="line">    data = json.dumps(p).encode(&#x27;utf-8&#x27;)</span><br><span class="line">    req = request.Request(api_endpoint, data=data, headers=&#123;&#x27;Content-Type&#x27;: &#x27;application/json&#x27;&#125;)</span><br><span class="line">    try:</span><br><span class="line">        with request.urlopen(req) as response:</span><br><span class="line">            print(f&quot;API request successful. Status code: &#123;response.getcode()&#125;&quot;)</span><br><span class="line">            return True</span><br><span class="line">    except urllib.error.URLError as e:</span><br><span class="line">        if hasattr(e, &#x27;reason&#x27;):</span><br><span class="line">            print(f&quot;Failed to reach the server. Reason: &#123;e.reason&#125;&quot;)</span><br><span class="line">        elif hasattr(e, &#x27;code&#x27;):</span><br><span class="line">            print(f&quot;The server couldn&#x27;t fulfill the request. Error code: &#123;e.code&#125;&quot;)</span><br><span class="line">        print(f&quot;API endpoint: &#123;api_endpoint&#125;&quot;)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        print(f&quot;An error occurred: &#123;str(e)&#125;&quot;)</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">def wait_for_directory_creation(directory, timeout):</span><br><span class="line">    print(f&quot;Waiting for directory &#123;directory&#125; to be created...&quot;)</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    while time.time() - start_time &lt; timeout:</span><br><span class="line">        if os.path.exists(directory):</span><br><span class="line">            print(f&quot;Directory &#123;directory&#125; found.&quot;)</span><br><span class="line">            return True</span><br><span class="line">        time.sleep(5)  # Check every 5 seconds</span><br><span class="line">    print(f&quot;Timeout waiting for directory &#123;directory&#125; to be created.&quot;)</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">def wait_for_images(image_folder, expected_count, timeout):</span><br><span class="line">    print(&quot;Waiting for images to be generated...&quot;)</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    while time.time() - start_time &lt; timeout:</span><br><span class="line">        if os.path.exists(image_folder):</span><br><span class="line">            image_files = [f for f in os.listdir(image_folder) if f.endswith(&#x27;.png&#x27;)]</span><br><span class="line">            if len(image_files) &gt;= expected_count:</span><br><span class="line">                print(f&quot;Found all &#123;expected_count&#125; images.&quot;)</span><br><span class="line">                return True</span><br><span class="line">        time.sleep(5)  # Check every 5 seconds</span><br><span class="line">    print(&quot;Timeout waiting for images to be generated.&quot;)</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    print(f&quot;LoRA directory: &#123;lora_dir&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    # Generate images</span><br><span class="line">    expected_image_count = process_loras(lora_dir, workflow)</span><br><span class="line"></span><br><span class="line">    absolute_output_path = os.path.join(comfyui_output_dir, current_datetime)</span><br><span class="line">    print(f&quot;Absolute output path: &#123;absolute_output_path&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    # Create the image strip</span><br><span class="line">    if wait_for_directory_creation(absolute_output_path, directory_creation_timeout):</span><br><span class="line">        print(f&quot;Expected image count: &#123;expected_image_count&#125;&quot;)</span><br><span class="line">        if wait_for_images(absolute_output_path, expected_image_count, image_generation_timeout):</span><br><span class="line">            output_strip_filename = os.path.join(absolute_output_path, &quot;output_image_strip.png&quot;)</span><br><span class="line">            create_image_strip(lora_dir, absolute_output_path, output_strip_filename)</span><br><span class="line">        else:</span><br><span class="line">            print(&quot;Failed to generate all images in time.&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;Output directory was not created.&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>sample <code>.env</code> file</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">API_WORKFLOW_DIR=/weka2/home-yeo/workflows</span><br><span class="line">COMFYUI_OUTPUT_DIR = /weka2/home-yeo/ComfyUI/output/</span><br><span class="line">LORA_DIR=/admin/home-yeo/workspace/ComfyUI/models/loras/sd35_large/fantasy_art_01</span><br><span class="line">API_WORKFLOW_FILE=sd35_fantasy_art_02_api.json</span><br><span class="line">API_ENDPOINT=127.0.0.1:8188</span><br><span class="line">FONT_TTF_PATH=/weka2/home-yeo/fonts/arial.ttf</span><br><span class="line">BOLD_FONT_TTF_PATH=/weka2/home-yeo/fonts/arialbd.ttf</span><br></pre></td></tr></table></figure></li>
</ul>
<p>Fantasy Art <code>LoRA</code></p>
<p>Prompt</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a three fourth perspective waist up portrait view of a young woman with messy long blonde hair and light purple eyes, looking at viewer with a closed mouth smile, wearing tight black dress, a faded pink simple background during golden hour</span><br></pre></td></tr></table></figure>

<p>결국 ‘24,000’ 단계에서 거의 마지막에 체크포인트를 선택하게 되었습니다.</p>
<p>나는 또한 건전성 확인을 위해 수행한 다른 모든 훈련에 대해 동일한 실험을 실행했습니다.</p>
<p>Cinema Photo <code>LoRA</code></p>
<p>Prompt</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a few hooded figures walking on an empty road in the rain, desolate, high skyscrapers</span><br></pre></td></tr></table></figure>

<p>John Singer Sargent <code>LoRA</code></p>
<p>Prompt</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">an abandoned beach with a lighthouse</span><br></pre></td></tr></table></figure>

<p>Underexposed Photography <code>LoRA</code></p>
<p>Prompt</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">waist up view of a woman posing on a runway, streetwear in the style of alexander mcqueen</span><br></pre></td></tr></table></figure>


<p>전문적인 이유로 원래 그리드의 특정 부분이 생략되었습니다. 전체 그리드에는 모든 청중에게 적합하지 않을 수 있는 콘텐츠가 포함되어 있으므로 기술적인 측면에 초점을 맞추기 위해 잘린 버전이 표시됩니다.</p>
<p>Pixel Art <code>LoRA</code></p>
<p>Prompt</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a plush chibi mythical creature</span><br></pre></td></tr></table></figure>

<p>Ethnic Paint <code>LoRA</code></p>
<p>Prompt</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a skyline view of a futuristic maritime village floating above ground, <span class="keyword">in</span> the clouds, towering skyscrapers, golden hour, day time lighting</span><br></pre></td></tr></table></figure>

<h2 id="A-x2F-B-evaluation"><a href="#A-x2F-B-evaluation" class="headerlink" title="A&#x2F;B evaluation"></a>A&#x2F;B evaluation</h2><h3 id="Improving-x2F-tuning-generations-with-APG-scaling"><a href="#Improving-x2F-tuning-generations-with-APG-scaling" class="headerlink" title="Improving&#x2F;tuning generations with APG scaling"></a>Improving&#x2F;tuning generations with APG scaling</h3><p>최고의 미적 결과를 제공하는 ‘LoRA’ 체크포인트를 찾았으면 ‘APG’ 스케일링을 통해 이를 더욱 향상시킬 수 있습니다. ‘APG’ 스케일링은 적응형 예측 지침을 의미합니다.</p>
<p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/abs/2410.02416">APG 논문</a> 초록의 핵심 부분</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Our approach, termed adaptive projected guidance (APG), retains the quality-boosting advantages of CFG while enabling the use of higher guidance scales without oversaturation. APG is easy to implement and introduces practically no additional computational overhead to the sampling process.</span><br></pre></td></tr></table></figure>

<p>이것이 이 샘플 워크플로에 포함된 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/logtd/ComfyUI-APGScaling">ComfyUI 노드</a>입니다. 세 가지 다른 이미지를 생성합니다. 하나는 기본 이미지, 하나는 ***<code>APG</code> 스케일링 없이 ***<code>LoRA</code> 적용, 세 번째 이미지는 ***<code>사용***</code> LoRA<code> 적용 APG</code> 스케일링.</p>
<p>The parameters for APG are:</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">eta</span><br><span class="line">norm<span class="emphasis">_threshold</span></span><br><span class="line"><span class="emphasis">use_</span>momentum</span><br><span class="line">momentum</span><br></pre></td></tr></table></figure>

<p>이 노드에 대해 그렇게 많이 심층 분석하지는 않았지만 이미지 품질이 좋든 나쁘든 변경됩니다.</p>
<h3 id="Before-and-after-comparison"><a href="#Before-and-after-comparison" class="headerlink" title="Before and after comparison"></a>Before and after comparison</h3><p>Fantasy Art</p>
<p>Prompt</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a three fourth perspective waist up portrait view of a young woman with messy long blonde hair and light purple eyes, perfect face, looking at viewer with a closed mouth smile, wearing loose black dress, a faded pink simple background during golden hour</span><br></pre></td></tr></table></figure>

<p><code>Base model</code></p>
<p><code>LoRA</code></p>
<p><code>LoRA</code> + <code>APG</code></p>
<p>Cinema Photo</p>
<p>Prompt</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a wide view of a figure looking up at a meteor breaking apart</span><br></pre></td></tr></table></figure>

<p><code>Base model</code></p>
<p><code>LoRA</code></p>
<p><code>LoRA</code> + <code>APG</code></p>
<p>John Singer Sargent</p>
<p>Prompt</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">an abandoned beach with a lighthouse</span><br></pre></td></tr></table></figure>

<p><code>Base model</code></p>
<p><code>LoRA</code></p>
<p><code>LoRA</code> + <code>APG</code></p>
<p>Underexposed Photography</p>
<p>Prompt</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">waist up view of a woman posing on a runway, streetwear in the style of alexander mcqueen</span><br></pre></td></tr></table></figure>

<p><code>Base model</code></p>
<p><code>LoRA</code></p>
<p><code>LoRA</code> + <code>APG</code></p>
<p>Pixel Art</p>
<p>Prompt</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a sci-fi venetian town near the water</span><br></pre></td></tr></table></figure>

<p><code>Base model</code></p>
<p><code>LoRA</code></p>
<p><code>LoRA</code> + <code>APG</code></p>
<p>Ethnic Paint</p>
<p>Prompt</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a man in his late 30s to early 40s, rendered in a dark, moody style, The subject is depicted from the shoulders up, facing the viewer directly, He has a full, thick beard and mustache, which is dark and well-groomed, with a few strands of gray, His hair is short and neatly combed, with a few strands falling over his forehead, His eyes are dark and piercing, with a slight hint of sadness or introspection, </span><br></pre></td></tr></table></figure>

<p><code>Base model</code></p>
<p><code>LoRA</code></p>
<p><code>LoRA</code> + <code>APG</code></p>
<p><code>APG</code> 는 그 말에 충실한 것 같습니다. 채도를 줄여줍니다. 개인적으로 나는 바랜 색상을 선호하지 않지만 밋밋한 “RAW” 같은 이미지를 얻을 수 있는 좋은 방법이 될 수 있습니다.</p>
<h2 id="Other-fine-tuning-tools-x2F-libraries-for-SD3-5"><a href="#Other-fine-tuning-tools-x2F-libraries-for-SD3-5" class="headerlink" title="Other fine-tuning tools&#x2F;libraries for SD3.5"></a>Other fine-tuning tools&#x2F;libraries for SD3.5</h2><p>Hugging Face의 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://huggingface.co/blog/sd3-5#training-loras-with-sd35-large-with-Quantization">이 스크립트 및 구성</a>을 참조하세요. 이는 사용하기가 더 간단하지만 결과는 약간 더 나쁠 수 있습니다.</p>
<h2 id="Conclusion-amp-Feedback"><a href="#Conclusion-amp-Feedback" class="headerlink" title="Conclusion &amp; Feedback"></a>Conclusion &amp; Feedback</h2><p>여기 있는 모든 정보가 출시일에 SD3.5 Large를 미세 조정하는 데 도움이 되기를 바랍니다. ‘DiT’ 아키텍처는 여전히 상대적으로 새로운 것이기 때문에 우리는 구성, 질감 및 전체적인 미학 측면에서 최고의 이미지 품질을 달성하기 위해 다양한 방법을 시도했습니다. 최상의 결과를 얻지 못하는 문제가 발생하는 경우 훈련 중에 보다 세부적인 레이어 조작을 적극 권장합니다.</p>
<h2 id="Two-cents-from-Dango"><a href="#Two-cents-from-Dango" class="headerlink" title="Two cents from Dango"></a>Two cents from Dango</h2><p>따라서 SD3.5 시리즈의 주요 설계자 중 하나인 Dango의 추가 정보는 다음과 같습니다.</p>
<p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://huggingface.co/Dango233">Dango’s Hugging Face profile</a></p>
<h3 id="Diving-into-SD3-5-Large-Architecture"><a href="#Diving-into-SD3-5-Large-Architecture" class="headerlink" title="Diving into SD3.5 Large Architecture"></a>Diving into SD3.5 Large Architecture</h3><p>SD 3.5 Large의 큰 그림을 이해하기 위해 먼저 아키텍처를 인쇄해 보겠습니다.</p>
<p>모델을 로컬 디렉터리에 다운로드하는 경우 <code>stable-diffusion-3-medium-diffusers</code>와 유사한 파일 구조를 가져야 합니다.</p>
<p>SD3.5 Large의 경우 다음과 같습니다.</p>
<p>키를 나열하려고 하면 샤딩된 디퓨저 형식의 기본 모델에서 오류가 발생하므로 이를 단일 모델로 병합하는 코드입니다. 이 시점에서 나는 모델의 로컬 버전으로 작업하고 있었지만 <code>.cache</code>에 다운로드한 Hugging Face 버전과 동일합니다.</p>
<p>Example path:</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home-kasukanra/.cache/huggingface/hub/models--stabilityai--stable-diffusion-3.5-large/snapshots/1a43aa3b9bb52ead637f9693a228092aa802a5dd/transformer</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import safetensors.torch</span><br><span class="line"></span><br><span class="line">shards = [</span><br><span class="line">    &quot;/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/test_convert/transformer/diffusion_pytorch_model-00001-of-00002.safetensors&quot;,</span><br><span class="line">    &quot;/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/test_convert/transformer/diffusion_pytorch_model-00002-of-00002.safetensors&quot;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># Initialize an empty state dictionary</span><br><span class="line">combined_state_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"># Load each shard and merge into combined_state_dict</span><br><span class="line">for shard in shards:</span><br><span class="line">    ckpt = safetensors.torch.load_file(shard)</span><br><span class="line">    combined_state_dict.update(ckpt)</span><br><span class="line"></span><br><span class="line"># Specify the output path for the combined model</span><br><span class="line">output_path = &quot;/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/merged/combined_model.safetensors&quot;</span><br><span class="line"></span><br><span class="line"># Save the combined state dictionary to a single .safetensors file</span><br><span class="line">safetensors.torch.save_file(combined_state_dict, output_path)</span><br><span class="line">print(f&quot;Combined model saved successfully at &#123;output_path&#125;&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>제 경우에는 병합된 모델(<code>combined_model.safetensors</code>)이 있으면 이 스크립트를 실행하여 아키텍처를 텍스트 파일에 저장하세요. 스크립트는 변환기 모델의 일반적인 순차 흐름을 출력합니다.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import safetensors.torch</span><br><span class="line">import re</span><br><span class="line">import json</span><br><span class="line">from collections import defaultdict</span><br><span class="line"></span><br><span class="line">def group_keys(keys):</span><br><span class="line">    groups = defaultdict(list)</span><br><span class="line">    for key in keys:</span><br><span class="line">        if &#x27;transformer_blocks&#x27; in key:</span><br><span class="line">            block_num = int(re.search(r&#x27;transformer_blocks\.(\d+)&#x27;, key).group(1))</span><br><span class="line">            groups[f&#x27;transformer_block_&#123;block_num&#125;&#x27;].append(key)</span><br><span class="line">        elif &#x27;embed&#x27; in key:</span><br><span class="line">            groups[&#x27;embedding&#x27;].append(key)</span><br><span class="line">        elif &#x27;pos_embed&#x27; in key:</span><br><span class="line">            groups[&#x27;positional_embedding&#x27;].append(key)</span><br><span class="line">        elif &#x27;time_text_embed&#x27; in key:</span><br><span class="line">            groups[&#x27;time_text_embedding&#x27;].append(key)</span><br><span class="line">        elif &#x27;norm_out&#x27; in key:</span><br><span class="line">            groups[&#x27;output_normalization&#x27;].append(key)</span><br><span class="line">        elif &#x27;proj_out&#x27; in key:</span><br><span class="line">            groups[&#x27;output_projection&#x27;].append(key)</span><br><span class="line">        else:</span><br><span class="line">            groups[&#x27;other&#x27;].append(key)</span><br><span class="line">    return groups</span><br><span class="line"></span><br><span class="line">def order_groups(groups):</span><br><span class="line">    order = [</span><br><span class="line">        &#x27;embedding&#x27;,</span><br><span class="line">        &#x27;positional_embedding&#x27;,</span><br><span class="line">        &#x27;time_text_embedding&#x27;,</span><br><span class="line">    ] + [f&#x27;transformer_block_&#123;i&#125;&#x27; for i in range(38)] + [</span><br><span class="line">        &#x27;output_normalization&#x27;,</span><br><span class="line">        &#x27;output_projection&#x27;,</span><br><span class="line">        &#x27;other&#x27;</span><br><span class="line">    ]</span><br><span class="line">    return &#123;k: groups[k] for k in order if k in groups&#125;</span><br><span class="line"></span><br><span class="line">def pretty_print_and_save(ckpt, output_file):</span><br><span class="line">    keys_list = list(ckpt.keys())</span><br><span class="line">    grouped_keys = group_keys(keys_list)</span><br><span class="line">    ordered_groups = order_groups(grouped_keys)</span><br><span class="line">    </span><br><span class="line">    output = []</span><br><span class="line">    for group, keys in ordered_groups.items():</span><br><span class="line">        output.append(f&quot;\n&#123;group.upper()&#125;:&quot;)</span><br><span class="line">        output.extend(sorted(keys))</span><br><span class="line">    </span><br><span class="line">    pretty_output = &#x27;\n&#x27;.join(output)</span><br><span class="line">    </span><br><span class="line">    with open(output_file, &#x27;w&#x27;) as f:</span><br><span class="line">        f.write(pretty_output)</span><br><span class="line">    </span><br><span class="line">    print(f&quot;Grouped keys have been saved to &#123;output_file&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># Load the checkpoint</span><br><span class="line">checkpoint_path = &quot;/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/merged/combined_model.safetensors&quot;</span><br><span class="line">ckpt = safetensors.torch.load_file(checkpoint_path)</span><br><span class="line"></span><br><span class="line"># Pretty-print and save the grouped keys to a file</span><br><span class="line">output_file = &quot;ckpt_keys_grouped_output.txt&quot;</span><br><span class="line">pretty_print_and_save(ckpt, output_file)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr>
<ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://stabilityai.notion.site/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6">Stable Diffusion 3.5 Large Fine-tuning Tutorial</a></li>
</ul>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="../../../tags/stable-diffusion/">stable diffusion</a><a class="post-meta__tags" href="../../../tags/LORA/">LORA</a><a class="post-meta__tags" href="../../../tags/Lora/">Lora</a><a class="post-meta__tags" href="../../../tags/%ED%9B%88%EB%A0%A8/">훈련</a><a class="post-meta__tags" href="../../../tags/%ED%95%99%EC%8A%B5/">학습</a><a class="post-meta__tags" href="../../../tags/SD-3-5/">SD 3.5</a><a class="post-meta__tags" href="../../../tags/Large/">Large</a><a class="post-meta__tags" href="../../../tags/Fine-tuning/">Fine-tuning</a><a class="post-meta__tags" href="../../../tags/Tutorial/">Tutorial</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="../../11/2024-11-01-python_poetry/"><i class="fa fa-chevron-left">  </i><span>Poetry: debugging</span></a></div><div class="next-post pull-right"><a href="../2024-10-18-Effective_Python_CHAPTER_2/"><span>CHAPTER 2 리스트와 딕셔너리</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'https://sejoung.github.io/2024/10/2024-10-25-Stable%20Diffusion_3_5_Large_Fine-tuning_Tutorial/';
  this.page.identifier = '2024/10/2024-10-25-Stable Diffusion_3_5_Large_Fine-tuning_Tutorial/';
  this.page.title = 'Stable Diffusion 3.5 Large Fine-tuning Tutorial';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'kimsejoung' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-scr" src="https://kimsejoung.disqus.com/count.js" async></script></div></div><footer class="footer-bg" style="background-image: url(https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Van_Gogh_-_Terrasse_des_Caf%C3%A9s_an_der_Place_du_Forum_in_Arles_am_Abend1.jpeg/1024px-Van_Gogh_-_Terrasse_des_Caf%C3%A9s_an_der_Place_du_Forum_in_Arles_am_Abend1.jpeg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2025 By sejoung</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="external nofollow noopener noreferrer" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="../../../js/third-party/anime.min.js"></script><script src="../../../js/third-party/jquery.min.js"></script><script src="../../../js/third-party/jquery.fancybox.min.js"></script><script src="../../../js/third-party/velocity.min.js"></script><script src="../../../js/third-party/velocity.ui.min.js"></script><script src="../../../js/utils.js?version=1.7.0"></script><script src="../../../js/fancybox.js?version=1.7.0"></script><script src="../../../js/sidebar.js?version=1.7.0"></script><script src="../../../js/copy.js?version=1.7.0"></script><script src="../../../js/fireworks.js?version=1.7.0"></script><script src="../../../js/transition.js?version=1.7.0"></script><script src="../../../js/scroll.js?version=1.7.0"></script><script src="../../../js/head.js?version=1.7.0"></script><script src="../../../js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>