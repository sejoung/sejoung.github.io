<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>폭간의 기술블로그</title>
    <link>https://sejoung.github.io/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>잘정리하자</description>
    <pubDate>Tue, 02 Sep 2025 07:13:40 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Git LFS와 pre-commit 훅으로 대용량 파일 자동 추적하기</title>
      <link>https://sejoung.github.io/2025/09/2025-09-02-git_lfs_pre_commit/</link>
      <guid>https://sejoung.github.io/2025/09/2025-09-02-git_lfs_pre_commit/</guid>
      <pubDate>Tue, 02 Sep 2025 07:13:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Git-LFS와-pre-commit-훅으로-대용량-파일-자동-추적하기&quot;&gt;&lt;a href=&quot;#Git-LFS와-pre-commit-훅으로-대용량-파일-자동-추적하기&quot; class=&quot;headerlink&quot; title=&quot;Git LFS와 pre-com
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Git-LFS와-pre-commit-훅으로-대용량-파일-자동-추적하기"><a href="#Git-LFS와-pre-commit-훅으로-대용량-파일-자동-추적하기" class="headerlink" title="Git LFS와 pre-commit 훅으로 대용량 파일 자동 추적하기"></a>Git LFS와 pre-commit 훅으로 대용량 파일 자동 추적하기</h1><p>pre-commit 파일</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/usr/bin/env bash</span></span><br><span class="line">set -euo pipefail</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">===== 설정: 임계값 100MB =====</span></span><br><span class="line">THRESH=$((100*1024*1024))</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">git-lfs 설치 여부 확인</span></span><br><span class="line">if ! command -v git-lfs &gt;/dev/null 2&gt;&amp;1; then</span><br><span class="line">  echo &quot;[pre-commit] git-lfs not found. Run &#x27;git lfs install&#x27; first.&quot; &gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">스테이징된 추가/수정 파일 목록</span></span><br><span class="line">files=$(git diff --cached --name-only --diff-filter=AM -z | tr &#x27;\0&#x27; &#x27;\n&#x27;)</span><br><span class="line"></span><br><span class="line">for f in $files; do</span><br><span class="line">  [ -f &quot;$f&quot; ] || continue</span><br><span class="line">  size=$(stat -c%s &quot;$f&quot; 2&gt;/dev/null || stat -f%z &quot;$f&quot;)</span><br><span class="line"></span><br><span class="line">  if [ &quot;$size&quot; -ge &quot;$THRESH&quot; ]; then</span><br><span class="line">    echo &quot;[LFS] &#x27;$f&#x27; is $size bytes (&gt;= $THRESH). Tracking with Git LFS...&quot;</span><br><span class="line">    git lfs track -- &quot;$f&quot; &gt;/dev/null</span><br><span class="line">    git add .gitattributes &quot;$f&quot;</span><br><span class="line">    echo &quot;[LFS] Tracked + re-staged: $f&quot;</span><br><span class="line">  fi</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">exit 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="hooksPath-확인"><a href="#hooksPath-확인" class="headerlink" title="hooksPath 확인"></a>hooksPath 확인</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --get core.hooksPath</span><br></pre></td></tr></table></figure><h2 id="로컬-설정"><a href="#로컬-설정" class="headerlink" title="로컬 설정"></a>로컬 설정</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --local core.hooksPath .githooks</span><br></pre></td></tr></table></figure><h2 id="글로벌-설정"><a href="#글로벌-설정" class="headerlink" title="글로벌 설정"></a>글로벌 설정</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global core.hooksPath .githooks</span><br></pre></td></tr></table></figure><h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/09/2025-09-02-git_lfs_pre_commit/#disqus_thread</comments>
    </item>
    
    <item>
      <title>JEP 444: Virtual Threads</title>
      <link>https://sejoung.github.io/2025/08/2025-08-11-JEP_444/</link>
      <guid>https://sejoung.github.io/2025/08/2025-08-11-JEP_444/</guid>
      <pubDate>Mon, 11 Aug 2025 05:41:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Virtual-Threads&quot;&gt;&lt;a href=&quot;#Virtual-Threads&quot; class=&quot;headerlink&quot; title=&quot;Virtual Threads&quot;&gt;&lt;/a&gt;Virtual Threads&lt;/h1&gt;&lt;h2 id=&quot;Summary&quot;&gt;&lt;a h
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Virtual-Threads"><a href="#Virtual-Threads" class="headerlink" title="Virtual Threads"></a>Virtual Threads</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Java 플랫폼에 가상 스레드를 소개합니다.<br>가상 스레드는 처리량이 높은 동시 애플리케이션의 작성, 유지 관리 및 모니터링에 드는 노력을 크게 줄여주는 가벼운 스레드입니다.</p><h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><p>가상 스레드는 JEP 425 에서 미리보기 기능으로 제안되어 JDK 19 에 포함되었습니다.<br>피드백을 위한 시간을 확보하고 더 많은 경험을 얻기 위해 JEP 436 에서 다시 미리보기 기능으로 제안되어 JDK 20 에 포함되었습니다.<br>이 JEP는 개발자 피드백을 반영하여 JDK 20에서 다음과 같은 변경 사항을 적용하여 JDK 21에서 가상 스레드를 완성할 것을 제안합니다.</p><ul><li><p>가상 스레드는 이제 항상 스레드 로컬 변수를 지원합니다.<br>미리보기 릴리스에서처럼 스레드 로컬 변수를 가질 수 없는 가상 스레드를 생성하는 것은 더 이상 불가능합니다.<br>스레드 로컬 변수에 대한 지원이 보장됨에 따라, 더 많은 기존 라이브러리를 가상 스레드에서 변경 없이 사용할 수 있으며,<br>작업 지향 코드를 가상 스레드로 마이그레이션하는 데 도움이 됩니다.</p></li><li><p>API를 통해 직접 생성된 가상 스레드 Thread.Builder(를 통해 생성된 스레드와 반대 Executors.newVirtualThreadPerTaskExecutor())는<br>이제 기본적으로 수명 내내 모니터링되며 가상 스레드 관찰 섹션에 설명된 새 스레드 덤프를 통해 관찰할 수 있습니다.</p></li></ul><h2 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h2><ul><li><p>간단한 요청당 스레드 스타일로 작성된 서버 애플리케이션을 최적에 가까운 하드웨어 활용도로 확장할 수 있습니다.</p></li><li><p>최소한의 변경으로 API를 사용하는 기존 코드에서 java.lang.Thread가상 스레드를 채택할 수 있도록 합니다.</p></li><li><p>기존 JDK 도구를 사용하여 가상 스레드의 쉬운 문제 해결, 디버깅 및 프로파일링을 지원합니다.</p></li></ul><h2 id="Non-Goals"><a href="#Non-Goals" class="headerlink" title="Non-Goals"></a>Non-Goals</h2><ul><li>기존 스레드 구현을 제거하거나 기존 애플리케이션을 가상 스레드를 사용하도록 자동으로 마이그레이션하는 것이 목표가 아닙니다.</li><li>Java의 기본 동시성 모델을 변경하는 것이 목표가 아닙니다.</li><li>Java 언어나 Java 라이브러리에서 새로운 데이터 병렬 처리 구조를 제공하는 것이 목표가 아닙니다.<br>Stream API는 대용량 데이터 세트를 병렬로 처리하는 데 여전히 선호되는 방식입니다.</li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Java 개발자들은 거의 30년 동안 동시 서버 애플리케이션의 구성 요소로 스레드를 사용해 왔습니다.<br>모든 메서드의 모든 명령문은 스레드 내에서 실행되며, Java는 멀티스레드이므로 여러 스레드가 동시에 실행됩니다.<br>스레드는 Java의 동시성 단위입니다. 다른 단위와 동시에(그리고 대체로 독립적으로) 실행되는 순차적인 코드 조각입니다.<br>각 스레드는 로컬 변수를 저장하고 메서드 호출을 조정하는 스택을 제공하며, 문제 발생 시 컨텍스트를 제공합니다.<br>동일한 스레드의 메서드에서 예외가 발생하고 포착되므로 개발자는 스레드의 스택 추적을 사용하여 발생한 상황을 파악할 수 있습니다.<br>스레드는 도구의 핵심 개념이기도 합니다. 디버거는 스레드 메서드의 명령문을 단계별로 실행하고,<br>프로파일러는 여러 스레드의 동작을 시각화하여 성능을 파악하는 데 도움을 줍니다.</p><h3 id="The-thread-per-request-style"><a href="#The-thread-per-request-style" class="headerlink" title="The thread-per-request style"></a>The thread-per-request style</h3><p>서버 애플리케이션은 일반적으로 서로 독립적인 동시 사용자 요청을 처리하므로,<br>애플리케이션이 요청 전체 기간 동안 해당 요청에 전용 스레드를 할당하여 처리하는 것이 합리적입니다.<br>이러한 요청당 스레드 방식은 플랫폼의 동시성 단위를 사용하여 애플리케이션의 동시성 단위를 나타내므로 이해하기 쉽고,<br>프로그래밍하기 쉬우며, 디버깅 및 프로파일링이 쉽습니다.</p><p>서버 애플리케이션의 확장성은 대기 시간, 동시성 및 처리량과 관련된 Little의 법칙 에 의해 지배됩니다.<br>주어진 요청 처리 기간(즉, 대기 시간) 동안 애플리케이션이 동시에 처리하는 요청 수(즉, 동시성)는 도착 속도(즉, 처리량)에 비례하여 증가해야 합니다.<br>예를 들어, 평균 대기 시간이 50ms인 애플리케이션이 10개의 요청을 동시에 처리하여 초당 200개의 요청 처리량을 달성한다고 가정해 보겠습니다.<br>해당 애플리케이션이 초당 2000개의 요청 처리량으로 확장되려면 100개의 요청을 동시에 처리해야 합니다.<br>각 요청이 요청 기간 동안 스레드에서 처리되는 경우 애플리케이션이 따라가려면 처리량이 증가함에 따라 스레드 수도 증가해야 합니다.</p><p>안타깝게도 JDK는 스레드를 운영 체제(OS) 스레드를 감싸는 래퍼로 구현하기 때문에 사용 가능한 스레드 수가 제한적입니다.<br>OS 스레드는 비용이 많이 들기 때문에 너무 많을 수 없으므로 요청당 스레드 구현 방식에는 적합하지 않습니다.<br>각 요청이 해당 기간 동안 스레드, 즉 OS 스레드를 사용하게 되면 CPU나 네트워크 연결과 같은 다른 리소스가 고갈되기 훨씬 전에 스레드 수가 제한 요소가 되는 경우가 많습니다.<br>JDK의 현재 스레드 구현은 애플리케이션의 처리량을 하드웨어가 지원할 수 있는 수준보다 훨씬 낮은 수준으로 제한합니다.<br>이는 스레드를 풀링하는 경우에도 마찬가지입니다. 풀링은 새 스레드를 시작하는 데 드는 높은 비용을 피하는 데 도움이 되지만 총 스레드 수는 증가하지 않기 때문입니다.</p><h3 id="Improving-scalability-with-the-asynchronous-style"><a href="#Improving-scalability-with-the-asynchronous-style" class="headerlink" title="Improving scalability with the asynchronous style"></a>Improving scalability with the asynchronous style</h3><p>하드웨어를 최대한 활용하고자 하는 일부 개발자들은 요청당 스레드 방식을 포기하고 스레드 공유 방식을 채택했습니다.<br>요청 처리 코드는 처음부터 끝까지 한 스레드에서 요청을 처리하는 대신,<br>다른 I&#x2F;O 작업이 완료될 때까지 대기할 때 해당 스레드를 풀로 반환하여 다른 요청을 처리할 수 있도록 합니다.<br>이러한 세밀한 스레드 공유 방식은 코드가 계산을 수행하는 동안만 스레드를 유지하고 I&#x2F;O를 기다리는 동안은 유지하지 않으므로,<br>많은 스레드를 소모하지 않고도 많은 수의 동시 작업을 처리할 수 있도록 합니다.<br>OS 스레드 부족으로 인한 처리량 제한은 해소되지만, 높은 비용이 발생합니다.<br>비동기 프로그래밍 방식이라고 하는, I&#x2F;O 작업이 완료될 때까지 기다리지 않고 나중에 완료를 콜백에 알리는 별도의 I&#x2F;O 메서드 집합을 사용하는 방식이 필요합니다.<br>전용 스레드가 없으면 개발자는 요청 처리 로직을 작은 단계로 나누어야 합니다.<br>일반적으로 람다 표현식으로 작성하고, 이를 API( 예: CompletableFuture 또는 소위 “반응형” 프레임워크)를 사용하여 순차적인 파이프라인으로 구성해야 합니다.<br>따라서 루프나 블록과 같은 언어의 기본적인 순차적 구성 연산자를 사용하지 않게 됩니다 try&#x2F;catch.</p><p>비동기 방식에서는 요청의 각 단계가 서로 다른 스레드에서 실행될 수 있으며, 각 스레드는 서로 다른 요청에 속하는 단계들을 인터리브 방식으로 실행합니다.<br>이는 프로그램 동작을 이해하는 데 중요한 의미를 지닙니다. 스택 추적은 사용 가능한 컨텍스트를 제공하지 않고,<br>디버거는 요청 처리 로직을 단계별로 실행할 수 없으며, 프로파일러는 작업 비용을 호출자와 연관시킬 수 없습니다.<br>Java 스트림 API를 사용하여 짧은 파이프라인에서 데이터를 처리할 때는 람다 표현식을 작성하는 것이 어렵지 않지만,<br>애플리케이션의 모든 요청 처리 코드를 이러한 방식으로 작성해야 하는 경우에는 문제가 됩니다.<br>이러한 프로그래밍 방식은 애플리케이션의 동시성 단위인 비동기 파이프라인이 더 이상 플랫폼의 동시성 단위가 아니기 때문에 Java 플랫폼과 상충됩니다.</p><h3 id="Preserving-the-thread-per-request-style-with-virtual-threads"><a href="#Preserving-the-thread-per-request-style-with-virtual-threads" class="headerlink" title="Preserving the thread-per-request style with virtual threads"></a>Preserving the thread-per-request style with virtual threads</h3><p>애플리케이션이 플랫폼과의 조화를 유지하면서 확장 가능하도록 하려면 요청당 스레드 방식을 유지해야 합니다.<br>스레드를 더 효율적으로 구현하여 더 풍부하게 만들면 이를 달성할 수 있습니다.<br>운영 체제는 다양한 언어와 런타임이 스레드 스택을 사용하는 방식이 다르기 때문에 OS 스레드를 더 효율적으로 구현할 수 없습니다.<br>그러나 Java 런타임은 Java 스레드를 OS 스레드와의 일대일 대응 관계를 끊는 방식으로 구현할 수 있습니다.<br>운영 체제가 큰 가상 주소 공간을 제한된 양의 물리적 RAM에 매핑하여 메모리가 풍부한 것처럼 보이도록 하는 것처럼,<br>Java 런타임은 많은 수의 가상 스레드를 적은 수의 OS 스레드에 매핑하여 스레드가 풍부한 것처럼 보이도록 할 수 있습니다.</p><p>가상 스레드java.lang.Thread 는 특정 OS 스레드에 종속되지 않은 인스턴스입니다.<br>반면 플랫폼 스레드는java.lang.Thread OS 스레드를 감싸는 얇은 래퍼 형태로 기존 방식으로 구현된 인스턴스입니다.</p><p>요청당 스레드 방식의 애플리케이션 코드는 요청 전체 기간 동안 가상 스레드에서 실행될 수 있지만,<br>가상 스레드는 CPU에서 계산을 수행하는 동안만 OS 스레드를 사용합니다. 결과적으로 비동기 방식과 동일한 확장성을 제공하지만, 투명하게 구현됩니다.<br>가상 스레드에서 실행되는 코드가 API에서 블로킹 I&#x2F;O 작업을 호출하면 java.*런타임은 비블로킹 OS 호출을 수행하고 나중에 다시 시작할 수 있을 때까지<br>가상 스레드를 자동으로 일시 중단합니다. Java 개발자에게 가상 스레드는 단순히 생성 비용이 저렴하고 거의 무한히 풍부한 스레드입니다.<br>하드웨어 활용도가 최적에 가까워 높은 수준의 동시성과 그 결과 높은 처리량을 제공하는 동시에 애플리케이션은 Java 플랫폼 및 해당 도구의 멀티스레드 설계와 조화를 이룹니다.</p><h3 id="Implications-of-virtual-threads"><a href="#Implications-of-virtual-threads" class="headerlink" title="Implications of virtual threads"></a>Implications of virtual threads</h3><p>가상 스레드는 저렴하고 풍부하므로 절대 풀링해서는 안 됩니다. 모든 애플리케이션 작업마다 새로운 가상 스레드를 생성해야 합니다.<br>따라서 대부분의 가상 스레드는 수명이 짧고 호출 스택이 얕아 단일 HTTP 클라이언트 호출이나 단일 JDBC 쿼리 정도만 수행합니다.<br>반면 플랫폼 스레드는 무겁고 비용이 많이 들기 때문에 풀링해야 하는 경우가 많습니다.<br>플랫폼 스레드는 일반적으로 수명이 길고 호출 스택이 깊으며 여러 작업에서 공유됩니다.</p><p>요약하자면, 가상 스레드는 Java 플랫폼의 설계와 조화를 이루는 안정적인 요청당 스레드 스타일을 유지하면서도 사용 가능한 하드웨어를 최적으로 활용합니다.<br>가상 스레드를 사용한다고 해서 새로운 개념을 배울 필요는 없지만, 오늘날의 높은 스레드 비용에 대처하기 위해 개발된 습관을 버려야 할 수도 있습니다.<br>가상 스레드는 애플리케이션 개발자에게 도움이 될 뿐만 아니라, 프레임워크 설계자가 확장성을 저해하지 않으면서<br>플랫폼 설계와 호환되는 사용하기 쉬운 API를 제공하는 데에도 도움이 됩니다.</p><h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>오늘날 java.lang.ThreadJDK 의 모든 인스턴스는 플랫폼 스레드 입니다.<br>플랫폼 스레드는 기본 OS 스레드에서 Java 코드를 실행하고 코드의 전체 수명 동안 OS 스레드를 캡처합니다.<br>플랫폼 스레드의 수는 OS 스레드 수로 제한됩니다.</p><p>가상 스레드 는 기본 OS 스레드에서 Java 코드를 실행하지만, 코드의 전체 수명 동안 OS 스레드를 캡처하지 않는 인스턴스입니다<br>java.lang.Thread. 즉, 여러 가상 스레드가 동일한 OS 스레드에서 Java 코드를 실행하여 사실상 공유할 수 있습니다. 플랫폼 스레드는 귀중한 OS 스레드를 독점하는 반면,<br>가상 스레드는 그렇지 않습니다. 가상 스레드의 수는 OS 스레드 수보다 훨씬 많을 수 있습니다.</p><p>가상 스레드는 OS가 아닌 JDK에서 제공하는 가벼운 스레드 구현입니다.<br>가상 스레드는 사용자 모드 스레드 의 한 형태로, 다른 멀티스레드 언어(예: Go의 고루틴, Erlang의 프로세스)에서 성공적으로 사용되었습니다.<br>사용자 모드 스레드는 OS 스레드가 아직 성숙하고 널리 보급되지 않았던 초기 Java 버전에서 소위 “그린 스레드” 로 등장하기도 했습니다.<br>그러나 Java의 그린 스레드는 모두 하나의 OS 스레드를 공유했으며(M:1 스케줄링),<br>결국 OS 스레드의 래퍼로 구현된 플랫폼 스레드(1:1 스케줄링)에 밀려 성능이 떨어졌습니다.<br>가상 스레드는 M:N 스케줄링을 사용하는데, 이는 많은 수(M)의 가상 스레드가 적은 수(N)의 OS 스레드에서 실행되도록 스케줄링되는 방식입니다.</p><h3 id="Using-virtual-threads-vs-platform-threads"><a href="#Using-virtual-threads-vs-platform-threads" class="headerlink" title="Using virtual threads vs. platform threads"></a>Using virtual threads vs. platform threads</h3><p>개발자는 가상 스레드와 플랫폼 스레드 중 어떤 것을 사용할지 선택할 수 있습니다.<br>다음은 다수의 가상 스레드를 생성하는 예제 프로그램입니다.<br>이 프로그램은 먼저 ExecutorService제출된 각 작업에 대해 새로운 가상 스레드를 생성하는 를 가져옵니다.<br>그런 다음 10,000개의 작업을 제출하고 모든 작업이 완료될 때까지 기다립니다.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> (<span class="type">var</span> <span class="variable">executor</span> <span class="operator">=</span> Executors.newVirtualThreadPerTaskExecutor()) &#123;</span><br><span class="line">    IntStream.range(<span class="number">0</span>, <span class="number">10_000</span>).forEach(i -&gt; &#123;</span><br><span class="line">        executor.submit(() -&gt; &#123;</span><br><span class="line">            Thread.sleep(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">return</span> i;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;  <span class="comment">// executor.close() is called implicitly, and waits</span></span><br></pre></td></tr></table></figure><p>이 예제에서 작업은 간단한 코드, 즉 1초간의 대기 시간으로 이루어지며, 최신 하드웨어는 이러한 코드를 동시에 실행하는 10,000개의 가상 스레드를 쉽게 지원할 수 있습니다.<br>하지만 JDK는 내부적으로 소수의 OS 스레드, 아마도 하나 정도의 스레드에서 코드를 실행합니다.</p><p>ExecutorService이 프로그램이 각 작업에 대해 새로운 플랫폼 스레드를 생성하는 를 사용한다면 상황이 매우 달라질 것입니다 Executors.newCachedThreadPool().<br>예를 들어 ExecutorService, 는 10,000개의 플랫폼 스레드와 10,000개의 OS 스레드를 생성하려고 시도하며, 컴퓨터와 운영 체제에 따라 프로그램이 충돌할 수 있습니다.</p><p>프로그램이 대신 ExecutorService풀에서 플랫폼 스레드를 가져오는 를 사용한다면 상황은 그다지 나아지지 않을 것입니다 Executors.newFixedThreadPool(200). 는<br>ExecutorService 10,000개의 모든 작업에서 공유할 200개의 플랫폼 스레드를 생성하므로 많은 작업이 동시에 실행되지 않고<br>순차적으로 실행되고 프로그램을 완료하는 데 오랜 시간이 걸립니다. 이 프로그램의 경우 200개의 플랫폼 스레드가 있는 풀은<br>초당 200개의 작업 처리량만 달성할 수 있는 반면 가상 스레드는 (충분한 워밍업 후) 초당 약 10,000개의 작업 처리량을 달성합니다.<br>또한 10_000예제 프로그램의 를 로 변경하면 1_000_000프로그램은 1,000,000개의 작업을 제출하고 동시에 실행되는 1,000,000개의 가상 스레드를 만들고<br>(충분한 워밍업 후) 초당 약 1,000,000개의 작업 처리량을 달성합니다.</p><p>이 프로그램의 작업들이 단순히 휴면 상태가 아니라 1초 동안 계산(예: 거대한 배열 정렬)을 수행한다면,<br>프로세서 코어 수를 초과하는 스레드 수를 늘리는 것은 가상 스레드든 플랫폼 스레드든 아무런 도움이 되지 않습니다.<br>가상 스레드는 더 빠른 스레드가 아닙니다. 플랫폼 스레드보다 코드를 더 빠르게 실행하지도 않습니다.<br>가상 스레드는 속도(낮은 지연 시간)가 아닌 확장성(높은 처리량)을 제공하기 위해 존재합니다.<br>플랫폼 스레드보다 훨씬 더 많은 수의 가상 스레드가 존재할 수 있으므로, 리틀의 법칙에 따라 더 높은 처리량에 필요한 높은 동시성을 가능하게 합니다.</p><p>다시 말해, 가상 스레드는 다음과 같은 경우 애플리케이션 처리량을 크게 향상시킬 수 있습니다.</p><ul><li>동시 작업 수가 많고(수천개 이상)</li><li>작업 부하는 CPU에 국한되지 않습니다. 프로세서 코어보다 스레드가 훨씬 많으면 처리량이 향상될 수 없기 때문입니다.</li></ul><p>가상 스레드는 일반적인 서버 애플리케이션의 처리량을 개선하는 데 도움이 됩니다.<br>이러한 애플리케이션은 많은 수의 동시 작업으로 구성되어 있고 작업의 상당 부분이 대기하는 데 소요되기 때문입니다.</p><p>가상 스레드는 플랫폼 스레드가 실행할 수 있는 모든 코드를 실행할 수 있습니다.<br>특히 가상 스레드는 플랫폼 스레드와 마찬가지로 스레드 로컬 변수 와 스레드 인터럽트를 지원합니다.<br>즉, 요청을 처리하는 기존 Java 코드는 가상 스레드에서 쉽게 실행될 수 있습니다.<br>많은 서버 프레임워크는 이 작업을 자동으로 수행하여 모든 수신 요청에 대해 새 가상 스레드를 시작하고 애플리케이션의 비즈니스 로직을 해당 스레드에서 실행합니다.</p><p>다음은 두 개의 다른 서비스의 결과를 집계하는 서버 애플리케이션의 예입니다.<br>가상의 서버 프레임워크(그림에는 표시되지 않음)는 각 요청에 대해 새로운 가상 스레드를 생성하고 handle해당 가상 스레드에서 애플리케이션 코드를 실행합니다.<br>그러면 애플리케이션 코드는 ExecutorService첫 번째 예와 동일한 방식으로 두 개의 새로운 가상 스레드를 생성하여 리소스를 동시에 가져옵니다.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> <span class="title function_">handle</span><span class="params">(Request request, Response response)</span> &#123;</span><br><span class="line">    <span class="type">var</span> <span class="variable">url1</span> <span class="operator">=</span> ...</span><br><span class="line">    <span class="type">var</span> <span class="variable">url2</span> <span class="operator">=</span> ...</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">try</span> (<span class="type">var</span> <span class="variable">executor</span> <span class="operator">=</span> Executors.newVirtualThreadPerTaskExecutor()) &#123;</span><br><span class="line">        <span class="type">var</span> <span class="variable">future1</span> <span class="operator">=</span> executor.submit(() -&gt; fetchURL(url1));</span><br><span class="line">        <span class="type">var</span> <span class="variable">future2</span> <span class="operator">=</span> executor.submit(() -&gt; fetchURL(url2));</span><br><span class="line">        response.send(future1.get() + future2.get());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ExecutionException | InterruptedException e) &#123;</span><br><span class="line">        response.fail(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">String <span class="title function_">fetchURL</span><span class="params">(URL url)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="keyword">try</span> (<span class="type">var</span> <span class="variable">in</span> <span class="operator">=</span> url.openStream()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">String</span>(in.readAllBytes(), StandardCharsets.UTF_8);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>이처럼 간단한 차단 코드를 사용한 서버 애플리케이션은 많은 수의 가상 스레드를 사용할 수 있기 때문에 확장성이 뛰어납니다.</p><p>Executor.newVirtualThreadPerTaskExecutor()가상 스레드를 생성하는 유일한 방법은 아닙니다.<br>아래에서 설명하는java.lang.Thread.Builder 새로운 API는 가상 스레드를 생성하고 시작할 수 있습니다.<br>또한, 구조적 동시성은 특히 이 서버 예제와 유사한 코드에서 가상 스레드를 생성하고 관리하는 더욱 강력한 API를 제공하며,<br>이를 통해 스레드 간의 관계가 플랫폼과 해당 도구에 알려집니다.</p><h3 id="Do-not-pool-virtual-threads"><a href="#Do-not-pool-virtual-threads" class="headerlink" title="Do not pool virtual threads"></a>Do not pool virtual threads</h3><p>ExecutorService개발자는 일반적으로 애플리케이션 코드를 기존 스레드 풀 기반에서 작업당 가상 스레드 방식으로 마이그레이션합니다 ExecutorService.<br>스레드 풀은 다른 리소스 풀과 마찬가지로 비용이 많이 드는 리소스를 공유하도록 설계되었지만,<br>가상 스레드는 비용이 많이 들지 않으므로 풀링할 필요가 없습니다.</p><p>개발자는 제한된 리소스에 대한 동시 접근을 제한하기 위해 스레드 풀을 사용하는 경우가 있습니다.<br>예를 들어, 서비스가 20개 이상의 동시 요청을 처리할 수 없는 경우, 크기가 20인 스레드 풀에 제출된 작업을 통해 서비스에 대한 모든 요청을 처리하면 문제가 해결됩니다.<br>플랫폼 스레드의 높은 비용으로 인해 스레드 풀이 보편화되면서 이러한 관용구가 널리 사용되기 시작했습니다.<br>하지만 동시성을 제한하기 위해 가상 스레드를 풀링하려는 유혹에 빠지지 마십시오. 대신 세마포어와 같이 해당 목적으로 특별히 설계된 구조를 사용하십시오.</p><p>개발자는 스레드 풀과 함께 스레드 로컬 변수를 사용하여 동일한 스레드를 공유하는 여러 작업 간에 비용이 많이 드는 리소스를 공유하는 경우가 있습니다.<br>예를 들어, 데이터베이스 연결을 생성하는 데 비용이 많이 드는 경우, 해당 연결을 한 번 열어 스레드 로컬 변수에 저장한 후<br>나중에 동일한 스레드의 다른 작업에서 사용할 수 있습니다. 스레드 풀을 사용하는 코드에서 작업당 가상 스레드를 사용하는 코드로 마이그레이션하는 경우,<br>모든 가상 스레드에 대해 비용이 많이 드는 리소스를 생성하면 성능이 크게 저하될 수 있으므로 이러한 관용구 사용에 주의해야 합니다.<br>이러한 코드를 대체 캐싱 전략을 사용하도록 변경하여 비용이 많이 드는 리소스를 매우 많은 가상 스레드 간에 효율적으로 공유하도록 하십시오.</p><h3 id="Observing-virtual-threads"><a href="#Observing-virtual-threads" class="headerlink" title="Observing virtual threads"></a>Observing virtual threads</h3><p>명확한 코드를 작성하는 것만으로는 충분하지 않습니다.<br>실행 중인 프로그램의 상태를 명확하게 표현하는 것은 문제 해결, 유지 관리 및 최적화에도 필수적이며,<br>JDK는 오랫동안 스레드 디버깅, 프로파일링 및 모니터링 메커니즘을 제공해 왔습니다.<br>이러한 도구는 가상 스레드에도 동일한 기능을 제공해야 합니다. 단, 가상 스레드의 양이 많기 때문에 어느 정도 조정이 필요할 수 있습니다.<br>결국 가상 스레드는 .NET의 인스턴스이기 때문입니다 java.lang.Thread.</p><p>자바 디버거는 가상 스레드를 단계별로 실행하고, 호출 스택을 표시하고, 스택 프레임의 변수를 검사할 수 있습니다.<br>JDK의 로우오버헤드 프로파일링 및 모니터링 메커니즘인 JDK Flight Recorder(JFR)는 애플리케이션 코드의 이벤트(예: 객체 할당 및 I&#x2F;O 작업)를<br>해당 가상 스레드와 연결할 수 있습니다. 이러한 도구는 비동기 방식으로 작성된 애플리케이션에서는 이러한 작업을 수행할 수 없습니다.<br>비동기 방식에서는 작업이 스레드와 관련이 없으므로 디버거는 작업 상태를 표시하거나 조작할 수 없으며,<br>프로파일러는 작업이 I&#x2F;O 대기에 소요되는 시간을 파악할 수 없습니다.</p><p>스레드 덤프는 요청당 스레드 방식으로 작성된 애플리케이션 문제 해결에 널리 사용되는 또 다른 도구입니다.<br>하지만 JDK의 기존 스레드 덤프는 jstack이나 를 사용하여 얻은 jcmd스레드 목록을 단순하게 표시합니다.<br>이는 수십 또는 수백 개의 플랫폼 스레드에는 적합하지만, 수천 또는 수백만 개의 가상 스레드에는 적합하지 않습니다.<br>따라서 기존 스레드 덤프를 확장하여 가상 스레드를 포함하지 않습니다.<br>대신, jcmd 에서 가상 스레드와 플랫폼 스레드를 의미 있는 방식으로 그룹화하여 보여주는 새로운 유형의 스레드 덤프를 도입할 것입니다.<br>프로그램이 구조적 동시성을 사용하면 스레드 간의 더욱 풍부한 관계를 보여줄 수 있습니다 .</p><p>많은 스레드를 시각화하고 분석하는 데 도구가 도움이 될 수 있으므로 jcmd일반 텍스트 외에도 JSON 형식으로 새 스레드 덤프를 내보낼 수 있습니다.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">jcmd &lt;pid&gt; Thread.dump_to_file -format=json &lt;file&gt;</span></span><br></pre></td></tr></table></figure><p>새로운 스레드 덤프 형식에는 기존 스레드 덤프에 나타나는 객체 주소, 잠금, JNI 통계, 힙 통계 및 기타 정보가 포함되지 않습니다.<br>또한, 많은 스레드를 나열해야 할 수도 있으므로, 새 스레드 덤프를 생성해도 애플리케이션이 중단되지 않습니다.</p><p>시스템 속성을 jdk.trackAllThreads로 설정하면 false(즉 -Djdk.trackAllThreads&#x3D;false, 명령줄 옵션을 사용하여) Thread.BuilderAPI를 통해<br>직접 생성된 가상 스레드가 런타임에 추적되지 않아 새 스레드 덤프에 나타나지 않을 수 있습니다.<br>이 경우 새 스레드 덤프에는 네트워크 I&#x2F;O 작업에서 차단된 가상 스레드와 위에 표시된 작업당 새 스레드로 생성된 가상 스레드가 나열됩니다 ExecutorService.</p><p>가상 스레드는 JDK에 구현되어 있으며 특정 OS 스레드에 종속되지 않으므로 OS에서는 보이지 않고,<br>OS는 가상 스레드의 존재를 인식하지 못합니다. OS 수준 모니터링을 통해 JDK 프로세스가 가상 스레드보다 적은 OS 스레드를 사용하는 것을 확인할 수 있습니다.</p><h3 id="Scheduling-virtual-threads"><a href="#Scheduling-virtual-threads" class="headerlink" title="Scheduling virtual threads"></a>Scheduling virtual threads</h3><p>유용한 작업을 수행하려면 스레드를 스케줄링해야 합니다. 즉, 프로세서 코어에서 실행되도록 할당해야 합니다.<br>OS 스레드로 구현된 플랫폼 스레드의 경우, JDK는 OS의 스케줄러에 의존합니다. 반면, 가상 스레드의 경우 JDK는 자체 스케줄러를 사용합니다.<br>JDK 스케줄러는 가상 스레드를 프로세서에 직접 할당하는 대신, 플랫폼 스레드에 할당합니다(이는 앞서 언급한 가상 스레드의 M:N 스케줄링과 같습니다).<br>그러면 플랫폼 스레드는 평소처럼 OS에 의해 스케줄링됩니다.</p><p>JDK의 가상 스레드 스케줄러는 ForkJoinPoolFIFO 모드로 작동하는 작업 훔치기(work-stealing)입니다.<br>스케줄러의 병렬 처리 는 가상 스레드 스케줄링에 사용 가능한 플랫폼 스레드 수를 의미합니다. 기본적으로 병렬 처리는 사용 가능한 프로세서 수와 같지만,<br>시스템 속성을 사용하여 조정할 수 있습니다 jdk.virtualThreadScheduler.parallelism.<br>이는 병렬 스트림 구현 등에 사용되며 LIFO 모드로 작동하는 공통 풀ForkJoinPool 과는 다릅니다.</p><p>스케줄러가 가상 스레드를 할당하는 플랫폼 스레드를 가상 스레드의 캐리어 라고 합니다.<br>가상 스레드는 수명 주기 동안 여러 캐리어에 스케줄링될 수 있습니다. 즉, 스케줄러는 가상 스레드와 특정 플랫폼 스레드 간의 연관성을 유지하지 않습니다.<br>Java 코드 관점에서 볼 때, 실행 중인 가상 스레드는 현재 캐리어와 논리적으로 독립적입니다.</p><ul><li>가상 스레드에서는 통신사의 신원을 알 수 없습니다. 반환되는 값은 Thread.currentThread()항상 가상 스레드 자체입니다.</li><li>캐리어와 가상 스레드의 스택 추적은 별개입니다. 가상 스레드에서 발생한 예외에는 캐리어의 스택 프레임이 포함되지 않습니다. 스레드 덤프에는 가상 스레드 스택에 있는 캐리어의 스택 프레임이 표시되지 않으며, 그 반대의 경우도 마찬가지입니다.</li><li>캐리어의 스레드 로컬 변수는 가상 스레드에서 사용할 수 없으며 그 반대의 경우도 마찬가지입니다.</li></ul><p>또한, Java 코드 관점에서는 가상 스레드와 그 실행자가 일시적으로 OS 스레드를 공유한다는 사실이 눈에 띄지 않습니다.<br>반면 네이티브 코드 관점에서는 가상 스레드와 실행자가 모두 동일한 네이티브 스레드에서 실행됩니다.<br>따라서 동일한 가상 스레드에서 여러 번 호출되는 네이티브 코드는 호출될 때마다 다른 OS 스레드 식별자를 사용할 수 있습니다.</p><p>스케줄러는 현재 가상 스레드에 대한 시간 공유를 구현하지 않습니다. 시간 공유는 할당된 CPU 시간을 소비한 스레드를 강제로 선점하는 방식입니다.<br>플랫폼 스레드 수가 비교적 적고 CPU 사용률이 100%일 때 시간 공유는 일부 작업의 지연 시간을 줄이는 데 효과적일 수 있지만,<br>가상 스레드가 백만 개일 때는 시간 공유가 그만큼 효과적일지 확실하지 않습니다.</p><h3 id="Executing-virtual-threads"><a href="#Executing-virtual-threads" class="headerlink" title="Executing virtual threads"></a>Executing virtual threads</h3><p>가상 스레드를 활용하기 위해 프로그램을 다시 작성할 필요는 없습니다.<br>가상 스레드는 애플리케이션 코드가 스케줄러에게 명시적으로 제어권을 넘겨줄 것을 요구하거나 기대하지 않습니다.<br>다시 말해, 가상 스레드는 협력적이 지 않습니다. 사용자 코드는 플랫폼 스레드가 프로세서 코어에 할당되는 방식이나 시기를 가정해서는 안 되는 것처럼,<br>가상 스레드가 플랫폼 스레드에 할당되는 방식이나 시기를 가정해서는 안 됩니다.</p><p>가상 스레드에서 코드를 실행하기 위해 JDK의 가상 스레드 스케줄러는 가상 스레드를 플랫폼 스레드에 마운트하여 플랫폼 스레드에서 실행되도록 할당합니다.<br>이렇게 하면 플랫폼 스레드가 가상 스레드의 실행 매개체가 됩니다. 나중에 코드를 실행한 후 가상 스레드는 실행 매개체에서 마운트 해제 될 수 있습니다.<br>그러면 플랫폼 스레드가 해제되어 스케줄러가 다른 가상 스레드를 마운트하여 다시 실행 매개체가 됩니다.</p><p>일반적으로 가상 스레드는 JDK에서 I&#x2F;O 또는 기타 블로킹 작업(예: .)으로 인해 블로킹될 때 마운트 해제됩니다 BlockingQueue.take().<br>블로킹 작업이 완료될 준비가 되면(예: 소켓에서 바이트 수신) 가상 스레드를 스케줄러에 다시 제출하고, 스케줄러는 가상 스레드를 캐리어에 마운트하여 실행을 재개합니다.</p><p>가상 스레드의 마운트 및 언마운트 작업은 빈번하고 투명하게 수행되며, OS 스레드를 차단하지 않습니다.<br>예를 들어, 앞서 살펴본 서버 애플리케이션에는 차단 작업에 대한 호출을 포함하는 다음 코드 줄이 포함되어 있습니다.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.send(future1.get() + future2.get());</span><br></pre></td></tr></table></figure><p>이러한 작업으로 인해 가상 스레드가 여러 번 마운트 및 마운트 해제됩니다.<br>일반적으로는 호출할 때마다 한 번씩 수행되고 get(), I&#x2F;O를 수행하는 과정에서도 여러 번 수행될 수 있습니다 send(…).</p><p>JDK에서 대부분의 블로킹 작업은 가상 스레드를 마운트 해제하여 해당 스레드의 캐리어와 기본 OS 스레드가 새 작업을 수행할 수 있도록 합니다.<br>그러나 JDK의 일부 블로킹 작업은 가상 스레드를 마운트 해제하지 않아 해당 캐리어와 기본 OS 스레드를 모두 차단합니다.<br>이는 OS 수준(예: 많은 파일 시스템 작업) 또는 JDK 수준(예: Object.wait())의 제한 때문입니다.<br>이러한 블로킹 작업의 구현은 스케줄러의 병렬 처리를 일시적으로 확장하여 OS 스레드 캡처를 보완합니다.<br>결과적으로 스케줄러의 플랫폼 스레드 수가 ForkJoinPool일시적으로 사용 가능한 프로세서 수를 초과할 수 있습니다.<br>스케줄러에서 사용할 수 있는 최대 플랫폼 스레드 수는 시스템 속성을 사용하여 조정할 수 있습니다 jdk.virtualThreadScheduler.maxPoolSize.</p><p>가상 스레드가 캐리어에 고정되어 있기 때문에 차단 작업 중에 마운트 해제할 수 없는 두 가지 시나리오가 있습니다 .</p><ul><li>블록이나 메서드 내부에서 코드를 실행할 때 synchronized또는</li><li>native메서드나 외부 함수를 실행할 때 .</li></ul><p>고정은 애플리케이션을 부정확하게 만들지는 않지만, 확장성을 저해할 수 있습니다.<br>가상 스레드가 I&#x2F;O와 같은 차단 작업을 수행하거나 BlockingQueue.take()고정된 상태에서 실행되면,<br>해당 스레드의 캐리어와 기본 OS 스레드는 작업 시간 동안 차단됩니다. 장시간 빈번한 고정은 캐리어를 포착하여 애플리케이션의 확장성을 저해할 수 있습니다.</p><p>스케줄러는 병렬 처리를 확장하여 고정(pinning)을 보상하지 않습니다.<br>대신, 자주 synchronized실행되고 잠재적으로 긴 I&#x2F;O 작업을 보호하는 블록이나 메서드를 수정하여 빈번하고<br>오래 지속되는 고정을 방지하십시오 java.util.concurrent.locks.ReentrantLock. 드물게 사용되거나(예: 시작 시에만 수행됨)<br>메모리 내 작업을 보호하는 블록이나 메서드는 교체할 필요가 없습니다 synchronized. 항상 그렇듯이 잠금 정책은 간단하고 명확하게 유지하도록 노력하십시오.</p><p>새로운 진단 기능은 코드를 가상 스레드로 마이그레이션하고 특정 사용을 synchronized잠금으로 대체해야 하는지 여부를 평가하는 데 도움이 됩니다 java.util.concurrent.</p><ul><li>JDK Flight Recorder(JFR) 이벤트는 고정된 스레드가 차단될 때 발생합니다( JDK Flight Recorder 참조 ).</li><li>시스템 속성은 jdk.tracePinnedThreads스레드가 고정된 상태에서 차단될 때 스택 추적을 트리거합니다. </li><li>를 실행하면 -Djdk.tracePinnedThreads&#x3D;full스레드가 고정된 상태에서 차단될 때 전체 스택 추적이 출력되고, </li><li>네이티브 프레임과 모니터를 보유한 프레임이 강조 표시됩니다. 를 실행하면 -Djdk.tracePinnedThreads&#x3D;short문제가 있는 프레임만 출력됩니다.</li></ul><p>향후 릴리스에서는 위의 첫 번째 제한 사항, 즉 synchronized. 내부 고정 기능을 제거할 수 있을 것입니다.<br>두 번째 제한 사항은 네이티브 코드와의 원활한 상호 작용을 위해 필요합니다.</p><h3 id="Memory-use-and-interaction-with-garbage-collection"><a href="#Memory-use-and-interaction-with-garbage-collection" class="headerlink" title="Memory use and interaction with garbage collection"></a>Memory use and interaction with garbage collection</h3><p>가상 스레드 스택은 Java의 가비지 수집 힙에 스택 청크 객체로 저장됩니다.<br>스택은 애플리케이션 실행 시 메모리 효율성과 JVM에 설정된 플랫폼 스레드 스택 크기까지 깊이 있는 스택을 수용하기 위해 증가하거나 감소합니다.<br>이러한 효율성 덕분에 서버 애플리케이션에서 다수의 가상 스레드를 사용할 수 있으며, 결과적으로 요청당 스레드 방식의 지속적인 활용이 가능해집니다.</p><p>위의 두 번째 예 에서 , 가상 프레임워크가 각 요청을 처리할 때 새로운 가상 스레드를 생성하고 해당 메서드를 호출한다는 점을 기억하세요.<br>깊은 호출 스택의 끝(인증, 트랜잭션 등)에서 handle호출하더라도 , 짧은 작업만 수행하는 여러 가상 스레드가 생성됩니다.<br>따라서 깊은 호출 스택을 가진 각 가상 스레드에는 메모리를 거의 소모하지 않는 얕은 호출 스택을 가진 여러 가상 스레드가 생성됩니다.handlehandle</p><p>가상 스레드가 필요로 하는 힙 공간과 가비지 컬렉터 활동량은 일반적으로 비동기 코드와 비교하기 어렵습니다.<br>백만 개의 가상 스레드는 최소 백만 개의 객체를 필요로 하지만, 플랫폼 스레드 풀을 공유하는 백만 개의 작업도 마찬가지입니다.<br>또한, 요청을 처리하는 애플리케이션 코드는 일반적으로 I&#x2F;O 작업 전반에 걸쳐 데이터를 유지합니다.<br>요청당 스레드 코드는 해당 데이터를 로컬 변수에 보관하고, 이 로컬 변수는 힙의 가상 스레드 스택에 저장됩니다.<br>반면, 비동기 코드는 파이프라인의 한 단계에서 다음 단계로 전달되는 힙 객체에 동일한 데이터를 보관해야 합니다.<br>한편, 가상 스레드에 필요한 스택 프레임 레이아웃은 컴팩트 객체보다 낭비적입니다.<br>반면, 가상 스레드는 저수준 GC 상호 작용에 따라 여러 상황에서 스택을 변경하고 재사용할 수 있는 반면,<br>비동기 파이프라인은 항상 새 객체를 할당해야 하므로 가상 스레드가 더 적은 할당을 필요로 할 수 있습니다.<br>전반적으로 요청당 스레드와 비동기 코드의 힙 소비량과 가비지 컬렉터 활동은 거의 유사합니다.<br>시간이 지남에 따라 가상 스레드 스택의 내부 표현이 훨씬 더 간결해질 것으로 기대합니다.</p><p>플랫폼 스레드 스택과 달리 가상 스레드 스택은 GC 루트가 아닙니다.<br>따라서 가상 스레드 스택에 포함된 참조는 G1과 같은 가비지 컬렉터가 동시 힙 스캐닝을 수행하는 stop-the-world 일시 정지 상태에서 탐색되지 않습니다.</p><p>가상 스레드의 현재 한계는 G1 GC가 거대한 스택 청크 객체를 지원하지 않는다는 것입니다.<br>가상 스레드의 스택이 영역 크기의 절반(512KB 정도)에 도달하면 a가 StackOverflowErrorthrow될 수 있습니다.</p><h3 id="Detailed-changes"><a href="#Detailed-changes" class="headerlink" title="Detailed changes"></a>Detailed changes</h3><p>나머지 하위 섹션에서는 Java 플랫폼과 그 구현 전반에 걸쳐 제안하는 변경 사항을 자세히 설명합니다.</p><ul><li>java.lang.Thread</li><li>Thread-local variables</li><li>java.util.concurrent</li><li>Networking</li><li>java.io </li><li>Java Native Interface (JNI)</li><li>Debugging (JVM TI, JDWP, and JDI)</li><li>JDK Flight Recorder (JFR)</li><li>Java Management Extensions (JMX)</li></ul><h4 id="java-lang-Thread"><a href="#java-lang-Thread" class="headerlink" title="java.lang.Thread"></a>java.lang.Thread</h4><p>API를 다음과 같이 업데이트합니다 java.lang.Thread.</p><ul><li>Thread.Builder, Thread.ofVirtual(), 및 는 Thread.ofPlatform()가상 및 플랫폼 스레드를 생성하는 새로운 API입니다. 예를 들어,<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Thread</span> <span class="variable">thread</span> <span class="operator">=</span> Thread.ofVirtual().name(<span class="string">&quot;duke&quot;</span>).unstarted(runnable);</span><br></pre></td></tr></table></figure>“duke” 라는 이름의 아직 시작되지 않은 새로운 가상 스레드를 만듭니다.</li><li>Thread.startVirtualThread(Runnable)가상 스레드를 만들고 시작하는 편리한 방법입니다.</li><li>A는 Thread.Builder스레드나 를 생성할 수 있으며 ThreadFactory, 이를 통해 동일한 속성을 가진 여러 스레드를 생성할 수 있습니다.</li><li>Thread.isVirtual()스레드가 가상 스레드인지 테스트합니다.</li><li>Thread.getAllStackTraces()이제 모든 스레드가 아닌 모든 플랫폼 스레드의 맵을 반환합니다.</li></ul><p>이 JEP에서는 API java.lang.Thread가 변경되지 않습니다. Thread클래스에 정의된 생성자는 이전과 마찬가지로 플랫폼 스레드를 생성합니다. 새로운 공개 생성자는 없습니다.</p><p>가상 스레드와 플랫폼 스레드 간의 주요 API 차이점은 다음과 같습니다.</p><ul><li>공개 Thread생성자는 가상 스레드를 생성할 수 없습니다.</li><li>가상 스레드는 항상 데몬 스레드입니다. 이 Thread.setDaemon(boolean)메서드는 가상 스레드를 데몬이 아닌 스레드로 변경할 수 없습니다.</li><li>가상 스레드는 고정된 우선순위를 갖습니다 Thread.NORM_PRIORITY. 이 Thread.setPriority(int)메서드는 가상 스레드에는 영향을 미치지 않습니다.<br>이 제한은 향후 릴리스에서 다시 검토될 수 있습니다. </li><li>가상 스레드는 스레드 그룹의 활성 멤버가 아닙니다. 가상 스레드에서 호출하면 Thread.getThreadGroup()이름이 인 플레이스홀더 스레드 그룹을 반환합니다<br>“VirtualThreads”. Thread.BuilderAPI는 가상 스레드의 스레드 그룹을 설정하는 메서드를 정의하지 않습니다.</li><li>집합 으로 실행할 경우 가상 스레드에는 권한이 없습니다 SecurityManager.</li></ul><h4 id="Thread-local-variables"><a href="#Thread-local-variables" class="headerlink" title="Thread-local variables"></a>Thread-local variables</h4><p>가상 스레드는 플랫폼 스레드처럼 스레드 로컬 변수( ThreadLocal)와 상속 가능한 스레드 로컬 변수( InheritableThreadLocal)를 지원하므로<br>스레드 로컬을 사용하는 기존 코드를 실행할 수 있습니다. 그러나 가상 스레드는 매우 많을 수 있으므로<br>스레드 로컬을 사용할 때는 신중하게 고려해야 합니다. 특히, 스레드 풀에서 동일한 스레드를 공유하는 여러 작업 간에 비용이 많이 드는<br>리소스를 풀링하는 데 스레드 로컬을 사용하지 마십시오. 가상 스레드는 수명 동안 단일 작업만 실행하도록 설계되었으므로 풀링해서는 안 됩니다.<br>수백만 개의 스레드로 실행할 때 메모리 사용량을 줄이기 위해 가상 스레드를 준비하면서 JDK java.base모듈에서 스레드 로컬의 많은 사용을 제거했습니다.</p><p>시스템 속성을 jdk.traceVirtualThreadLocals사용하면 가상 스레드가 스레드 로컬 변수의 값을 설정할 때 스택 추적을 트리거할 수 있습니다.<br>이 진단 출력은 가상 스레드를 사용하도록 코드를 마이그레이션할 때 스레드 로컬 변수를 제거하는 데 도움이 될 수 있습니다.<br>true스택 추적을 트리거하려면 시스템 속성을 로 설정하십시오. 기본값은 입니다 false.</p><p>일부 사용 사례에서는 범위가 지정된 값( JEP 429 )이 스레드 로컬보다 더 나은 대안이 될 수 있습니다.</p><h4 id="java-util-concurrent"><a href="#java-util-concurrent" class="headerlink" title="java.util.concurrent"></a>java.util.concurrent</h4><p>잠금을 지원하는 기본 API인 는 java.util.concurrent.LockSupport이제 가상 스레드를 지원합니다.<br>가상 스레드를 파킹하면 기본 플랫폼 스레드가 다른 작업을 수행할 수 있도록 해제되고, 가상 스레드를 파킹 해제하면 해당 스레드가 계속 진행되도록 예약됩니다.<br>이 변경을 통해 가상 스레드에서 호출될 때 LockSupport해당 스레드를 사용하는 모든 API( Locks, Semaphores, 블로킹 큐 등)가 정상적으로 파킹될 수 있습니다.</p><p>Executors.newThreadPerTaskExecutor(ThreadFactory)또한 ExecutorService, 각 작업에 대해 새 스레드를 생성하는 Executors.newVirtualThreadPerTaskExecutor()를 생성합니다<br>이러한 메서드를 사용하면 스레드 풀과 ExecutorService를 사용하는 기존 코드와의 마이그레이션 및 상호 운용성이 가능해집니다.</p><h4 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h4><p>java.net및 패키지 의 네트워킹 API 구현은 java.nio.channels이제 가상 스레드와 함께 작동합니다.<br>예를 들어 네트워크 연결을 설정하거나 소켓에서 읽는 등의 가상 스레드에서 차단된 작업은 기본 플랫폼 스레드를 해제하여 다른 작업을 수행합니다.</p><p>java.net.Socket중단 및 취소를 허용하기 위해, , ServerSocket, 에 정의된 차단 I&#x2F;O 메서드는 DatagramSocket이제 가상 스레드에서 호출될 때 중단 가능 하도록 지정되었습니다.<br>소켓에서 차단된 가상 스레드를 중단하면 스레드가 언파크되고 소켓이 닫힙니다. InterruptibleChannel 에서 가져온 이러한 유형의 소켓에서 차단 I&#x2F;O 작업은 항상 중단 가능했으므로,<br>이번 변경을 통해 생성자를 사용하여 생성된 이러한 API의 동작과 채널에서 가져온 API의 동작이 일치하게 됩니다.</p><h4 id="java-io"><a href="#java-io" class="headerlink" title="java.io"></a>java.io</h4><p>이 java.io패키지는 바이트 및 문자 스트림을 위한 API를 제공합니다. 이러한 API의 구현은 매우 동기화되어 있으며,<br>가상 스레드에서 사용될 때 고정을 방지하기 위해 변경이 필요합니다.</p><p>배경 지식으로, 바이트 지향 입출력 스트림은 스레드로부터 안전하도록 지정되어 있지 않으며, close()읽기 또는 쓰기 메서드에서 스레드가 차단된 상태에서 호출될 때<br>예상되는 동작을 지정하지 않습니다. 대부분의 경우, 여러 동시 스레드에서 특정 입출력 스트림을 사용하는 것은 합리적이지 않습니다.<br>문자 지향 판독기&#x2F;기록기 또한 스레드로부터 안전하도록 지정되어 있지 않지만, 하위 클래스에 잠금 객체를 제공합니다.<br>고정 외에도 이러한 클래스의 동기화는 문제가 있고 일관성이 없습니다. 예를 들어, 잠금 에서 사용되는 스트림 디코더와 인코더는 InputStreamReader OutputStreamWriter객체가<br>아닌 스트림 객체에서 동기화됩니다.</p><p>고정을 방지하기 위해 이제 구현은 다음과 같이 작동합니다.</p><ul><li>BufferedInputStream, BufferedOutputStream, BufferedReader, BufferedWriter, PrintStream, 그리고 PrintWriter이제 직접 사용할 때 모니터 대신 명시적 잠금을 사용합니다.<br>이러한 클래스는 하위 클래스화될 때 이전과 마찬가지로 동기화됩니다.</li><li>InputStreamReader OutputStreamWriter 에서 사용되는 스트림 디코더와 인코더는 현재 포함하는 InputStreamReader것과 OutputStreamWriter동일한 잠금을 사용합니다</li></ul><p>더 나아가 이러한 불필요한 잠금을 모두 제거하는 것은 이 JEP의 범위를 벗어납니다.</p><p>BufferedOutputStream또한 , BufferedWriter, , 및 스트림 인코더 에서 사용하는 버퍼의 초기 크기가 OutputStreamWriter<br>이제 더 작아져서 힙에 많은 스트림이나 작성자가 있는 경우 메모리 사용량이 줄었습니다.<br>이는 소켓 연결에 버퍼링된 스트림이 있는 백만 개의 가상 스레드가 있는 경우에 발생할 수 있는 현상입니다.</p><h4 id="Java-Native-Interface-JNI"><a href="#Java-Native-Interface-JNI" class="headerlink" title="Java Native Interface (JNI)"></a>Java Native Interface (JNI)</h4><p>IsVirtualThreadJNI는 객체가 가상 스레드인지 테스트하기 위해 새로운 함수 하나를 정의합니다 .<br>그 외에는 JNI 사양이 변경되지 않았습니다.</p><h2 id="Debugging"><a href="#Debugging" class="headerlink" title="Debugging"></a>Debugging</h2><p>디버깅 아키텍처는 JVM TI(JVM Tool Interface), JDWP(Java Debug Wire Protocol), JDI(Java Debug Interface)의 세 가지 인터페이스로 구성됩니다.<br>이제 세 가지 인터페이스 모두 가상 스레드를 지원합니다.</p><p>JVM TI 에 대한 업데이트는 다음과 같습니다.</p><ul><li>jthread(즉, 객체에 대한 JNI 참조 ) 를 사용하여 호출되는 대부분의 함수는 Thread가상 스레드에 대한 참조를 사용하여 호출할 수 있습니다.<br>AgentStartFunction, , PopFrame, ForceEarlyReturn*, StopThread, 와 같은 소수의 함수는 GetThreadCpuTime가상 스레드에서 지원되지 않거나 선택적으로 지원됩니다.<br>이 SetLocal*함수들은 중단점 또는 단일 단계 이벤트에서 일시 중단된 가상 스레드의 최상위 프레임에서 로컬 변수를 설정하는 것으로 제한됩니다.</li><li>이제 모든 스레드가 아닌 모든 플랫폼 스레드를 반환하도록 함수가 지정 되었습니다 GetAllThreads.GetAllStackTraces</li><li>초기 VM 시작이나 힙 반복 중에 게시된 이벤트를 제외한 모든 이벤트는 가상 스레드 컨텍스트에서 이벤트 콜백을 호출할 수 있습니다.</li><li>일시 중단&#x2F;재개 구현을 통해 디버거가 가상 스레드를 일시 중단하고 재개할 수 있으며, 가상 스레드가 마운트되면 플랫폼 스레드도 일시 중단될 수 있습니다.</li><li>새로운 기능을 can_support_virtual_threads사용하면 에이전트가 가상 스레드의 스레드 시작 및 종료 이벤트를 더욱 세부적으로 제어할 수 있습니다.</li><li>새로운 기능은 가상 스레드의 대량 중단 및 재개를 지원합니다. 이를 위해서는 해당 can_support_virtual_threads기능이 필요합니다.</li></ul><p>기존 JVM TI 에이전트는 대부분 이전과 동일하게 작동하지만, 가상 스레드에서 지원되지 않는 함수를 호출할 경우 오류가 발생할 수 있습니다.<br>이는 가상 스레드를 인식하지 못하는 에이전트가 가상 스레드를 사용하는 애플리케이션과 함께 사용될 때 발생합니다.<br>플랫폼 스레드만 포함하는 배열을 반환하도록 변경된 사항은 일부 에이전트에 문제가 될 수 있습니다.<br>ThreadStartThreadEnd 및 이벤트를 GetAllThreads활성화하는 기존 에이전트는 이러한 이벤트를 플랫폼 스레드로 제한하는 기능이 없기 때문에 성능 문제가 발생할 수 있습니다.</p><p>JDWP 에 대한 업데이트 내용은 다음과 같습니다.</p><ul><li>새로운 명령을 사용하면 디버거에서 스레드가 가상 스레드인지 테스트할 수 있습니다.</li><li>명령 의 새로운 수정자를 EventRequest사용하면 디버거가 스레드 시작 및 종료 이벤트를 플랫폼 스레드로 제한할 수 있습니다.</li></ul><p>JDI 업데이트 내용은 다음과 같습니다.</p><ul><li>com.sun.jdi.ThreadReference스레드가 가상 스레드인지 테스트하는 새로운 메서드 입니다.</li><li>새로운 메서드를 사용하여 com.sun.jdi.request.ThreadStartRequest플랫폼 com.sun.jdi.request.ThreadDeathRequest스레드에 대한 요청에 대해 생성된 이벤트를 제한합니다.</li></ul><p>위에서 언급했듯이 가상 스레드는 스레드 그룹에서 활성 스레드로 간주되지 않습니다. 따라서 JVM TI 함수 GetThreadGroupChildren,<br>JDWP 명령 ThreadGroupReference&#x2F;Children및 JDI 메서드 에서 반환되는 스레드 목록에는 com.sun.jdi.ThreadGroupReference.threads()플랫폼 스레드만 포함됩니다.</p><h2 id="JDK-Flight-Recorder-JFR"><a href="#JDK-Flight-Recorder-JFR" class="headerlink" title="JDK Flight Recorder (JFR)"></a>JDK Flight Recorder (JFR)</h2><p>JFR은 여러 가지 새로운 이벤트를 통해 가상 스레드를 지원합니다.</p><ul><li>jdk.VirtualThreadStart가상 스레드 시작과 종료를 나타 냅니다 jdk.VirtualThreadEnd. 이러한 이벤트는 기본적으로 비활성화되어 있습니다.</li><li>jdk.VirtualThreadPinned가상 스레드가 고정된 상태에서 대기 중임을 나타냅니다. 즉, 플랫폼 스레드를 해제하지 않은 상태입니다( 위 참조 ).<br>이 이벤트는 기본적으로 활성화되며 임계값은 20ms입니다.</li><li>jdk.VirtualThreadSubmitFailed가상 스레드 시작 또는 언파킹이 실패했음을 나타내며, 이는 리소스 문제로 인한 것일 수 있습니다.<br>이 이벤트는 기본적으로 활성화되어 있습니다.</li></ul><h2 id="Java-Management-Extensions-JMX"><a href="#Java-Management-Extensions-JMX" class="headerlink" title="Java Management Extensions (JMX)"></a>Java Management Extensions (JMX)</h2><p>java.lang.management.ThreadMXBean플랫폼 스레드의 모니터링 및 관리만 지원합니다. 이 findDeadlockedThreads()메서드는 교착 상태에 있는 플랫폼 스레드의 사이클을 찾습니다.<br>교착 상태에 있는 가상 스레드의 사이클은 찾지 않습니다.</p><p>새로운 메서드는 위에서com.sun.management.HotSpotDiagnosticsMXBean 설명한 새로운 스타일의 스레드 덤프를 생성합니다.<br>이 메서드는 로컬 또는 원격 JMX 도구에서 플랫폼을 통해 간접적으로 호출할 수도 있습니다 .MBeanServer</p><h2 id="Alternatives"><a href="#Alternatives" class="headerlink" title="Alternatives"></a>Alternatives</h2><ul><li><p>비동기 API를 계속 사용해야 합니다. 비동기 API는 동기 API와 통합하기 어렵고, 동일한 I&#x2F;O 작업을 두 가지 방식으로 표현하는 분리된 환경을 조성하며,<br>플랫폼에서 문제 해결, 모니터링, 디버깅 및 프로파일링을 위한 컨텍스트로 사용할 수 있는 작업 시퀀스에 대한 통합된 개념을 제공하지 않습니다.</p></li><li><p>Java 언어에 구문적 스택리스 코루틴 (예: async&#x2F;await )을 추가합니다 . 이는 사용자 모드 스레드보다 구현이 쉽고, 일련의 작업 맥락을 나타내는 통합 구조를 제공합니다.<br>하지만 이 구조는 스레드와는 별개의 새로운 개념으로, 여러 면에서 스레드와 유사하지만 미묘한 차이점도 있습니다.<br>스레드용 API와 코루틴용 API로 세상을 나누게 될 것이며, 새로운 스레드 유사 구조를 플랫폼의 모든 계층과 툴에 도입해야 할 것입니다.<br>생태계가 이를 받아들이는 데는 더 오랜 시간이 걸릴 것이고, 사용자 모드 스레드만큼 플랫폼과 조화를 이루지 못할 것입니다.<br>구문적 코루틴을 채택한 대부분의 언어는 사용자 모드 스레드(예: Kotlin), 레거시 의미 보장(예: 본질적으로 단일 스레드인 JavaScript),<br>또는 언어별 기술적 제약(예: C++)을 구현할 수 없기 때문에 그렇게 했습니다. 이러한 제약은 Java에는 적용되지 않습니다.</p></li><li><p>사용자 모드 스레드를 표현하는 새로운 공개 클래스를 도입합니다. 이 클래스는 .NET과는 무관합니다 java.lang.Thread.<br>이는 25년 동안 이 클래스에 축적된 불필요한 짐을 버릴 수 있는 기회가 될 것입니다 Thread.<br>이 접근 방식의 여러 변형을 탐구하고 프로토타입을 개발했지만, 모든 경우 기존 코드를 어떻게 실행할지에 대한 문제를 해결해야 했습니다.<br>가장 큰 문제는 Thread.currentThread()기존 코드에서 직간접적으로 광범위하게 사용된다는 것입니다(예: 잠금 소유권 확인 또는 스레드 로컬 변수).<br>이 메서드는 현재 실행 중인 스레드를 나타내는 객체를 반환해야 합니다. 사용자 모드 스레드를 나타내는 새로운 클래스를 도입한다면,<br>사용자 모드 스레드 객체에 위임하는 래퍼 currentThread()객체처럼 보이는 일종의 래퍼 객체를 반환해야 할 것입니다 Thread.<br>현재 실행 스레드를 두 개의 객체로 표현하는 것은 혼란스러울 수 있으므로, 결국 기존 ThreadAPI를 유지하는 것이 큰 문제가 되지 않는다는 결론을 내렸습니다.<br>와 같은 몇 가지 메서드를 제외하고는 currentThread()개발자가 ThreadAPI를 직접 사용하는 경우는 거의 없으며, 대부분 와 같은 상위 수준 API를 사용하여 상호 작용합니다.<br>시간이 지남에 따라 와 같은 클래스 및 관련 클래스 ExecutorService에서 불필요한 부분을 제거하고,<br>더 이상 사용되지 않는 메서드를 지원 중단하고 제거할 것입니다.ThreadThreadGroup</p></li></ul><h2 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h2><ul><li>기존 테스트를 통해 여기서 제안하는 변경 사항이 다양한 구성 및 실행 모드에서 예기치 않은 회귀를 일으키지 않는지 확인할 수 있습니다.</li><li>테스트 하네스를 확장하여 jtreg기존 테스트를 가상 스레드 환경에서 실행할 수 있도록 하겠습니다. 이렇게 하면 여러 테스트를 두 버전으로 실행할 필요가 없어집니다.</li><li>새로운 테스트에서는 모든 신규 및 개정된 API와 가상 스레드를 지원하도록 변경된 모든 영역을 테스트합니다.</li><li>새로운 스트레스 테스트는 신뢰성과 성능에 중요한 영역을 대상으로 합니다.</li><li>새로운 마이크로벤치마크는 성능이 중요한 영역을 타겟으로 합니다.</li><li>대규모 테스트를 위해 Helidon 과 Jetty 를 포함한 여러 기존 서버를 사용할 것입니다.</li></ul><h2 id="Risks-and-Assumptions"><a href="#Risks-and-Assumptions" class="headerlink" title="Risks and Assumptions"></a>Risks and Assumptions</h2><p>이 제안의 주요 위험은 기존 API와 구현의 변경으로 인한 호환성 문제입니다.</p><ul><li>java.io.BufferedInputStream, BufferedOutputStream, BufferedReader, BufferedWriter, PrintStream, 및 클래스 에서 사용되는 내부(문서화되지 않은)<br>잠금 프로토콜의 개정은 PrintWriterI&#x2F;O 메서드가 호출되는 스트림에서 동기화된다고 가정하는 코드에 영향을 미칠 수 있습니다.<br>이러한 변경 사항은 이러한 클래스를 확장하고 슈퍼클래스의 잠금을 가정하는 코드에는 영향을 미치지 않으며,<br>해당 API에서 제공하는 잠금 객체를 확장하거나 java.io.Reader사용 하는 코드에도 영향을 미치지 않습니다 java.io.Writer</li></ul><p>몇 가지 소스 및 바이너리 비호환 변경 사항은 다음을 확장하는 코드에 영향을 미칠 수 있습니다 java.lang.Thread.</p><ul><li>Thread여러 개의 새로운 메서드를 정의합니다. 기존 소스 파일의 코드가 확장되고 Thread하위 클래스의 메서드가 새로운 Thread메서드와 충돌하는 경우,<br>해당 파일은 변경 없이 컴파일되지 않습니다.</li><li>Thread.Builder새로운 중첩 인터페이스입니다. 기존 소스 파일의 코드가 를 확장하고 Thread, 이라는 클래스를 가져오고 Builder,<br>하위 클래스의 코드가 를 Builder간단한 이름으로 참조하는 경우, 해당 파일은 변경 없이 컴파일되지 않습니다.</li><li>Thread.isVirtual()새로운 final 메서드입니다. 기존에 컴파일된 코드가 확장 Thread되고 하위 클래스가 동일한 이름과 반환 유형을 가진 메서드를 선언하는 경우,<br>IncompatibleClassChangeError하위 클래스가 로드되면 런타임에 오류가 발생합니다.</li></ul><p>기존 코드와 가상 스레드 또는 새로운 API를 활용하는 최신 코드를 혼합할 때 플랫폼 스레드와 가상 스레드 간에 몇 가지 동작 차이가 나타날 수 있습니다.</p><ul><li>이 Thread.setPriority(int)방법은 가상 스레드에는 영향을 미치지 않으며, 가상 스레드의 우선순위는 항상 .입니다 Thread.NORM_PRIORITY.</li><li>이 Thread.setDaemon(boolean)방법은 가상 스레드에는 영향을 미치지 않으며, 가상 스레드는 항상 데몬 스레드입니다.</li><li>Thread.getAllStackTraces()이제 모든 스레드의 맵이 아닌 모든 플랫폼 스레드의 맵을 반환합니다.</li><li>java.net.Socket, ServerSocket, 로 정의된 블로킹 I&#x2F;O 메서드는 DatagramSocket이제 가상 스레드 컨텍스트에서 호출될 때 인터럽트 가능합니다.<br>소켓 작업에서 블로킹된 스레드가 인터럽트되면 기존 코드가 중단될 수 있으며, 이로 인해 스레드가 깨어나고 소켓이 닫힙니다.</li><li>가상 스레드는 의 활성 멤버가 아닙니다 ThreadGroup. Thread.getThreadGroup()가상 스레드에서 호출하면 “VirtualThreads”비어 있는 더미 그룹이 반환됩니다.</li><li>보안 관리자를 설정한 상태로 실행하면 가상 스레드에 권한이 없습니다.<br>Java 17 이상에서 보안 관리자를 사용하여 실행하는 방법에 대한 자세한 내용은 JEP 411(보안 관리자 제거를 위한 지원 중단)을 참조하십시오.</li><li>JVM TI에서 GetAllThreads및 GetAllStackTraces함수는 가상 스레드를 반환하지 않습니다.<br>ThreadStart및 ThreadEnd이벤트를 활성화하는 기존 에이전트는 이벤트를 플랫폼 스레드로 제한하는 기능이 없기 때문에 성능 문제가 발생할 수 있습니다.</li><li>API java.lang.management.ThreadMXBean는 플랫폼 스레드의 모니터링과 관리를 지원하지만 가상 스레드는 지원하지 않습니다.</li><li>이 -XX:+PreserveFramePointer플래그는 가상 스레드 성능에 엄청난 부정적 영향을 미칩니다.</li></ul><h2 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h2><ul><li>JDK 18의 JEP 416(메서드 핸들을 사용한 코어 리플렉션 재구현) 에서는 VM 네이티브 리플렉션 구현을 제거했습니다.<br>이를 통해 메서드가 리플렉션 방식으로 호출될 때 가상 스레드가 정상적으로 대기할 수 있습니다.</li><li>JDK 13의 JEP 353(레거시 소켓 API 재구현) 과 JDK 15의 JEP 373(레거시 DatagramSocket API 재구현) 은<br>가상 스레드와 함께 사용하도록 설계된 새로운 구현으로 java.net.Socket, ServerSocket, 및 구현을 대체했습니다.DatagramSocket</li><li>JDK 18의 JEP 418(인터넷 주소 확인 SPI) 은 호스트 이름 및 주소 조회를 위한 서비스 제공자 인터페이스를 정의했습니다.<br>이를 통해 타사 라이브러리는 java.net.InetAddress호스트 조회 중 스레드를 고정하지 않는 대체 확인자를 구현할 수 있습니다.</li></ul><h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr><ul><li><a href="http://openjdk.java.net/jeps/444">JEP_444</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/08/2025-08-11-JEP_444/#disqus_thread</comments>
    </item>
    
    <item>
      <title>GitHub Pages에서 Custom Domain 설정하기</title>
      <link>https://sejoung.github.io/2025/07/2025-07-08-GitHub_Pages_DNS_Setting/</link>
      <guid>https://sejoung.github.io/2025/07/2025-07-08-GitHub_Pages_DNS_Setting/</guid>
      <pubDate>Tue, 08 Jul 2025 08:11:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;GitHub-Pages에서-Custom-Domain-설정하기&quot;&gt;&lt;a href=&quot;#GitHub-Pages에서-Custom-Domain-설정하기&quot; class=&quot;headerlink&quot; title=&quot;GitHub Pages에서 Custom Doma
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="GitHub-Pages에서-Custom-Domain-설정하기"><a href="#GitHub-Pages에서-Custom-Domain-설정하기" class="headerlink" title="GitHub Pages에서 Custom Domain 설정하기"></a>GitHub Pages에서 Custom Domain 설정하기</h1><h2 id="A-레코드-설정"><a href="#A-레코드-설정" class="headerlink" title="A 레코드 설정"></a>A 레코드 설정</h2><p>A 레코드 설정을 @로 해서 다음 IP 주소로 설정합니다. </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">A @ 185.199.108.153</span><br><span class="line">A @ 185.199.109.153</span><br><span class="line">A @ 185.199.110.153</span><br><span class="line">A @ 185.199.111.153</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="도메인-설정"><a href="#도메인-설정" class="headerlink" title="도메인 설정"></a>도메인 설정</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CNAME tech.EXAMPLE.COM sejoung.github.io.</span><br></pre></td></tr></table></figure><h2 id="확인-방법"><a href="#확인-방법" class="headerlink" title="확인 방법"></a>확인 방법</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dig WWW.EXAMPLE.COM +nostats +nocomments +nocmd</span><br><span class="line">&gt; TECH.EXAMPLE.COM.             3592    IN      CNAME   sejoung.github.io.</span><br><span class="line">sejoung.github.io.3600INA185.199.108.153</span><br><span class="line">sejoung.github.io.3600INA185.199.109.153</span><br><span class="line">sejoung.github.io.3600INA185.199.110.153</span><br><span class="line">sejoung.github.io.3600INA185.199.111.153</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><hr><ul><li><a href="https://docs.github.com/ko/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site#configuring-an-apex-domain">사용자 지정 도메인 구성 정보</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/07/2025-07-08-GitHub_Pages_DNS_Setting/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Windows에서 심볼릭 링크 만들기</title>
      <link>https://sejoung.github.io/2025/05/2025-05-27-symlinks_windows/</link>
      <guid>https://sejoung.github.io/2025/05/2025-05-27-symlinks_windows/</guid>
      <pubDate>Tue, 27 May 2025 09:39:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Windows에서-심볼릭-링크-만들기&quot;&gt;&lt;a href=&quot;#Windows에서-심볼릭-링크-만들기&quot; class=&quot;headerlink&quot; title=&quot;Windows에서 심볼릭 링크 만들기&quot;&gt;&lt;/a&gt;Windows에서 심볼릭 링크 만들기&lt;/h1&gt;&lt;
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Windows에서-심볼릭-링크-만들기"><a href="#Windows에서-심볼릭-링크-만들기" class="headerlink" title="Windows에서 심볼릭 링크 만들기"></a>Windows에서 심볼릭 링크 만들기</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mklink /d &lt;링크가 필요한 프로젝트&gt;  &lt;실제 파일이 존재하는곳&gt;</span><br></pre></td></tr></table></figure><p>위 와 같이 mklink 명령어를 사용하여 심볼릭 링크를 만들 수 있습니다.</p><h2 id="간단한-심볼릭-링크-TOOL"><a href="#간단한-심볼릭-링크-TOOL" class="headerlink" title="간단한 심볼릭 링크 TOOL"></a>간단한 심볼릭 링크 TOOL</h2><h3 id="Symbolic11"><a href="#Symbolic11" class="headerlink" title="Symbolic11"></a>Symbolic11</h3><p>Symbolic11은 심볼릭 링크를 쉽게 만들 수 있는 GUI 프로그램입니다. 이 프로그램을 사용하면 명령어를 입력하지 않고도 심볼릭 링크를 생성할 수 있습니다.</p><p><img src="https://github.com/Benisgo/Symbolic11/blob/master/Assets/WindowsSandboxClient_YawPuzrYbi.png?raw=true" alt="그림"></p><h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><hr><ul><li><a href="https://github.com/Benisgo/Symbolic11">Symbolic11</a></li><li><a href="https://learn.microsoft.com/ko-kr/windows-server/administration/windows-commands/mklink">mklink</a></li><li><a href="https://github.com/raspopov/SageLinks">SageLinks</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/05/2025-05-27-symlinks_windows/#disqus_thread</comments>
    </item>
    
    <item>
      <title>우분투 nvidia 드라이버 업데이트</title>
      <link>https://sejoung.github.io/2025/05/2025-05-16-nvidia_drive_update_ubuntu/</link>
      <guid>https://sejoung.github.io/2025/05/2025-05-16-nvidia_drive_update_ubuntu/</guid>
      <pubDate>Fri, 16 May 2025 02:49:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;우분투-nvidia-드라이버-업데이트&quot;&gt;&lt;a href=&quot;#우분투-nvidia-드라이버-업데이트&quot; class=&quot;headerlink&quot; title=&quot;우분투 nvidia 드라이버 업데이트&quot;&gt;&lt;/a&gt;우분투 nvidia 드라이버 업데이트&lt;/h1&gt;&lt;
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="우분투-nvidia-드라이버-업데이트"><a href="#우분투-nvidia-드라이버-업데이트" class="headerlink" title="우분투 nvidia 드라이버 업데이트"></a>우분투 nvidia 드라이버 업데이트</h1><h2 id="사용-가능한-NVIDIA-드라이버-목록-보기"><a href="#사용-가능한-NVIDIA-드라이버-목록-보기" class="headerlink" title="사용 가능한 NVIDIA 드라이버 목록 보기"></a>사용 가능한 NVIDIA 드라이버 목록 보기</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ubuntu-drivers devices</span><br></pre></td></tr></table></figure><h2 id="최신-권장-드라이버-자동-설치"><a href="#최신-권장-드라이버-자동-설치" class="headerlink" title="최신 권장 드라이버 자동 설치"></a>최신 권장 드라이버 자동 설치</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ubuntu-drivers autoinstall</span><br></pre></td></tr></table></figure><h2 id="수동으로-특정-버전-설치"><a href="#수동으로-특정-버전-설치" class="headerlink" title="수동으로 특정 버전 설치"></a>수동으로 특정 버전 설치</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install nvidia-driver-575</span><br></pre></td></tr></table></figure><h2 id="드라이버-설치-후-재부팅"><a href="#드라이버-설치-후-재부팅" class="headerlink" title="드라이버 설치 후 재부팅"></a>드라이버 설치 후 재부팅</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><hr>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/05/2025-05-16-nvidia_drive_update_ubuntu/#disqus_thread</comments>
    </item>
    
    <item>
      <title>우분투 에서 5090 driver 설치</title>
      <link>https://sejoung.github.io/2025/05/2025-05-07-nvidia-driver-install-5090/</link>
      <guid>https://sejoung.github.io/2025/05/2025-05-07-nvidia-driver-install-5090/</guid>
      <pubDate>Wed, 07 May 2025 09:37:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;우분투-에서-5090-driver-설치&quot;&gt;&lt;a href=&quot;#우분투-에서-5090-driver-설치&quot; class=&quot;headerlink&quot; title=&quot;우분투 에서 5090 driver 설치&quot;&gt;&lt;/a&gt;우분투 에서 5090 driver 설치&lt;/
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="우분투-에서-5090-driver-설치"><a href="#우분투-에서-5090-driver-설치" class="headerlink" title="우분투 에서 5090 driver 설치"></a>우분투 에서 5090 driver 설치</h1><h2 id="기존-드라이버-삭제"><a href="#기존-드라이버-삭제" class="headerlink" title="기존 드라이버 삭제"></a>기존 드라이버 삭제</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove --purge &#x27;^nvidia-.*&#x27;</span><br><span class="line">sudo apt purge nvidia* -y</span><br><span class="line">sudo apt remove nvidia* -y</span><br><span class="line">sudo apt autoremove &amp;&amp; sudo apt autoclean</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a href="https://www.nvidia.com/ko-kr/geforce/drivers/">geforce&#x2F;drivers</a> 여기서 최신 드라이버 다운로드</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install pkg-config libglvnd-dev build-essential</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./NVIDIA-Linux-x86_64-570.144.run --no-kernel-modules</span><br></pre></td></tr></table></figure><p>driver 설치때 MIT&#x2F;GPL 라이센스 동의 해야함</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install git</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/NVIDIA/open-gpu-kernel-modules</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd open-gpu-kernel-modules</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout 570.144</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make modules -j$(nproc)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo make modules_install -j$(nproc)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo depmod</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot -n</span><br></pre></td></tr></table></figure><h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><hr><ul><li><a href="https://www.nvidia.com/ko-kr/geforce/drivers/">geforce&#x2F;drivers</a></li><li><a href="https://hiseon.me/general/how-to-install-nvidia-drivers-on-ubuntu-24-04/">How to install Nvidia drivers on Ubuntu 24.04</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/05/2025-05-07-nvidia-driver-install-5090/#disqus_thread</comments>
    </item>
    
    <item>
      <title>우분투 설치시 네트워크 드라이브(R8125) 인식 안됨</title>
      <link>https://sejoung.github.io/2025/05/2025-05-07-r8125_drive_ubuntu/</link>
      <guid>https://sejoung.github.io/2025/05/2025-05-07-r8125_drive_ubuntu/</guid>
      <pubDate>Wed, 07 May 2025 06:45:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;우분투-설치시-네트워크-드라이브-R8125-인식-안됨&quot;&gt;&lt;a href=&quot;#우분투-설치시-네트워크-드라이브-R8125-인식-안됨&quot; class=&quot;headerlink&quot; title=&quot;우분투 설치시 네트워크 드라이브(R8125) 인식 안됨&quot;&gt;&lt;/
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="우분투-설치시-네트워크-드라이브-R8125-인식-안됨"><a href="#우분투-설치시-네트워크-드라이브-R8125-인식-안됨" class="headerlink" title="우분투 설치시 네트워크 드라이브(R8125) 인식 안됨"></a>우분투 설치시 네트워크 드라이브(R8125) 인식 안됨</h1><h2 id="현상"><a href="#현상" class="headerlink" title="현상"></a>현상</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lshw -C network</span><br></pre></td></tr></table></figure><p>위에 명형어를 수행하면 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*-network UNCLAIMED</span><br></pre></td></tr></table></figure><p>위처럼 나오고 유선랜이 잡히지 않음</p><h2 id="해결-방법"><a href="#해결-방법" class="headerlink" title="해결 방법"></a>해결 방법</h2><p><a href="https://github.com/awesometic/realtek-r8125-dkms">realtek-r8125-dkms</a></p><p>위에 링크를 통해서 드라이버를 설치하면 됨 네트워크 없이는 의존성을 설치하기 힘듬 wifi를 통해서 설치하면 됨</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:awesometic/ppa</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt update</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install realtek-r8125-dkms</span><br></pre></td></tr></table></figure><h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><hr><ul><li><a href="https://github.com/awesometic/realtek-r8125-dkms">realtek-r8125-dkms</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/05/2025-05-07-r8125_drive_ubuntu/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Docker login 시 mac keychain 에러</title>
      <link>https://sejoung.github.io/2025/04/2025-04-23-docker_login_mac/</link>
      <guid>https://sejoung.github.io/2025/04/2025-04-23-docker_login_mac/</guid>
      <pubDate>Wed, 23 Apr 2025 02:43:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Docker-login-시-mac-keychain-에러&quot;&gt;&lt;a href=&quot;#Docker-login-시-mac-keychain-에러&quot; class=&quot;headerlink&quot; title=&quot;Docker login 시 mac keychain 에러&quot;&gt;
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Docker-login-시-mac-keychain-에러"><a href="#Docker-login-시-mac-keychain-에러" class="headerlink" title="Docker login 시 mac keychain 에러"></a>Docker login 시 mac keychain 에러</h1><h2 id="로그인"><a href="#로그인" class="headerlink" title="로그인"></a>로그인</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo 키정보 | docker login -u 사용자 --password-stdin</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="예시"><a href="#예시" class="headerlink" title="예시"></a>예시</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo dckr_pat_ewhqgTvF1234GFjYM4ziAEWNctrw | docker login -u zolla --password-stdin</span><br></pre></td></tr></table></figure><h3 id="에러"><a href="#에러" class="headerlink" title="에러"></a>에러</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error saving credentials: error storing credentials - err: exit status 1, out: `error getting credentials - err: exit status 1, out: `keychain cannot be accessed because the current session does not allow user interaction. The keychain may be locked; unlock it by running &quot;security -v unlock-keychain ~/Library/Keychains/login.keychain-db&quot; and try again``</span><br></pre></td></tr></table></figure><h2 id="해결-방법"><a href="#해결-방법" class="headerlink" title="해결 방법"></a>해결 방법</h2><p>Keychain 잠금 해제</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">security unlock-keychain -p &#x27;비밀번호&#x27; ~/Library/Keychains/login.keychain-db</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">security unlock-keychain -p &#x27;1234&#x27; ~/Library/Keychains/login.keychain-db</span><br></pre></td></tr></table></figure><h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><hr>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/04/2025-04-23-docker_login_mac/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Docker build 시 SSH 키 설정 사용</title>
      <link>https://sejoung.github.io/2025/04/2025-04-07-docker_build_ssh_git/</link>
      <guid>https://sejoung.github.io/2025/04/2025-04-07-docker_build_ssh_git/</guid>
      <pubDate>Mon, 07 Apr 2025 05:24:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Docker-build-시-SSH-키-설정-사용&quot;&gt;&lt;a href=&quot;#Docker-build-시-SSH-키-설정-사용&quot; class=&quot;headerlink&quot; title=&quot;Docker build 시 SSH 키 설정 사용&quot;&gt;&lt;/a&gt;Docker b
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Docker-build-시-SSH-키-설정-사용"><a href="#Docker-build-시-SSH-키-설정-사용" class="headerlink" title="Docker build 시 SSH 키 설정 사용"></a>Docker build 시 SSH 키 설정 사용</h1><p>컴파일 언어를 사용해서 배포할때는 바이너리만 배포하면 되니깐 문제가 없는데(모든 의존성을 build 시점에 같이 묶임)<br>파이썬 같은 인터프리터 언어를 사용할때는 배포할 시점에 의존성 파일을 다운받아야 된다.<br>venv를 사용해서 가상환경을 만들고, 그 안에 의존성 파일을 다운받는 경우가 많다.</p><p>이시점에 pip를 사용해서 의존성 파일을 다운받는데, private git hub에 있는 의존성 파일을 다운받아야 하는 경우가 있다.</p><p>이때는 SSH 키를 사용해서 private git hub에 접근해야 한다.</p><p>이 문제를 해결하기 위해서 Dockerfile에서 SSH 키를 사용해서 private git hub에 접근하는 방법을 알아보자.</p><h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2><p>처리 방법이 여러 방법이 있는데 저는 기본적인 방법을 사용했다.</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.11</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> ROOT=/test</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">mkdir</span> -p <span class="variable">$ROOT</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">mkdir</span> -p /root/.ssh</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> id_ed25519 /root/.ssh/id_ed25519</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">chmod</span> 600 /root/.ssh/id_ed25519 &amp;&amp; \</span></span><br><span class="line"><span class="language-bash">    ssh-keyscan github.com &gt;&gt; /root/.ssh/known_hosts</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> <span class="variable">$ROOT</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> . .</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip install --upgrade pip</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip install -r requirements.txt</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">rm</span> -rf /root/.ssh</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="string">&quot;python3.11&quot;</span>, <span class="string">&quot;src/main.py&quot;</span>]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>위와 같이 ssh 키를 복사하고, 권한을 설정하고, ssh-keyscan을 사용해서 github.com의 공개키를 known_hosts에 추가한다.</p><p>이렇게 하면 ssh 키를 사용해서 private git hub에 접근할 수 있다.</p><p>그리고 코드가 다 동작하면 ssh 키를 삭제한다.</p><h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><hr>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/04/2025-04-07-docker_build_ssh_git/#disqus_thread</comments>
    </item>
    
    <item>
      <title>jenkins에서 python 빌드하기</title>
      <link>https://sejoung.github.io/2025/03/2025-03-25-jenkins_python_build/</link>
      <guid>https://sejoung.github.io/2025/03/2025-03-25-jenkins_python_build/</guid>
      <pubDate>Tue, 25 Mar 2025 12:00:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;jenkins에서-python-빌드하기&quot;&gt;&lt;a href=&quot;#jenkins에서-python-빌드하기&quot; class=&quot;headerlink&quot; title=&quot;jenkins에서 python 빌드하기&quot;&gt;&lt;/a&gt;jenkins에서 python 빌드하기&lt;/
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="jenkins에서-python-빌드하기"><a href="#jenkins에서-python-빌드하기" class="headerlink" title="jenkins에서 python 빌드하기"></a>jenkins에서 python 빌드하기</h1><p>아래 와 같은 오류가 날수 있는데 해당 오류는 apt install python-dev 로 해결할수 있다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Python.h : no such file or directory</span><br></pre></td></tr></table></figure><p>macOS를 host로 사용시에 docker 설정에서는 docker in docker 를 사용할때 아래와 같은 설정이 정상 동작 하지 않는다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">volumes:</span><br><span class="line">  - ./jenkins_home:/var/jenkins_home</span><br><span class="line">  - /var/run/docker.sock:/var/run/docker.sock  # Docker-in-Docker (필요한 경우)</span><br><span class="line">  - /usr/bin/docker:/usr/bin/docker  # Docker 명령어 사용 가능하게 설정</span><br></pre></td></tr></table></figure><p><code>/usr/bin/docker:/usr/bin/docker</code> 여기가 문제인데 이부분을 제거하고 docker명령어를 install 하면 정상 동작한다.</p><h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><hr>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/03/2025-03-25-jenkins_python_build/#disqus_thread</comments>
    </item>
    
    <item>
      <title>macos 화면 공유하기 및 SSH 설정</title>
      <link>https://sejoung.github.io/2025/03/2025-03-17-macos_screen_share/</link>
      <guid>https://sejoung.github.io/2025/03/2025-03-17-macos_screen_share/</guid>
      <pubDate>Mon, 17 Mar 2025 06:34:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;macos-화면-공유하기-및-SSH-설정&quot;&gt;&lt;a href=&quot;#macos-화면-공유하기-및-SSH-설정&quot; class=&quot;headerlink&quot; title=&quot;macos 화면 공유하기 및 SSH 설정&quot;&gt;&lt;/a&gt;macos 화면 공유하기 및 SSH 
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="macos-화면-공유하기-및-SSH-설정"><a href="#macos-화면-공유하기-및-SSH-설정" class="headerlink" title="macos 화면 공유하기 및 SSH 설정"></a>macos 화면 공유하기 및 SSH 설정</h1><p>macos 화면 공유하기 및 SSH 설정을 하기 위해서는 다음과 같은 설정을 해주어야 한다.</p><p><img src="https://sejoung.github.io/images/2025_03_17_01.png" alt="mac os 1"></p><p><img src="https://sejoung.github.io/images/2025_03_17_02.png" alt="mac os 2"></p><p>위에 설정이 끝나면 아래의 툴로 통해서 접속하거나 터미널에서 <code>ssh</code> 명령어로 접속이 가능하다.</p><p><img src="https://sejoung.github.io/images/2025_03_17_03.png" alt="mac os 3"></p><p>화면 공유 툴 터미널에서 접속 방법</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">open vnc://192.168.0.1</span><br></pre></td></tr></table></figure><h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><hr><ul><li><a href="https://support.apple.com/ko-kr/guide/mac-help/mh14066/mac">다른 Mac의 화면 공유하기</a></li><li><a href="https://support.apple.com/ko-kr/guide/mac-help/mh11848/mac">Mac 화면 공유 켜거나 끄기</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/03/2025-03-17-macos_screen_share/#disqus_thread</comments>
    </item>
    
    <item>
      <title>최신 image gen AI 기술 관련 기사</title>
      <link>https://sejoung.github.io/2025/02/2025-02-11-lora_articles/</link>
      <guid>https://sejoung.github.io/2025/02/2025-02-11-lora_articles/</guid>
      <pubDate>Tue, 11 Feb 2025 05:30:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;최신-image-gen-AI-기술-관련-기사&quot;&gt;&lt;a href=&quot;#최신-image-gen-AI-기술-관련-기사&quot; class=&quot;headerlink&quot; title=&quot;최신 image gen AI 기술 관련 기사&quot;&gt;&lt;/a&gt;최신 image gen A
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="최신-image-gen-AI-기술-관련-기사"><a href="#최신-image-gen-AI-기술-관련-기사" class="headerlink" title="최신 image gen AI 기술 관련 기사"></a>최신 image gen AI 기술 관련 기사</h1><h2 id="LoRa-관련-기사"><a href="#LoRa-관련-기사" class="headerlink" title="LoRa 관련 기사"></a>LoRa 관련 기사</h2><ul><li><a href="https://civitai.com/articles/11394/understanding-flux-lora-training-parameters">Flux LoRA 훈련 매개변수 이해</a></li><li><a href="https://civitai.com/articles/4/make-your-own-loras-easy-and-free">쉽고 무료로 나만의 로라를 만들어보세요</a></li><li><a href="https://civitai.com/articles/10381/my-online-training-parameter-for-style-lora-on-illustrious-and-some-of-my-thoughts">Illustrious의 Style LoRa에 대한 내 온라인 교육 매개변수와 내 생각 중 일부</a></li><li><a href="https://civitai.com/articles/9223/how-to-accidentally-create-a-pretty-good-lora">우연히 꽤 괜찮은 LoRA를 만드는 방법</a></li><li><a href="https://civitai.com/articles/2179/how-to-train-style-lora-101">스타일 로라 101 훈련 방법</a></li><li><a href="https://civitai.com/articles/10584/writing-a-good-description-for-your-lora">LoRA에 대한 좋은 설명 작성하기</a></li><li><a href="https://civitai.com/articles/9205/how-i-create-my-own-sdxl-loras">내 SDXL LoRA를 만드는 방법</a></li></ul><h2 id="안정된-확산-관련-기사"><a href="#안정된-확산-관련-기사" class="headerlink" title="안정된 확산 관련 기사"></a>안정된 확산 관련 기사</h2><ul><li><a href="https://civitai.com/articles/11359/mastering-the-cfg-scale-in-stable-diffusion">안정된 확산에서 CFG 스케일 마스터링</a></li></ul><h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr><ul><li><a href="https://civitai.com/articles/11394/understanding-flux-lora-training-parameters">Flux LoRA 훈련 매개변수 이해</a></li><li><a href="https://civitai.com/articles/4/make-your-own-loras-easy-and-free">쉽고 무료로 나만의 로라를 만들어보세요</a></li><li><a href="https://civitai.com/articles/10381/my-online-training-parameter-for-style-lora-on-illustrious-and-some-of-my-thoughts">Illustrious의 Style LoRa에 대한 내 온라인 교육 매개변수와 내 생각 중 일부</a></li><li><a href="https://civitai.com/articles/9223/how-to-accidentally-create-a-pretty-good-lora">우연히 꽤 괜찮은 LoRA를 만드는 방법</a></li><li><a href="https://civitai.com/articles/11359/mastering-the-cfg-scale-in-stable-diffusion">안정된 확산에서 CFG 스케일 마스터링</a></li><li><a href="https://civitai.com/articles/2179/how-to-train-style-lora-101">스타일 로라 101 훈련 방법</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2025/02/2025-02-11-lora_articles/#disqus_thread</comments>
    </item>
    
    <item>
      <title>2024년 간략 회고</title>
      <link>https://sejoung.github.io/2024/12/2024-12-19-2024_retrospect/</link>
      <guid>https://sejoung.github.io/2024/12/2024-12-19-2024_retrospect/</guid>
      <pubDate>Wed, 18 Dec 2024 06:47:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;2024년-간략-회고&quot;&gt;&lt;a href=&quot;#2024년-간략-회고&quot; class=&quot;headerlink&quot; title=&quot;2024년 간략 회고&quot;&gt;&lt;/a&gt;2024년 간략 회고&lt;/h1&gt;&lt;h2 id=&quot;업무&quot;&gt;&lt;a href=&quot;#업무&quot; class=&quot;head
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="2024년-간략-회고"><a href="#2024년-간략-회고" class="headerlink" title="2024년 간략 회고"></a>2024년 간략 회고</h1><h2 id="업무"><a href="#업무" class="headerlink" title="업무"></a>업무</h2><h3 id="stable-diffusion"><a href="#stable-diffusion" class="headerlink" title="stable diffusion"></a>stable diffusion</h3><p>이미지 생성쪽에 대한 연구를 진행하면서 보게 되는 모델이다</p><ul><li>sd 1.5</li><li>sd 2.1</li><li>sdxl</li></ul><p>위에 모델 순으로 성능이 좋아지는 것을 확인할 수 있었다.</p><h3 id="fine-tuning"><a href="#fine-tuning" class="headerlink" title="fine-tuning"></a>fine-tuning</h3><ul><li>LORA</li><li>dreambooth</li></ul><p>위에 2가지 방법으로 fine-tuning을 진행하였다.</p><h3 id="process-improvement"><a href="#process-improvement" class="headerlink" title="process improvement"></a>process improvement</h3><h3 id="evaluation"><a href="#evaluation" class="headerlink" title="evaluation"></a>evaluation</h3><p>평가 기준을 정하고 평가를 진행하였다.</p><h2 id="출장"><a href="#출장" class="headerlink" title="출장"></a>출장</h2><h3 id="San-Francisco-Bay-Area"><a href="#San-Francisco-Bay-Area" class="headerlink" title="San Francisco(Bay Area)"></a>San Francisco(Bay Area)</h3><ul><li>Microsoft 방문</li><li>현지 VC 방문</li><li>현지 회사 방문</li></ul><h3 id="Seattle-AWS-AI-Accelerator"><a href="#Seattle-AWS-AI-Accelerator" class="headerlink" title="Seattle(AWS AI Accelerator)"></a>Seattle(AWS AI Accelerator)</h3><ul><li>AWS AI Accelerator 참가</li></ul><h3 id="Las-Vegas-AWS-re-Invent"><a href="#Las-Vegas-AWS-re-Invent" class="headerlink" title="Las Vegas(AWS re:Invent)"></a>Las Vegas(AWS re:Invent)</h3><ul><li>AWS AI Accelerator 마지막 세션으로 참가</li><li>발표 및 다양한 클라이언트 확인</li></ul><h3 id="Los-Angeles"><a href="#Los-Angeles" class="headerlink" title="Los Angeles"></a>Los Angeles</h3><ul><li>현지 회사 방문</li></ul><h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr><ul><li><a href="https://dreambooth.github.io/">dreambooth</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2024/12/2024-12-19-2024_retrospect/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Poetry: debugging</title>
      <link>https://sejoung.github.io/2024/11/2024-11-01-python_poetry/</link>
      <guid>https://sejoung.github.io/2024/11/2024-11-01-python_poetry/</guid>
      <pubDate>Fri, 01 Nov 2024 08:00:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Poetry-debugging&quot;&gt;&lt;a href=&quot;#Poetry-debugging&quot; class=&quot;headerlink&quot; title=&quot;Poetry: debugging&quot;&gt;&lt;/a&gt;Poetry: debugging&lt;/h1&gt;&lt;p&gt;poetry insta
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Poetry-debugging"><a href="#Poetry-debugging" class="headerlink" title="Poetry: debugging"></a>Poetry: debugging</h1><p>poetry install 이 않되서 삼질 함</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poetry --vvv install</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">poetry -vvv install</span><br><span class="line"></span><br><span class="line">Loading configuration file /home/dev/.config/pypoetry/config.toml</span><br><span class="line">Adding repository pytorch (https://download.pytorch.org/whl/cu124) and setting it as supplemental</span><br><span class="line">Using virtualenv: /repositories/SimpleTuner/.venv</span><br><span class="line">Installing dependencies from lock file</span><br><span class="line"></span><br><span class="line">Finding the necessary packages for the current system</span><br><span class="line"></span><br><span class="line">Package operations: 152 installs, 2 updates, 0 removals, 12 skipped</span><br><span class="line"></span><br><span class="line">  - Installing nvidia-nvjitlink-cu12 (12.4.99): Pending...</span><br><span class="line">Checking if keyring is available</span><br><span class="line">[keyring:keyring.backend] Loading KWallet</span><br><span class="line">[keyring:keyring.backend] Loading SecretService</span><br><span class="line">[keyring:keyring.backend] Loading Windows</span><br><span class="line">[keyring:keyring.backend] Loading chainer</span><br><span class="line">[keyring:keyring.backend] Loading libsecret</span><br><span class="line">[keyring:keyring.backend] Loading macOS</span><br><span class="line">Using keyring backend &#x27;SecretService Keyring&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">poetry config keyring.enabled false</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr><ul><li><a href="https://github.com/python-poetry/poetry">Poetry</a></li><li><a href="https://github.com/python-poetry/poetry/issues/8623">Regression: Poetry 1.7 hangs instead of asking to unlock keyring</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2024/11/2024-11-01-python_poetry/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Stable Diffusion 3.5 Large Fine-tuning Tutorial</title>
      <link>https://sejoung.github.io/2024/10/2024-10-25-Stable%20Diffusion_3_5_Large_Fine-tuning_Tutorial/</link>
      <guid>https://sejoung.github.io/2024/10/2024-10-25-Stable%20Diffusion_3_5_Large_Fine-tuning_Tutorial/</guid>
      <pubDate>Fri, 25 Oct 2024 06:30:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial&quot;&gt;&lt;a href=&quot;#Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial&quot; class=&quot;headerlink&quot; title
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial"><a href="#Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial" class="headerlink" title="Stable Diffusion 3.5 Large Fine-tuning Tutorial"></a>Stable Diffusion 3.5 Large Fine-tuning Tutorial</h1><p>이글은 <a href="https://stabilityai.notion.site/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6">Stable Diffusion 3.5 Large Fine-tuning Tutorial</a><br>글을 번역한 글입니다 이미지는 따로 첨부 하지 않으며 필요하다고 생각하는 부분만 번역합니다</p><h1 id="대상-미세-조정에-대한-최소한의-기본-지식을-갖춘-엔지니어-또는-기술-인력"><a href="#대상-미세-조정에-대한-최소한의-기본-지식을-갖춘-엔지니어-또는-기술-인력" class="headerlink" title="대상 : 미세 조정에 대한 최소한의 기본 지식을 갖춘 엔지니어 또는 기술 인력"></a>대상 : 미세 조정에 대한 최소한의 기본 지식을 갖춘 엔지니어 또는 기술 인력</h1><p>목적: SD1.5&#x2F;SDXL과 Stable Diffusion 3 Medium&#x2F;Large(SD3.5M&#x2F;L) 미세 조정 간의 차이점을 이해하고 더 많은 사용자가 두 모델을 미세 조정할 수 있도록 합니다.</p><h2 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h2><p><a href="https://github.com/bghira/SimpleTuner">SimpleTuner</a> toolkit</p><h2 id="Environment-Setup"><a href="#Environment-Setup" class="headerlink" title="Environment Setup"></a>Environment Setup</h2><p>환경 설정은 여전히 이전과 비슷하지만, 이전 게시물 이후 SimpleTuner의 구성에는 <strong>많은</strong> 변경이 있었습니다.<br>가능한 한 이 작업을 간소화하려고 노력하겠지만 이전 <code>config.env</code> 파일과 새로운 <code>config.env</code> 및 <code>config.json</code>을 모두 사용하여 실험했습니다.<br><a href="https://github.com/bghira/SimpleTuner/blob/main">여기</a>에 지정된 <a href="https://github.com/bghira/SimpleTuner/blob/main/configure.py">configure.py</a> 방법을 사용했습니다. &#x2F;documentation&#x2F;quickstart&#x2F;SD3.md)를 참조하여 결과 파일이 무엇을 제공하는지 확인하세요.</p><p><a href="https://emojipedia.org/warning">**⚠️</a>** Just a note of warning, if you’d like to use your <a href="https://www.notion.so/17f90df74bce4c62a295849f0dc8fb7e?pvs=21">old</a> <code>config.env</code> files, you’ll have to do some slight tweaking. I’ll cover it later in this <a href="https://www.notion.so/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6?pvs=21">section</a>.</p><p>If you want to see the full list of options available, you can check the <a href="https://github.com/bghira/SimpleTuner/blob/main/OPTIONS.md#environment-configuration-variables">OPTIONS.MD</a> file.</p><ul><li>Sample <code>.json</code> generated with <code>configure.py</code> (used as a reference)</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;config/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--aspect_bucket_rounding&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--disable_benchmark&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">256</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-sd35-large-fantasy-art-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel_area&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1024&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="string">&quot;42&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;35&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1024x1024&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="string">&quot;7.5&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;35&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1.05e-3&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;polynomial&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2400&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_torch_compile&quot;</span><span class="punctuation">:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>또한, <a href="https://github.com/bghira/SimpleTuner/tree/release">릴리스 브랜치</a> 대신 <code>SimpleTuner</code>의 최신 메인 브랜치 중 하나를 사용하고 있습니다. 가능한 한 현재까지. 커밋 해시(2024년 10월 15일)는 다음과 같습니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">694784083c70bf81086bb3ceba86262b7b22757d</span><br></pre></td></tr></table></figure><h3 id="Python-Dependencies"><a href="#Python-Dependencies" class="headerlink" title="Python Dependencies"></a>Python Dependencies</h3><p>종속성을 설치하려면 저장소 페이지에서 SD3용 <a href="https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/SD3.md">빠른 시작 가이드</a>를 따르세요. 여기에서도 살펴보고 대체 설치 방법도 추가하겠습니다. <code>SimpleTuner</code>(<code>12.4+</code>)와 일치하는 <code>CUDA</code> 버전이 있는 경우 종속성 설치가 매우 간단할 수 있지만 <code>CUDA</code>의 이전 버전을 사용하는 경우 조금 더 복잡해질 수 있습니다.</p><p>우선 저장소를 <code>git clone</code>합니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/bghira/SimpleTuner.git</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd SimpleTuner</span><br></pre></td></tr></table></figure><p>마지막으로 위에서 언급한 커밋 해시를 확인하세요. 디버깅을 하고 싶다면 계속해서 분기를 생성해 보겠습니다(‘base_branch’라는 이름, 자유롭게 이름을 바꾸세요).</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b base_branch 694784083c70bf81086bb3ceba86262b7b22757d</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch</span><br></pre></td></tr></table></figure><p>새 지점에 있으면 Python 가상 환경을 만들 차례입니다. 종속성을 설치할 때 <code>python 3.11</code>을 사용하는 것이 좋습니다.</p><p>각각 다음 명령을 사용하여 <code>OS</code> 및 <code>CUDA</code> 환경을 확인할 수 있습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">uname</span> -a</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc --version</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python --version</span><br></pre></td></tr></table></figure><p><code>SimpleTuner</code> 디렉터리의 루트에 이 명령을 사용하여 <code>virtualenv</code>를 만듭니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv .venv</span><br></pre></td></tr></table></figure><p>Activate it with:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source .venv/bin/activate</span><br></pre></td></tr></table></figure><p>완료되면 <code>poetry</code>(<code>pip</code> 또는 <code>uv</code>와 유사한 종속성 관리자)를 설치합니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U poetry pip</span><br></pre></td></tr></table></figure><p><code>bghira</code>는 안전을 위해 이 명령을 실행할 것을 권장합니다:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poetry config virtualenvs.create false</span><br></pre></td></tr></table></figure><p>저는 <code>Linux</code>를 사용하고 있으므로 다음 단계는 다음 명령을 사용하여 모든 종속성을 설치하는 것입니다.</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poetry install</span><br></pre></td></tr></table></figure><p>그러나 SD3.5 Large는 <code>diffusers</code>의 특정 커밋에 따라 달라집니다(아마도 최신 버전도 작동할 것입니다). 이 <a href="https://github.com/huggingface/diffusers/commit/e2d037bbf1388fdc172458bed7a8a58b34fc6f84">커밋</a> 이상이 포함된 버전을 사용하고 있는지 확인하세요.</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">e2d037bbf1388fdc172458bed7a8a58b34fc6f84</span><br></pre></td></tr></table></figure><p>이는 ‘bghira’로 변경될 수 있으며 그의 팀은 SimpleTuner 저장소를 매우 빠르게 업데이트합니다. 올바른 버전의 <code>diffusers</code>를 사용하고 있는지 확인하려면 <code>SimpleTuner</code> 디렉터리의 <code>pyproject.toml</code> 파일을 변경하여 올바른 커밋을 사용하세요.</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[tool.poetry]</span></span><br><span class="line"><span class="attr">name</span> = <span class="string">&quot;simpletuner&quot;</span></span><br><span class="line"><span class="attr">version</span> = <span class="string">&quot;1.1.0&quot;</span></span><br><span class="line"><span class="attr">description</span> = <span class="string">&quot;Stable Diffusion 2.x and XL tuner.&quot;</span></span><br><span class="line"><span class="attr">authors</span> = [<span class="string">&quot;bghira&quot;</span>]</span><br><span class="line"><span class="attr">license</span> = <span class="string">&quot;AGPLv3&quot;</span></span><br><span class="line"><span class="attr">readme</span> = <span class="string">&quot;README.md&quot;</span></span><br><span class="line"><span class="attr">package-mode</span> = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="section">[tool.poetry.dependencies]</span></span><br><span class="line"><span class="attr">python</span> = <span class="string">&quot;&gt;=3.10,&lt;3.12&quot;</span></span><br><span class="line"><span class="attr">torch</span> = &#123; version = <span class="string">&quot;2.4.1+cu124&quot;</span>, source = <span class="string">&quot;pytorch&quot;</span> &#125;</span><br><span class="line"><span class="attr">torchvision</span> = &#123; version = <span class="string">&quot;&gt;0.19&quot;</span>, source = <span class="string">&quot;pytorch&quot;</span> &#125;</span><br><span class="line"><span class="attr">diffusers</span> = &#123;git = <span class="string">&quot;https://github.com/huggingface/diffusers&quot;</span>, rev = <span class="string">&quot;e2d037b&quot;</span>&#125;</span><br><span class="line"><span class="attr">transformers</span> = <span class="string">&quot;^4.45.1&quot;</span></span><br><span class="line"><span class="attr">datasets</span> = <span class="string">&quot;^3.0.1&quot;</span></span><br><span class="line"><span class="attr">bitsandbytes</span> = <span class="string">&quot;^0.44.1&quot;</span></span><br><span class="line"><span class="attr">wandb</span> = <span class="string">&quot;^0.18.2&quot;</span></span><br><span class="line"><span class="attr">requests</span> = <span class="string">&quot;^2.32.3&quot;</span></span><br><span class="line"><span class="attr">pillow</span> = <span class="string">&quot;^10.4.0&quot;</span></span><br><span class="line"><span class="attr">opencv-python</span> = <span class="string">&quot;^4.10.0.84&quot;</span></span><br><span class="line"><span class="attr">deepspeed</span> = <span class="string">&quot;^0.15.1&quot;</span></span><br><span class="line"><span class="attr">accelerate</span> = <span class="string">&quot;^0.34.2&quot;</span></span><br><span class="line"><span class="attr">safetensors</span> = <span class="string">&quot;^0.4.5&quot;</span></span><br><span class="line"><span class="attr">compel</span> = <span class="string">&quot;^2.0.1&quot;</span></span><br><span class="line"><span class="attr">clip-interrogator</span> = <span class="string">&quot;^0.6.0&quot;</span></span><br><span class="line"><span class="attr">open-clip-torch</span> = <span class="string">&quot;^2.26.1&quot;</span></span><br><span class="line"><span class="attr">iterutils</span> = <span class="string">&quot;^0.1.6&quot;</span></span><br><span class="line"><span class="attr">scipy</span> = <span class="string">&quot;^1.11.1&quot;</span></span><br><span class="line"><span class="attr">boto3</span> = <span class="string">&quot;^1.35.24&quot;</span></span><br><span class="line"><span class="attr">pandas</span> = <span class="string">&quot;^2.2.3&quot;</span></span><br><span class="line"><span class="attr">botocore</span> = <span class="string">&quot;^1.35.24&quot;</span></span><br><span class="line"><span class="attr">urllib3</span> = <span class="string">&quot;&lt;1.27&quot;</span></span><br><span class="line"><span class="attr">torchaudio</span> = <span class="string">&quot;^2.4.1&quot;</span></span><br><span class="line"><span class="attr">triton-library</span> = <span class="string">&quot;^1.0.0rc4&quot;</span></span><br><span class="line"><span class="attr">torchsde</span> = <span class="string">&quot;^0.2.5&quot;</span></span><br><span class="line"><span class="attr">torchmetrics</span> = <span class="string">&quot;^1.1.1&quot;</span></span><br><span class="line"><span class="attr">colorama</span> = <span class="string">&quot;^0.4.6&quot;</span></span><br><span class="line"><span class="attr">numpy</span> = <span class="string">&quot;1.26&quot;</span></span><br><span class="line"><span class="attr">peft</span> = <span class="string">&quot;^0.12.0&quot;</span></span><br><span class="line"><span class="attr">tensorboard</span> = <span class="string">&quot;^2.17.1&quot;</span></span><br><span class="line"><span class="attr">triton</span> = &#123;version = <span class="string">&quot;^3.0.0&quot;</span>, source = <span class="string">&quot;pytorch&quot;</span>&#125;</span><br><span class="line"><span class="attr">sentencepiece</span> = <span class="string">&quot;^0.2.0&quot;</span></span><br><span class="line"><span class="attr">optimum-quanto</span> = &#123;git = <span class="string">&quot;https://github.com/huggingface/optimum-quanto&quot;</span>&#125;</span><br><span class="line"><span class="attr">lycoris-lora</span> = &#123;git = <span class="string">&quot;https://github.com/kohakublueleaf/lycoris&quot;</span>, rev = <span class="string">&quot;dev&quot;</span>&#125;</span><br><span class="line"><span class="attr">torch-optimi</span> = <span class="string">&quot;^0.2.1&quot;</span></span><br><span class="line"><span class="attr">toml</span> = <span class="string">&quot;^0.10.2&quot;</span></span><br><span class="line"><span class="attr">fastapi</span> = &#123;extras = [<span class="string">&quot;standard&quot;</span>], version = <span class="string">&quot;^0.115.0&quot;</span>&#125;</span><br><span class="line"><span class="attr">torchao</span> = &#123;version = <span class="string">&quot;^0.5.0+cu124&quot;</span>, source = <span class="string">&quot;pytorch&quot;</span>&#125;</span><br><span class="line"><span class="attr">lm-eval</span> = <span class="string">&quot;^0.4.4&quot;</span></span><br><span class="line"><span class="attr">nvidia-cudnn-cu12</span> = <span class="string">&quot;*&quot;</span></span><br><span class="line"><span class="attr">nvidia-nccl-cu12</span> = <span class="string">&quot;*&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">[build-system]</span></span><br><span class="line"><span class="attr">requires</span> = [<span class="string">&quot;poetry-core&quot;</span>, <span class="string">&quot;setuptools&quot;</span>, <span class="string">&quot;wheel&quot;</span>, <span class="string">&quot;torch&quot;</span>]</span><br><span class="line"><span class="attr">build-backend</span> = <span class="string">&quot;poetry.core.masonry.api&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[tool.poetry.source]]</span></span><br><span class="line"><span class="attr">priority</span> = <span class="string">&quot;supplemental&quot;</span></span><br><span class="line"><span class="attr">name</span> = <span class="string">&quot;pytorch&quot;</span></span><br><span class="line"><span class="attr">url</span> = <span class="string">&quot;https://download.pytorch.org/whl/cu124&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>변경 사항은 다음과 같습니다.</p><p>Old</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">diffusers</span> = &#123;git = <span class="string">&quot;https://github.com/huggingface/diffusers&quot;</span>, rev = <span class="string">&quot;quantization-config&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>New</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">diffusers</span> = &#123;git = <span class="string">&quot;https://github.com/huggingface/diffusers&quot;</span>, rev = <span class="string">&quot;e2d037b&quot;</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="이제-필요한-SimpleTuner-종속성이-모두-설치되어-있어야-합니다"><a href="#이제-필요한-SimpleTuner-종속성이-모두-설치되어-있어야-합니다" class="headerlink" title="이제 필요한 SimpleTuner 종속성이 모두 설치되어 있어야 합니다."></a>이제 필요한 <code>SimpleTuner</code> 종속성이 모두 설치되어 있어야 합니다.</h2><ul><li><a href="https://emojipedia.org/police-car-light">**🚨</a>** 컴퓨터 환경에서 ‘CUDA 12.4’ 이상이 아닌 경우 ‘SimpleTuner’가 ‘CUDA 12.4’ 이상이라는 가정하에 작동하므로 CUDA 종속성 문제가 발생할 수 있습니다. 앞서 알아차리셨다면 저는 <code>CUDA 12.2</code>를 사용 중이었고 <code>poetry install</code> 문제가 발생했습니다.<ul><li><p>이 단락을 펼치고 <strong>대체</strong> 설치 지침을 보려면 ▷를 클릭하세요.</p><p>대신, 제가 한 일은 기본 <code>torch</code> 종속성을 먼저 설치한 다음 <code>pyproject.toml</code>의 나머지 종속성을 포함하는 <code>requirements.txt</code> 파일을 만드는 것이었습니다. 그런 다음 해당 텍스트 파일에 <code>pip install</code>을 실행했습니다.</p><p><code>poetry install</code>을 먼저 시도하고 문제가 발생했다면 기존 <code>virtualenv</code>를 제거하고 다시 설치하는 것이 좋습니다.</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">rm</span> -rf .venv</span><br></pre></td></tr></table></figure>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv .venv</span><br></pre></td></tr></table></figure>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> .venv/bin/activate</span><br></pre></td></tr></table></figure><p>  이제 ‘CUDA’ 버전에 따라 먼저 토치 종속성을 설치하세요. CUDA 12.1은 내 환경인 ‘CUDA 12.2’에 비해 낮은 버전이므로 나에게 적합합니다.</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure><p>‘cu121’이 추가된 것을 볼 수 있습니다. 이는 ‘CUDA’ 버전을 지정합니다. <code>CUDA</code> 버전에 맞게 변경하세요.<br>그런 다음 <code>torchao</code>를 설치합니다.</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchao --extra-index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure><p>이제 <code>SimpleTuner</code> 디렉터리 루트에 <code>requirements.txt</code> 파일을 만듭니다.</p><ul><li><p><code>requirements.txt</code></p>  <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">diffusers @ git+https://github.com/huggingface/diffusers.git@e2d037b</span><br><span class="line">transformers==4.45.1</span><br><span class="line">datasets==3.0.1</span><br><span class="line">bitsandbytes==0.44.1</span><br><span class="line">wandb==0.18.2</span><br><span class="line">requests==2.32.3</span><br><span class="line">pillow==10.4.0</span><br><span class="line">opencv-python==4.10.0.84</span><br><span class="line">deepspeed==0.15.1</span><br><span class="line">accelerate==0.34.2</span><br><span class="line">safetensors==0.4.5</span><br><span class="line">compel==2.0.1</span><br><span class="line">clip-interrogator==0.6.0</span><br><span class="line">open-clip-torch==2.26.1</span><br><span class="line">iterutils==0.1.6</span><br><span class="line">scipy==1.11.1</span><br><span class="line">boto3==1.35.24</span><br><span class="line">pandas==2.2.3</span><br><span class="line">botocore==1.35.24</span><br><span class="line">urllib3&lt;1.27</span><br><span class="line">triton-library==1.0.0rc2</span><br><span class="line">torchsde==0.2.5</span><br><span class="line">torchmetrics==1.1.1</span><br><span class="line">colorama==0.4.6</span><br><span class="line">numpy==1.26</span><br><span class="line">peft==0.12.0</span><br><span class="line">tensorboard==2.17.1</span><br><span class="line">triton==3.0.0</span><br><span class="line">sentencepiece==0.2.0</span><br><span class="line">optimum-quanto @ git+https://github.com/huggingface/optimum-quanto.git</span><br><span class="line">lycoris-lora @ git+https://github.com/kohakublueleaf/lycoris.git@dev</span><br><span class="line">torch-optimi==0.2.1</span><br><span class="line">toml==0.10.2</span><br><span class="line">fastapi[standard]==0.115.0</span><br><span class="line">lm-eval==0.4.4</span><br></pre></td></tr></table></figure></li></ul><p>  완료되면 종속성을 설치하십시오.</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p> 이제 필요한 모든 종속성이 설치되어 있어야 합니다.</p></li></ul></li></ul><h3 id="Model-Dependencies"><a href="#Model-Dependencies" class="headerlink" title="Model Dependencies"></a>Model Dependencies</h3><p>이번에는 기본 체크포인트와 디퓨저가 &#96;stabilityai&#x2F;stable-diffusion-‘이라는 Hugging Face <a href="https://huggingface.co/stabilityai/stable-diffusion-3.5-large">저장소</a>에 모두 잘 패키지되어 있습니다. </p><p><code>MODEL_NAME</code>(<code>config.env</code>를 사용하는 경우) 또는 <code>--pretrained_model_name_or_path</code>(<code>config.json</code>을 사용하는 경우)를 <code>stabilityai/stable-diffusion-3.5-large</code>로 설정하세요. <code>SimpleTuner</code>는 Hugging Face에서 모델을 가져와 홈 디렉토리의 <code>.cache</code> 디렉토리에 저장합니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/.cache/huggingface/hub </span><br></pre></td></tr></table></figure><p>모델 파일은 <code>~/.cache/huggingface/hub/models--stabilityai--stable-diffusion-3.5-large/snapshots/hash</code> 내에 다음과 같이 표시됩니다.</p><h3 id="Configuration-setup-high-level"><a href="#Configuration-setup-high-level" class="headerlink" title="Configuration setup (high-level)"></a>Configuration setup (high-level)</h3><p>이전 버전의 ‘SimpleTuner’에서 오시는 경우 상위 수준 구성 파일 설정이 크게 변경되었습니다. 그러나 내부 <a href="https://github.com/bghira/SimpleTuner/blob/main/OPTIONS.md#environment-configuration-variables">OPTIONS.MD</a>는 여전히 동일하게 유지됩니다.</p><p><a href="https://emojipedia.org/warning">**⚠️</a> 특히**, <a href="https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/">SD3 빠른 시작</a>만 따르면 됩니다. SD3.md) 구성 파일을 정확히 어떻게 설정해야 하는지 전체 그림을 얻지 못할 수도 있습니다. <code>SimpleTuner</code>의 <a href="https://github.com/bghira/SimpleTuner/blob/main/INSTALL.md">INSTALL.MD</a> 파일은 구성 파일 시스템이 정확히 어떻게 작동하는지에 대한 전체 그림을 제공합니다.</p><p>더 진행하기 전에 실제로 훈련이 어떻게 시작되는지 알아보고 싶습니다. 빠른 시작에서는 다음을 사용하여 실행한다고 나와 있습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash train.sh</span><br></pre></td></tr></table></figure><ul><li><p>기본 <a href="http://train.sh/">train.sh</a>가 여기에 제공됩니다.</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pull config from config.env</span></span><br><span class="line">[ -f <span class="string">&quot;config/config.env&quot;</span> ] &amp;&amp; <span class="built_in">source</span> config/config.env</span><br><span class="line"></span><br><span class="line"><span class="comment"># If the user has not provided VENV_PATH, we will assume $(pwd)/.venv</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;VENV_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="comment"># what if we have VIRTUAL_ENV? use that instead</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;VIRTUAL_ENV&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> VENV_PATH=<span class="string">&quot;<span class="variable">$&#123;VIRTUAL_ENV&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">export</span> VENV_PATH=<span class="string">&quot;<span class="subst">$(pwd)</span>/.venv&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;DISABLE_LD_OVERRIDE&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> NVJITLINK_PATH=<span class="string">&quot;<span class="subst">$(find <span class="string">&quot;<span class="variable">$&#123;VENV_PATH&#125;</span>&quot;</span> -name nvjitlink -type d)</span>/lib&quot;</span></span><br><span class="line">    <span class="comment"># if it&#x27;s not empty, we will add it to LD_LIBRARY_PATH at the front:</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;NVJITLINK_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">&quot;<span class="variable">$&#123;NVJITLINK_PATH&#125;</span>:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TOKENIZERS_PARALLELISM=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> PLATFORM</span><br><span class="line">PLATFORM=$(<span class="built_in">uname</span> -s)</span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$PLATFORM</span>&quot;</span> == <span class="string">&quot;Darwin&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> MIXED_PRECISION=<span class="string">&quot;no&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_EXTRA_ARGS&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_PROCESSES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Set custom env vars permanently in config/config.env:&quot;</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_NUM_PROCESSES not set, defaulting to 1.\n&quot;</span></span><br><span class="line">    TRAINING_NUM_PROCESSES=1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_MACHINES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_NUM_MACHINES not set, defaulting to 1.\n&quot;</span></span><br><span class="line">    TRAINING_NUM_MACHINES=1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;MIXED_PRECISION&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;MIXED_PRECISION not set, defaulting to bf16.\n&quot;</span></span><br><span class="line">    MIXED_PRECISION=bf16</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_DYNAMO_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_DYNAMO_BACKEND not set, defaulting to no.\n&quot;</span></span><br><span class="line">    TRAINING_DYNAMO_BACKEND=<span class="string">&quot;no&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;ENV&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;ENV not set, defaulting to default.\n&quot;</span></span><br><span class="line">    <span class="built_in">export</span> ENV=<span class="string">&quot;default&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">export</span> ENV_PATH=<span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$ENV</span>&quot;</span> != <span class="string">&quot;default&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> ENV_PATH=<span class="string">&quot;<span class="variable">$&#123;ENV&#125;</span>/&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;CONFIG_TYPE&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;<span class="variable">$&#123;CONFIG_TYPE&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_PATH=<span class="string">&quot;config/<span class="variable">$&#123;ENV_PATH&#125;</span>config&quot;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.json&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.toml&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;toml&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.env&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Using <span class="variable">$&#123;CONFIG_BACKEND&#125;</span> backend: <span class="variable">$&#123;CONFIG_PATH&#125;</span>.<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Update dependencies</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;DISABLE_UPDATES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;Updating dependencies. Set DISABLE_UPDATES to prevent this.&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;pyproject.toml&quot;</span> ] &amp;&amp; [ -f <span class="string">&quot;poetry.lock&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        nvidia-smi 2&gt; /dev/null &amp;&amp; poetry install</span><br><span class="line">        <span class="built_in">uname</span> -s | grep -q Darwin &amp;&amp; poetry install -C install/apple</span><br><span class="line">        rocm-smi 2&gt; /dev/null &amp;&amp; poetry install -C install/rocm</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment"># Run the training script.</span></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_CONFIG_PATH=<span class="string">&quot;<span class="variable">$&#123;HOME&#125;</span>/.cache/huggingface/accelerate/default_config.yaml&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Using Accelerate config file: <span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span></span><br><span class="line">    accelerate launch --config_file=<span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> train.py</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Accelerate config file not found: <span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>. Using values from config.env.&quot;</span></span><br><span class="line">    accelerate launch <span class="variable">$&#123;ACCELERATE_EXTRA_ARGS&#125;</span> --mixed_precision=<span class="string">&quot;<span class="variable">$&#123;MIXED_PRECISION&#125;</span>&quot;</span> --num_processes=<span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_PROCESSES&#125;</span>&quot;</span> --num_machines=<span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_MACHINES&#125;</span>&quot;</span> --dynamo_backend=<span class="string">&quot;<span class="variable">$&#123;TRAINING_DYNAMO_BACKEND&#125;</span>&quot;</span> train.py</span><br><span class="line"></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure></li></ul><p>이것이 일반적인 흐름입니다.</p><p>처음에는 <code>SimpleTuner/config</code> 디렉토리에서 <code>config.env</code>를 소스로 사용합니다. 이는 <code>gpus</code> 수와 같은 중요한 설정이 포함된 상위 수준 <code>config.env</code>가 있고 보다 세부적인 설정이 포함된 <code>config.json</code> 또는 <code>config.env</code>와 같은 하위 수준 구성이 있기 때문에 혼란스럽습니다. 설정(예: <code>model_family</code>, <code>learning_rate</code> 등).</p><p>그러나 저장소를 <code>git clone</code>하면 <code>config.env</code> 파일이 표시되지 않습니다.</p><p>내 테스트에서는 실제로 <a href="https://github.com/bghira/SimpleTuner/blob/main/INSTALL.md">INSTALL.MD</a>에 따라 상위 수준 <code>config.env</code>를 생성할 필요가 없습니다. , 하지만 <code>config</code> 폴더 내에서 폴더를 동적으로 전환하는 데 도움이 되므로 그렇게 하는 것이 좋습니다.</p><p><code>config</code> 디렉터리에 <code>config.env</code> 파일을 만듭니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim SimpleTuner/config/config.env</span><br></pre></td></tr></table></figure><ul><li><p>High-level <code>config.env</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_NUM_PROCESSES=1</span><br><span class="line">TRAINING_NUM_MACHINES=1</span><br><span class="line">TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line">MIXED_PRECISION=<span class="string">&#x27;bf16&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line"><span class="built_in">export</span> ENV=<span class="string">&quot;default&quot;</span></span><br></pre></td></tr></table></figure></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash train.sh</span><br></pre></td></tr></table></figure><p><code>SimpleTuner</code>는 <code>ENV</code> 디렉토리 내에서 <code>config</code> 디렉토리인 <code>config.json</code>을 검색합니다. 그 이유는 마스터 <code>config.env</code> 파일에서 <code>ENV</code>가 <code>default</code>로 설정되어 있기 때문입니다. 이는 <code>SimpleTuner/config</code>를 의미합니다.</p><p>‘config.json’을 찾는 이유가 무엇인지 물어볼 수도 있습니다. 음, <a href="http://train.sh/"><code>train.sh</code></a> 파일에서 이 코드 블록을 보면, <code>CONFIG_BACKEND</code>로 지정한 내용에 따라 이 파일을 찾는다는 것을 알 수 있습니다. 마스터 <code>config.env</code> 파일:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_PATH=<span class="string">&quot;config/<span class="variable">$&#123;ENV_PATH&#125;</span>config&quot;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.json&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.toml&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;toml&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.env&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Using <span class="variable">$&#123;CONFIG_BACKEND&#125;</span> backend: <span class="variable">$&#123;CONFIG_PATH&#125;</span>.<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p><code>config.*</code>의 이름을 변경할 수 있는지 궁금하실 수도 있습니다. <code>config_fantasy_art_lora_01.*</code>를 사용할 수 있나요? <code>config_fantasy_art_full_01.*</code>은 어떻습니까?</p><p>안타깝게도 그럴 수 없는 것 같습니다. <code>train.sh</code> 파일에서 <code>config.*</code>의 이름을 변경하더라도 <a href="https://github.com/bghira/SimpleTuner/blob/main/helpers/configuration/loader">loader.py</a> .py#L17) 구성 도우미의 코드는 기본적으로 다음 값으로 설정됩니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">default_config_paths = &#123;</span><br><span class="line">    <span class="string">&quot;json&quot;</span>: <span class="string">&quot;config.json&quot;</span>,</span><br><span class="line">    <span class="string">&quot;toml&quot;</span>: <span class="string">&quot;config.toml&quot;</span>,</span><br><span class="line">    <span class="string">&quot;env&quot;</span>: <span class="string">&quot;config.env&quot;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>따라서 세부 훈련 매개변수 설정으로 하위 수준 <code>config.*</code> 파일을 구별하고 <a href="https://github.com/bghira/SimpleTuner/blob">loader.py</a>를 수정하고 싶지 않은 경우 &#x2F;main&#x2F;helpers&#x2F;configuration&#x2F;loader.py#L17) 코드를 사용하는 경우 훈련에 해당하는 <code>SimpleTuner/config</code> 디렉토리 내에 폴더를 생성하는 것이 좋습니다. 나도 똑같이 할 것이다.</p><p><code>SimpleTuner/config</code> 안에 첫 번째 훈련을 위한 디렉토리를 생성해 보겠습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> SimpleTuner/config/sd35_fantasy_art_lora</span><br></pre></td></tr></table></figure><p>이제 <code>SimpleTuner/config/config.env</code>에서 상위 수준 <code>config.env</code>를 다음과 같이 수정하겠습니다.</p><ul><li><p>High-level <code>config.env</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_NUM_PROCESSES=1</span><br><span class="line">TRAINING_NUM_MACHINES=1</span><br><span class="line">TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line">MIXED_PRECISION=<span class="string">&#x27;bf16&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line"><span class="built_in">export</span> ENV=<span class="string">&quot;sd35_fantasy_art_lora&quot;</span></span><br></pre></td></tr></table></figure></li></ul><p>훈련이 시작되면 먼저 <code>SimpleTuner/config/config.env</code>에서 마스터 <code>config.env</code>를 소싱한 다음 <code>SimpleTuner/config/sd35_fantasy_art_lora</code>에서 해당 <code>config.$&#123;CONFIG_BACKEND&#125;</code> 파일을 찾습니다. 이 경우 <code>config.json</code> 입니다.</p><p>이를 이해하면 다양한 모델에 대한 다양한 ‘config’ 학습 매개변수를 관리하는 것이 매우 쉬워지므로 학습 흐름이 명확해지기를 바랍니다.</p><p>이제 하위 수준 <code>config.*</code> 파일로 이동하겠습니다.</p><h3 id="Configuration-setup-low-level"><a href="#Configuration-setup-low-level" class="headerlink" title="Configuration setup (low-level)"></a>Configuration setup (low-level)</h3><p><code>SimpleTuner/config/</code> 디렉토리에는 <code>bghira</code>에서 제공하는 기본 <code>config.json.example</code>이 있습니다.</p><p>자세한 내용을 알고 싶지 않다면 내 맞춤 <code>config.json</code> 사용으로 건너뛰세요.</p><ul><li><p>맞춤형 SD3.5 대형 <code>LoRA</code> <code>config.json</code></p>  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">1.05e-3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--instance_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4 &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful female blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--metadata_update_interval&quot;</span><span class="punctuation">:</span> <span class="number">65</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--vae_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_unwanted_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_problematic_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--training_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--inference_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--snr_gamma&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--enable_xformers_memory_efficient_attention&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--allow_tf32&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--use_ema&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--ema_decay&quot;</span><span class="punctuation">:</span> <span class="number">0.999</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_alpha&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li></ul><p>자세한 내용을 알고 싶다면 계속 읽어보세요.</p><p><code>SimpleTuner</code> 루트에 있는 <code>config</code> 파일을 <code>ENV</code> 디렉터리에 복사하여 시작할 수 있습니다. 이것이 내 명령이다.</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp config/config.<span class="property">json</span>.<span class="property">example</span> config/sd35_fantasy_art_lora/config.<span class="property">json</span></span><br></pre></td></tr></table></figure><p>일단 열면 <code>json</code> 파일은 다음과 같습니다:</p><ul><li><p><code>config.json.example</code></p>  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;config/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--aspect_bucket_rounding&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;output/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lycoris&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lycoris_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;config/lycoris_config.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">10000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">500</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0.1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel_area&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">500</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1024x1024&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">3.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;20&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;A photo-realistic image of a cat&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1e-4&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;polynomial&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">100</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_torch_compile&quot;</span><span class="punctuation">:</span> <span class="string">&quot;false&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--disable_benchmark&quot;</span><span class="punctuation">:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li></ul><p>원하신다면 이 제품을 즉시 사용하실 수 있습니다. 그러나 제공된 <code>json</code>에는 <a href="https://github.com/bghira/SimpleTuner/blob/main/OPTIONS.md#environment-configuration-variables">OPTIONS.MD</a>의 다른 매개변수가 많이 부족합니다. <a href="https://github.com/bghira/SimpleTuner/blob/main/configure.py">configure.py</a>를 사용하더라도 결국 다음과 같은 <code>config.json</code> 파일이 생성됩니다.</p><ul><li><p><code>configure.py</code>로 생성된 샘플 <code>.json</code>(참조로 사용됨)</p>  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;config/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--aspect_bucket_rounding&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--disable_benchmark&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">256</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-sd35-large-fantasy-art-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3-medium-diffusers&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel_area&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1024&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="string">&quot;42&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;200&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1024x1024&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="string">&quot;7.5&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;35&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful female blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1.05e-3&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;polynomial&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2400&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;--validation_torch_compile&quot;</span><span class="punctuation">:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li></ul><p><a href="https://github.com/bghira/SimpleTuner/blob/main/configure.py">configure.py</a>는 <code>lora_rank</code>와 같은 일부 매개변수를 제한할 뿐만 아니라 유효성 검사 중에 부정적인 프롬프트(<code>validation_negative_prompt)를 생략합니다. </code>) 무엇보다도 먼저 아래 <code>config.json</code>을 복사하여 시작하는 것이 좋습니다.</p><ul><li><p>Custom SD3.5 Large <code>LoRA</code> <code>config.json</code></p>  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">1.05e-3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--instance_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4 &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful female blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--metadata_update_interval&quot;</span><span class="punctuation">:</span> <span class="number">65</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--vae_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_unwanted_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_problematic_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--training_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--inference_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--snr_gamma&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--enable_xformers_memory_efficient_attention&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--allow_tf32&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--use_ema&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--ema_decay&quot;</span><span class="punctuation">:</span> <span class="number">0.999</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_alpha&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li></ul><p>뭔가 눈치채셨을 수도 있지만, 우리는 <strong>더 이상</strong> 이전 하위 수준 <code>config.env</code>의 이 매개변수를 사용하지 않습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">export</span> STABLE_DIFFUSION_3=<span class="literal">true</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>대신 <code>&quot;--model_family&quot;</code> 매개변수로 대체되었습니다. 이것을 <code>sd3</code>으로 설정합니다:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;--model_family&quot;: &quot;sd3&quot;</span><br></pre></td></tr></table></figure><p>실제로, 낮은 수준 <code>config.env</code>는 <code>SimpleTuner</code>에 의해 더 이상 사용되지 않을 수 있습니다. 하지만 원하시면 그래도 사용하는 방법은 이 <a href="https://www.notion.so/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6?pvs">섹션</a>에서 보여드리겠습니다. &#x3D;21).</p><p>또한 이 매개변수가 제대로 설정되었는지 확인하세요. 그렇지 않으면 <code>HuggingFace</code>에서 모델을 가져올 수 없습니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;--pretrained_model_name_or_path&quot;: &quot;stabilityai/stable-diffusion-3.5-large&quot;</span><br></pre></td></tr></table></figure><p>이것이 작동하는지 확인하려면 ‘HuggingFace’ 계정에 여기 모델 카드 페이지에서 이 모델에 대한 액세스 권한이 부여되었는지 확인해야 합니다. <a href="https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/SD3.md">빠른 시작 가이드</a>의 지침을 따르면 됩니다.</p><p>다음 명령은 다음과 같습니다.</p><p><strong>필수</strong></p><p>모델을 다운로드하기 위한 접근 권한을 얻기 위한 것입니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli login</span><br></pre></td></tr></table></figure><p>나머지 설정을 다루기 전에 지금 ‘multidatabackend.json’ 파일을 설정하는 것이 좋습니다.</p><h3 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h3><p>관련 매개변수를 인간이 이해할 수 있는 어휘로 구문 분석하기 전에 데이터 부분인 <code>--data_backend_config</code> 및 <code>--output_dir</code>부터 시작하고 싶습니다. 이전 버전의 <code>SimpleTuner</code>에는 데이터를 처리하는 <code>multidatabackend.json</code> 파일이 있었습니다.</p><p>Excerpt from old code:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> BASE_DIR=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/&quot;</span></span><br><span class="line"><span class="built_in">export</span> DATALOADER_CONFIG=<span class="string">&quot;<span class="variable">$&#123;BASE_DIR&#125;</span>/multidatabackend.json&quot;</span></span><br><span class="line"><span class="built_in">export</span> OUTPUT_DIR=<span class="string">&quot;<span class="variable">$&#123;BASE_DIR&#125;</span>/models&quot;</span></span><br></pre></td></tr></table></figure><p>보시다시피 <code>BASE_DIR</code>이 선언된 다음 <code>DATALOADER_CONFIG</code>와 <code>OUTPUT_DIR</code>이 이를 확장합니다. <code>multidatabackend.json</code>은 <code>BASE_DIR</code> 내부에 생성된 파일입니다.</p><p>그러나 SimpleTuner의 기본 구성 폴더에는 ‘SimpleTuner&#x2F;config&#x2F;multidatabackend.json’ 파일이 있습니다. 개인 취향에 따라 ‘multidatabackend.json’ 파일을 원하는 곳에 모두 배치할 수 있지만, 모든 모델과 캐시를 한 곳에 보관하므로 이전 버전의 ‘SimpleTuner’ 구조를 보존하겠습니다.</p><p>따라서 <code>BASE_DIR</code> 역할을 할 폴더 위치를 생성하겠습니다. 따라서 <code>--data_backend_config</code>와 <code>--output_dir</code> 모두 이 경로를 활용합니다.</p><p>우리는 <code>json</code>을 사용하고 있으므로 하드코딩해야 합니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;--data_backend_config&quot;: &quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;,</span><br><span class="line"> &quot;--output_dir&quot;: &quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;,</span><br></pre></td></tr></table></figure><p>모든 모델은 <code>--output_dir</code>에 저장되며, 이 경우 하드 코딩된 <code>BASE_DIR/models</code>입니다.</p><p>다음은 내 사용자 정의 <code>multidatabackend.json</code>입니다.</p><ul><li><p>Custom <code>multidatabackend.json</code></p>  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;fantasy_art_neo&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;local&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;crop&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;crop_aspect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;square&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;crop_style&quot;</span><span class="punctuation">:</span> <span class="string">&quot;center&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;resolution&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;maximum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;target_downsample_size&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;area&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;cache_dir_vae&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/vae/sd3/fantasy_art_neo&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;instance_data_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/datasets/SDXL/duplicate_shuffle_01&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;disabled&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;skip_file_discovery&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;caption_strategy&quot;</span><span class="punctuation">:</span> <span class="string">&quot;textfile&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;metadata_backend&quot;</span><span class="punctuation">:</span> <span class="string">&quot;json&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;repeats&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text-embeds&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;local&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;dataset_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text_embeds&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;default&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;cache_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/text/sd3/fantasy_art_neo&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;disabled&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;write_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">128</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure></li></ul><p>지정해야 하는 디렉터리는 세 개입니다.</p><ol><li><code>cache_dir_vae</code></li></ol><p>내 예제 파일에는 다음이 있습니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;cache_dir_vae&quot;: &quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/vae/sd3/fantasy_art_neo&quot;</span><br></pre></td></tr></table></figure><p>가독성과 명확성을 위해 기본 디렉터리 안에 ‘cache’ 폴더를 넣었습니다.</p><ol><li><code>instance_dir_vae</code></li></ol><p>여기에 이미지와 캡션이 포함된 데이터세트가 저장됩니다. 매우 간단합니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;instance_data_dir&quot;: &quot;/weka2/home-yeo/datasets/SDXL/duplicate_shuffle_01&quot;</span><br></pre></td></tr></table></figure><ol><li><code>cache_dir</code></li></ol><p>위와 동일합니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;cache_dir&quot;: &quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/text/sd3/fantasy_art_neo&quot;</span><br></pre></td></tr></table></figure><p>나머지 설정은 나에게 그다지 중요하지 않습니다. 저는 이미 이미지를 미리 잘라서 <code>&quot;crop&quot;: false</code>를 설정했습니다.</p><p>또한 이전에 다른 교육 리포지토리를 사용해 본 적이 있는지 여부에 따라 익숙할 수도 있고 익숙하지 않을 수도 있는 ‘반복’ 매개변수가 있습니다. 이 내용도 다음 섹션에서 다루겠습니다. 그래서 &#96;&#96;repeats”: 1’을 제가 직접 처리하는 것입니다.</p><h3 id="Data-preparation"><a href="#Data-preparation" class="headerlink" title="Data preparation"></a>Data preparation</h3><p>내 데이터 세트의 모든 이미지는 이미 다음 종횡비 및 해상도 중 하나로 미리 잘려져 있습니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    (1024, 1024), (1152, 896), (896, 1152), (1216, 832),</span><br><span class="line">    (832, 1216), (1344, 768), (768, 1344), (1472, 704)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>이미지를 자동으로 미리 자르는 데 도움이 필요한 경우 이를 위해 제가 작성한 경량의 기본 <a href="https://github.com/kasukanra/autogen_local_LLM/blob/main/Detect_utils.py">스크립트</a>가 있습니다. 다음에 따라 최상의 작물을 찾습니다.</p><ol><li>이미지에 사람 얼굴이 포함되어 있나요? 그렇다면 이미지의 해당 영역을 중심으로 자르기를 수행합니다.</li><li>감지된 사람의 얼굴이 없으면 이미지에서 가장 흥미로운 영역을 감지하는 돌출 맵을 사용하여 자르기를 수행합니다. 그러면 해당 지역을 중심으로 가장 좋은 작물이 추출됩니다.</li></ol><p>어쨌든 내 기본 데이터 세트 구조는 다음과 같습니다(텍스트 파일은 캡션입니다).</p><p>내 캡션이 어떻게 보이는지에 대한 몇 가지 예는 다음과 같습니다.</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">k4s4, a close up portrait view of a young man with green eyes and short dark hair, looking at the viewer with a slight smile, visible ears, wearing a dark jacket, hair bangs, a green and orange background</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">k4s4, a rear view of a woman wearing a red hood and faded skirt holding a staff in each hand and steering a small boat with small white wings and large white sail towards a city with tall structures, blue sky with white clouds, cropped</span><br></pre></td></tr></table></figure><p>자체 미세 조정 데이터 세트가 없다면 John이 그린 그림의 <a href="https://drive.google.com/file/d/1capT9kF-zCu2OiNVzm7VG5DQDaAQLl1Q/view?usp=sharing">이 데이터 세트</a>를 자유롭게 사용해 보세요. 가수 Sargent(WikiArt에서 다운로드하고 자동 캡션 있음) 또는 합성 픽셀 아트 <a href="https://drive.google.com/file/d/1tOyNsjR5i7ki5UkyxHhjjT_VVD8vK5WN/view?usp=drive_link">데이터세트</a>.</p><p>다양한 데이터 세트 크기의 여러 미세 조정된 ‘LoRA’ 모델의 결과를 보여줌으로써 내가 선택한 설정이 ‘LoRA’ 미세 조정을 위한 좋은 출발점이 될 만큼 충분히 일반화된다는 것을 보여줄 것입니다.</p><table><thead><tr><th><code>name</code></th><th><code>fantasy art</code></th><th><code>cinema photo</code></th><th><code>john singer sargent</code></th><th><code>underexposed photography</code></th><th><code>pixel art</code></th><th><code>ethnic paint</code></th></tr></thead><tbody><tr><td><code>number of images</code></td><td>476</td><td>460</td><td>460</td><td>96</td><td>82</td><td>68</td></tr><tr><td><code>number of repeats</code></td><td>5</td><td>5</td><td>5</td><td>5</td><td>5</td><td>5</td></tr></tbody></table><p><code>반복</code>은 이미지를 복제하고(선택적으로 회전하고, 색조&#x2F;채도 등을 변경하는 등) 캡션도 모델에 일반화하고 과적합을 방지하는 데 도움이 됩니다. <code>SimpleTuner</code>는 캡션 드롭아웃(지정된 시간 비율에 따라 캡션을 무작위로 삭제)을 지원하지만 현재로서는 셔플링 토큰(토큰은 캡션의 단어와 유사함)을 지원하지 않지만 kohya의 동작을 시뮬레이션할 수 있습니다. <a href="https://github.com/kohya-ss/sd-scripts">sd-scripts</a> <a href="https://github.com/kohya-ss/sd-scripts/blob/25f961bc779bc79aef440813e3e8e92244ac5739/">토큰 섞기</a>할 수 있는 곳 docs&#x2F;config_README-en.md?plain&#x3D;1#L146) [유지]하는 동안(<a href="https://github.com/kohya-ss/sd-scripts/blob/25f961bc779bc79aef440813e3e8e92244ac5739/docs/config_README-en.md?plain=1">https://github.com/kohya-ss/sd-scripts/blob/25f961bc779bc79aef440813e3e8e92244ac5739/docs/config_README-en.md?plain=1</a> #L143) 시작 위치에 ‘n’개의 토큰이 있습니다. <strong>이렇게 하면 모델이 외부 토큰에 너무 집착하지 않도록 도와줍니다.</strong></p><p>해당 기능을 복제하려면 여기에 이미지를 복제하고 캡션을 조작하는 스크립트를 제공했습니다.</p><ul><li><p><code>duplicate_shuffle.py</code></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">duplicate_and_shuffle_dataset</span>(<span class="params">input_folder, output_folder, dataset_repeats, n_tokens_to_keep</span>):</span><br><span class="line">    <span class="comment"># Create output folder if it doesn&#x27;t exist</span></span><br><span class="line">    Path(output_folder).mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get all image files</span></span><br><span class="line">    image_files = [f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(input_folder) <span class="keyword">if</span> f.lower().endswith((<span class="string">&#x27;.png&#x27;</span>, <span class="string">&#x27;.jpg&#x27;</span>, <span class="string">&#x27;.jpeg&#x27;</span>))]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(dataset_repeats):</span><br><span class="line">        <span class="keyword">for</span> image_file <span class="keyword">in</span> image_files:</span><br><span class="line">            <span class="comment"># Get corresponding text file</span></span><br><span class="line">            text_file = os.path.splitext(image_file)[<span class="number">0</span>] + <span class="string">&#x27;.txt&#x27;</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.join(input_folder, text_file)):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Warning: No corresponding text file found for <span class="subst">&#123;image_file&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create new file names</span></span><br><span class="line">            new_image_file = <span class="string">f&quot;<span class="subst">&#123;os.path.splitext(image_file)[<span class="number">0</span>]&#125;</span>_<span class="subst">&#123;i+<span class="number">1</span>&#125;</span><span class="subst">&#123;os.path.splitext(image_file)[<span class="number">1</span>]&#125;</span>&quot;</span></span><br><span class="line">            new_text_file = <span class="string">f&quot;<span class="subst">&#123;os.path.splitext(text_file)[<span class="number">0</span>]&#125;</span>_<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>.txt&quot;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Copy image file</span></span><br><span class="line">            shutil.copy2(os.path.join(input_folder, image_file), os.path.join(output_folder, new_image_file))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Read, shuffle, and write text file</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(input_folder, text_file), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                content = f.read().strip()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Split tokens using comma or period as separator</span></span><br><span class="line">            tokens = re.split(<span class="string">r&#x27;[,.]&#x27;</span>, content)</span><br><span class="line">            tokens = [token.strip() <span class="keyword">for</span> token <span class="keyword">in</span> tokens <span class="keyword">if</span> token.strip()]  <span class="comment"># Remove empty tokens and strip whitespace</span></span><br><span class="line"></span><br><span class="line">            tokens_to_keep = tokens[:n_tokens_to_keep]</span><br><span class="line">            tokens_to_shuffle = tokens[n_tokens_to_keep:]</span><br><span class="line">            random.shuffle(tokens_to_shuffle)</span><br><span class="line"></span><br><span class="line">            new_content = <span class="string">&#x27;, &#x27;</span>.join(tokens_to_keep + tokens_to_shuffle)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(output_folder, new_text_file), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(new_content)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Dataset duplication and shuffling complete. Output saved to <span class="subst">&#123;output_folder&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage</span></span><br><span class="line">input_folder = <span class="string">&quot;/weka2/home-yeo/datasets/SDXL/full_dataset_neo&quot;</span></span><br><span class="line">output_folder = <span class="string">&quot;/weka2/home-yeo/datasets/SDXL/duplicate_shuffle_1&quot;</span></span><br><span class="line">dataset_repeats = <span class="number">5</span></span><br><span class="line">n_tokens_to_keep = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">duplicate_and_shuffle_dataset(input_folder, output_folder, dataset_repeats, n_tokens_to_keep)</span><br></pre></td></tr></table></figure></li></ul><p>그렇게 하면 최종 데이터 세트는 아래 이미지와 비슷해집니다. 제가 사용한 설정으로는 5번의 ‘반복’이 허용되는 것 같았습니다.</p><h2 id="Returning-to-the-custom-config"><a href="#Returning-to-the-custom-config" class="headerlink" title="Returning to the custom config"></a>Returning to the custom config</h2><p>이제 사용자 정의 구성에서 이러한 특정 설정을 다루겠습니다.</p><h3 id="Learning-rate-x2F-steps"><a href="#Learning-rate-x2F-steps" class="headerlink" title="Learning rate&#x2F;steps"></a>Learning rate&#x2F;steps</h3><ul><li><p>Custom SD3.5 Large  <code>config.json</code> for LoRA training</p>  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">1.05e-3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--instance_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4 &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--metadata_update_interval&quot;</span><span class="punctuation">:</span> <span class="number">65</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--vae_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_unwanted_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_problematic_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--training_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--inference_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--snr_gamma&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--enable_xformers_memory_efficient_attention&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--allow_tf32&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--use_ema&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--ema_decay&quot;</span><span class="punctuation">:</span> <span class="number">0.999</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_alpha&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li></ul><p>이제 사용자 정의 구성에서 이러한 설정을 다루겠습니다.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">1.05e-3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="Steps-calculation"><a href="#Steps-calculation" class="headerlink" title="Steps calculation"></a>Steps calculation</h3><p>최대 훈련 단계는 간단한 수학 방정식을 기반으로 계산할 수 있습니다(<strong>단일 개념</strong>의 경우).</p><p>$$<br>\text{Max training steps} &#x3D; \left(\frac{\text{Number of samples} \times \text{Repeats}}{\text{Batch size}}\right) \times \text{Epochs}<br>$$</p><p>여기에는 네 가지 변수가 있습니다.</p><ul><li>배치 크기: 한 번의 반복으로 처리되는 샘플 수입니다.</li><li>샘플 수: 데이터 세트의 총 샘플 수입니다.</li><li>반복 횟수: 한 에포크 내에서 데이터 세트를 반복하는 횟수입니다.</li><li>Epochs: 전체 데이터세트가 처리되는 횟수입니다.</li></ul><p>‘fantasy art’ 데이터세트에는 ‘476’ 이미지가 있습니다. <code>multidatabackend.json</code>의 <code>5</code> 반복 위에 추가합니다. 나는 두 가지 이유로 <code>train_batch_size</code>를 <code>6</code>으로 선택했습니다:</p><ol><li>이 값을 사용하면 진행률 표시줄이 1~2초마다 업데이트되는 것을 볼 수 있습니다.</li><li>한 번의 반복으로 ‘6’개의 샘플을 취할 수 있을 만큼 충분히 크므로 훈련 과정에서 더 많은 일반화가 이루어지도록 합니다.</li></ol><p>30개 정도의 에포크를 원했다면 최종 계산은 다음과 같습니다.</p><p>$$<br>\text{Max training steps} &#x3D; \left(\frac{\text{476} \times \text{5}}{\text{6}}\right) \times \text{30}<br>$$</p><p>이는 대략 ‘11,900’ 단계와 같습니다.</p><p>괄호 안의 부분:</p><p>$$<br>\left(\frac{\text{476} \times \text{5}}{\text{6}}\right)<br>$$</p><p>는 에포크당 단계 수, 즉 ‘396’을 나타냅니다.</p><p>따라서 <code>CHECKPOINTING_STEPS</code>에 대해 이 값을 <code>400</code>으로 반올림했습니다.</p><p><a href="https://emojipedia.org/warning">**⚠️</a>** <code>MAX_NUM_STEPS</code>에 대해 <code>11,900</code>을 계산했지만 결국 <code>24,000</code>으로 설정했습니다. LoRA 훈련 샘플을 더 보고 싶었습니다. 따라서 원래 ‘11,900’ 이후의 모든 값은 내가 과도한 훈련을 했는지 여부에 대한 좋은 척도가 될 것입니다. 그래서 총 단계 <code>11,900</code> x <code>2</code> &#x3D; <code>23,800</code>을 두 배로 늘린 다음 반올림했습니다.</p><p><code>CHECKPOINTING_STEPS</code>는 모델 체크포인트를 저장하려는 빈도를 나타냅니다. ‘400’으로 설정하는 것은 제게는 한 시대에 꽤 가깝기 때문에 괜찮아 보였습니다.</p><p><code>CHECKPOINTING_LIMIT</code>은 이전 체크포인트를 덮어쓰기 전에 저장하려는 체크포인트 수입니다. 제 경우에는 체크포인트를 모두 유지하고 싶어서 ‘60’처럼 높은 숫자로 제한을 두었습니다.</p><h3 id="Multiple-concepts"><a href="#Multiple-concepts" class="headerlink" title="Multiple concepts"></a>Multiple concepts</h3><p>The above example is trained on a single concept with one unifying trigger word at the beginning: <code>k4s4</code>. However, if your dataset has multiple concepts&#x2F;trigger words, then your step calculation could be something like this so:</p><p><code>2</code> concepts <code>[a, b]</code></p><p>$$<br>\text{Max steps} &#x3D; \left(\frac{N_a \times R_a + N_b \times R_b}{\text{Batch size}}\right) \times \text{Epochs}<br>$$</p><p><code>i</code> concepts</p><p>$$<br>\text{Max steps} &#x3D; \left(\frac{\sum_{i \in C} N_i \times R_i}{\text{Batch size}}\right) \times \text{Epochs}<br>$$</p><p>마지막으로 학습률의 경우 ‘1.5e-3’으로 설정했습니다. 더 높을수록 기울기가 다음과 같이 폭발하기 때문입니다.</p><p>다른 관련 설정은 ‘LoRA’와 관련이 있습니다.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_alpha&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>개인적으로는 좀 더 높은 ‘LoRA’ 랭크와 알파를 사용해 아주 만족스러운 결과를 얻었습니다. 내 YouTube <a href="https://youtube.com/@kasukanra">채널</a>에서 ‘LoRA’ 순위를 높일수록 이미지 충실도가 어떻게 증가하는지에 대한 보다 정확한 경험적 분석을 보려면 최신 동영상을 시청할 수 있습니다. .</p><p>어쨌든 VRAM, 저장 용량 또는 그렇게 높아질 시간이 없다면 ‘256’ 또는 ‘128’과 같이 더 낮은 값을 선택할 수 있습니다.</p><p><code>lora_type</code>에 관해서는, 나는 시도되고 진실된 <code>standard</code>를 사용하겠습니다. ‘LoRA’의 ‘lycoris’ 유형에 대한 또 다른 옵션이 있지만 아직은 매우 실험적이며 잘 탐색되지 않았습니다. 나는 ‘lycoris’에 대해 직접 심층 분석했지만 만족스러운 결과를 얻을 수 있는 올바른 설정을 찾지 못했습니다.</p><h3 id="Custom-config-json-miscellaneous"><a href="#Custom-config-json-miscellaneous" class="headerlink" title="Custom config.json miscellaneous"></a>Custom <code>config.json</code> miscellaneous</h3><p>삶의 질을 위해 변경할 수 있는 몇 가지 추가 설정이 있습니다.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p><code>&quot;--validation_prompt&quot;: &quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</code></p><p><code>&quot;--validation_guidance&quot;: 7.5</code><br><code>&quot;--validation_steps&quot;: 200</code><br><code>&quot;--validation_num_inference_steps&quot;: 30</code><br><code>&quot;--validation_negative_prompt&quot;: &quot;blurry, cropped, ugly&quot;</code></p><p><code>&quot;--lr_scheduler&quot;: &quot;cosine&quot;</code></p><p><code>&quot;--lr_warmup_steps&quot;: 2400</code></p><p>이것들은 매우 자명합니다:</p><p><code>&quot;--validation_prompt&quot;</code></p><p>검증 이미지를 생성하는 데 사용할 프롬프트입니다. 이것이 당신의 긍정적인 메시지입니다.</p><p><code>&quot;--validation_negative_prompt&quot;</code></p><p>부정적인 프롬프트.</p><p><code>&quot;--validation_guidance&quot;</code></p><p>Classifier free guidance (CFG) scale.</p><p><code>&quot;--validation_num_inference_steps&quot;</code></p><p>사용할 샘플링 단계 수입니다.</p><p><code>&quot;--validation_seed&quot;</code></p><p>검증 이미지 생성 시 시드 값입니다.</p><p><code>&quot;--lr_warmup_steps&quot;</code></p><p>‘SimpleTuner’는 설정하지 않을 경우 기본 워밍업을 전체 훈련 단계의 ‘10%’로 설정했는데, 이는 제가 자주 사용하는 값입니다. 그래서 (<code>24,000</code> * <code>0.1</code> &#x3D; <code>2,400</code>)에 하드코딩했습니다. 자유롭게 변경해 보세요.</p><p><code>&quot;--validation_steps&quot;</code></p><p>검증 이미지를 생성하려는 빈도는 <code>&quot;--validation_steps&quot;</code>로 설정됩니다. 저는 400의 1&#x2F;2인 200으로 설정했습니다(판타지 아트 예제 데이터세트에 대한 한 시대의 단계 수). 이는 에포크의 1&#x2F;2마다 검증 이미지를 생성한다는 의미입니다. 온전한 확인을 위해 최소한 반기점마다 검증 이미지를 생성하는 것이 좋습니다. 그렇지 않으면 최대한 빨리 오류를 포착하지 못할 수도 있습니다.</p><p>마지막으로 <code>&quot;--lr_scheduler&quot;</code>와 <code>&quot;--lr_warmup_steps&quot;</code>입니다.</p><p>저는 ‘코사인’ 스케줄러를 사용했습니다. 다음과 같은 모습입니다.</p><h3 id="What-happened-to-the-low-level-config-env"><a href="#What-happened-to-the-low-level-config-env" class="headerlink" title="What happened to the low-level config.env ?"></a>What happened to the low-level <code>config.env</code> ?</h3><p>앞서 언급했듯이 <code>SimpleTuner</code>는 낮은 수준의 <code>config.env</code> 형식에서 벗어나 사용 편의성을 위해 <code>json</code>을 선택하는 것으로 보입니다. 대부분의 다른 교육 리포지토리도 <code>json</code>을 사용합니다.</p><p>그러나 <a href="https://github.com/bghira/SimpleTuner/blob/main/helpers/configuration/loader.py#L17">loader.py</a>의 코드를 기반으로 하위 수준 <code>config.env</code>는 계속 지원됩니다. . 또한 이전의 낮은 수준 <code>config.env</code> 파일이 이미 있는 <code>SimpleTuner</code>의 이전 사용자는 파일 형식을 전환하지 않고도 일부 매개변수를 조정하여 신속하게 속도를 얻을 수 있습니다(해당 [OPTIONS.MD](https &#x2F;&#x2F;github.com&#x2F;bghira&#x2F;SimpleTuner&#x2F;blob&#x2F;main&#x2F;OPTIONS.md#environment-configuration-variables)).</p><h2 id="이는-위의-config-json과-동일한-버전이지만-env-형식입니다"><a href="#이는-위의-config-json과-동일한-버전이지만-env-형식입니다" class="headerlink" title="이는 위의 config.json과 동일한 버전이지만 .env 형식입니다."></a>이는 위의 <code>config.json</code>과 동일한 버전이지만 <code>.env</code> 형식입니다.</h2><ul><li><p>Custom SD3.5 Large <code>LoRA</code> <code>config.env</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> MODEL_TYPE=<span class="string">&#x27;lora&#x27;</span></span><br><span class="line"><span class="built_in">export</span> MODEL_FAMILY=<span class="string">&#x27;sd3&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONTROLNET=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> USE_DORA=<span class="literal">false</span></span><br><span class="line"><span class="comment"># Restart where we left off. Change this to &quot;checkpoint-1234&quot; to start from a specific checkpoint.</span></span><br><span class="line"><span class="built_in">export</span> RESUME_CHECKPOINT=<span class="string">&quot;latest&quot;</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_STEPS=400</span><br><span class="line"><span class="comment"># This is how many checkpoints we will keep. Two is safe, but three is safer.</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_LIMIT=60</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is decided as a relatively conservative &#x27;constant&#x27; learning rate.</span></span><br><span class="line"><span class="comment"># Adjust higher or lower depending on how burnt your model becomes.</span></span><br><span class="line"><span class="built_in">export</span> LEARNING_RATE=1.05e-3</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using a Huggingface Hub model:</span></span><br><span class="line"><span class="built_in">export</span> MODEL_NAME=<span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make DEBUG_EXTRA_ARGS empty to disable wandb.</span></span><br><span class="line"><span class="built_in">export</span> DEBUG_EXTRA_ARGS=<span class="string">&quot;--report_to=wandb&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_PROJECT_NAME=<span class="string">&quot;sd35-training&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_RUN_NAME=<span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Max number of steps OR epochs can be used. Not both.</span></span><br><span class="line"><span class="built_in">export</span> MAX_NUM_STEPS=24000</span><br><span class="line"><span class="built_in">export</span> NUM_EPOCHS=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># A convenient prefix for all of your training paths.</span></span><br><span class="line"><span class="built_in">export</span> DATALOADER_CONFIG=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span></span><br><span class="line"><span class="built_in">export</span> OUTPUT_DIR=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span></span><br><span class="line"><span class="comment"># Set this to &quot;true&quot; to push your model to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_TO_HUB=<span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="comment"># If PUSH_TO_HUB and PUSH_CHECKPOINTS are both enabled, every saved checkpoint will be pushed to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_CHECKPOINTS=<span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="comment"># This will be the model name for your final hub upload, eg. &quot;yourusername/yourmodelname&quot;</span></span><br><span class="line"><span class="comment"># It defaults to the wandb project name, but you can override this here.</span></span><br><span class="line"><span class="comment"># export HUB_MODEL_NAME=$TRACKER_PROJECT_NAME</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, images will be resized so their SMALLER EDGE is 1024 pixels, maintaining aspect ratio.</span></span><br><span class="line"><span class="comment"># Setting this value to 768px might result in more reasonable training data sizes for SDXL.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION=1024</span><br><span class="line"><span class="comment"># If you want to have the training data resized by pixel area (Megapixels) rather than edge length,</span></span><br><span class="line"><span class="comment">#  set this value to &quot;area&quot; instead of &quot;pixel&quot;, and uncomment the next RESOLUTION declaration.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION_TYPE=<span class="string">&quot;pixel&quot;</span></span><br><span class="line"><span class="comment">#export RESOLUTION=1          # 1.0 Megapixel training sizes</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;pixel&quot;, the minimum resolution specifies the smaller edge length, measured in pixels. Recommended: 1024.</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;area&quot;, the minimum resolution specifies the total image area, measured in megapixels. Recommended: 1.</span></span><br><span class="line"><span class="built_in">export</span> MINIMUM_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># How many decimals to round aspect buckets to.</span></span><br><span class="line"><span class="comment">#export ASPECT_BUCKET_ROUNDING=2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use this to append an instance prompt to each caption, used for adding trigger words.</span></span><br><span class="line"><span class="comment"># This has not been tested in SDXL.</span></span><br><span class="line"><span class="built_in">export</span> INSTANCE_PROMPT=<span class="string">&quot;k4s4 &quot;</span></span><br><span class="line"><span class="comment"># If you also supply a user prompt library or `--use_prompt_library`, this will be added to those lists.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_PROMPT=<span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE=7.5</span><br><span class="line"><span class="comment"># You&#x27;ll want to set this to 0.7 if you are training a terminal SNR model.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE_RESCALE=0.0</span><br><span class="line"><span class="comment"># How frequently we will save and run a pipeline for validations.</span></span><br><span class="line"><span class="comment"># export VALIDATION_STEPS=200</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_STEPS=70</span><br><span class="line"><span class="built_in">export</span> VALIDATION_NUM_INFERENCE_STEPS=30</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> VALIDATION_NEGATIVE_PROMPT=<span class="string">&quot;blurry, cropped, ugly&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_SEED=42</span><br><span class="line"><span class="built_in">export</span> VALIDATION_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adjust this for your GPU memory size. This, and resolution, are the biggest VRAM killers.</span></span><br><span class="line"><span class="built_in">export</span> TRAIN_BATCH_SIZE=6</span><br><span class="line"><span class="comment"># Accumulate your update gradient over many steps, to save VRAM while still having higher effective batch size:</span></span><br><span class="line"><span class="comment"># effective batch size = ($TRAIN_BATCH_SIZE * $GRADIENT_ACCUMULATION_STEPS).</span></span><br><span class="line"><span class="built_in">export</span> GRADIENT_ACCUMULATION_STEPS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use any standard scheduler type. constant, polynomial, constant_with_warmup</span></span><br><span class="line"><span class="built_in">export</span> LR_SCHEDULE=<span class="string">&quot;cosine&quot;</span></span><br><span class="line"><span class="comment"># A warmup period allows the model and the EMA weights more importantly to familiarise itself with the current quanta.</span></span><br><span class="line"><span class="comment"># For the cosine or sine type schedules, the warmup period defines the interval between peaks or valleys.</span></span><br><span class="line"><span class="comment"># Use a sine schedule to simulate a warmup period, or a Cosine period to simulate a polynomial start.</span></span><br><span class="line"><span class="comment"># export LR_WARMUP_STEPS=$((MAX_NUM_STEPS / 10))</span></span><br><span class="line"><span class="built_in">export</span> LR_WARMUP_STEPS=2400</span><br><span class="line"></span><br><span class="line"><span class="comment"># Caption dropout probability. Set to 0.1 for 10% of captions dropped out. Set to 0 to disable.</span></span><br><span class="line"><span class="comment"># You may wish to disable dropout if you want to limit your changes strictly to the prompts you show the model.</span></span><br><span class="line"><span class="comment"># You may wish to increase the rate of dropout if you want to more broadly adopt your changes across the model.</span></span><br><span class="line"><span class="built_in">export</span> CAPTION_DROPOUT_PROBABILITY=0</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> METADATA_UPDATE_INTERVAL=65</span><br><span class="line"><span class="built_in">export</span> VAE_BATCH_SIZE=12</span><br><span class="line"></span><br><span class="line"><span class="comment"># If this is set, any images that fail to open will be DELETED to avoid re-checking them every time.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_ERRORED_IMAGES=0</span><br><span class="line"><span class="comment"># If this is set, any images that are too small for the minimum resolution size will be DELETED.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_SMALL_IMAGES=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bytedance recommends these be set to &quot;trailing&quot; so that inference and training behave in a more congruent manner.</span></span><br><span class="line"><span class="comment"># To follow the original SDXL training strategy, use &quot;leading&quot; instead, though results are generally worse.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"><span class="built_in">export</span> INFERENCE_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing this option or unsetting it uses vanilla training. Setting it reweights the loss by the position of the timestep in the noise schedule.</span></span><br><span class="line"><span class="comment"># A value &quot;5&quot; is recommended by the researchers. A value of &quot;20&quot; is the least impact, and &quot;1&quot; is the most impact.</span></span><br><span class="line"><span class="built_in">export</span> MIN_SNR_GAMMA=5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set this to an explicit value of &quot;false&quot; to disable Xformers. Probably required for AMD users.</span></span><br><span class="line"><span class="built_in">export</span> USE_XFORMERS=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># There&#x27;s basically no reason to unset this. However, to disable it, use an explicit value of &quot;false&quot;.</span></span><br><span class="line"><span class="comment"># This will save a lot of memory consumption when enabled.</span></span><br><span class="line"><span class="built_in">export</span> USE_GRADIENT_CHECKPOINTING=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Options below here may require a bit more complicated configuration, so they are not simple variables.</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TF32 is great on Ampere or Ada, not sure about earlier generations.</span></span><br><span class="line"><span class="built_in">export</span> ALLOW_TF32=<span class="literal">true</span></span><br><span class="line"><span class="comment"># AdamW 8Bit is a robust and lightweight choice. Adafactor might reduce memory consumption, and Dadaptation is slow and experimental.</span></span><br><span class="line"><span class="comment"># AdamW is the default optimizer, but it uses a lot of memory and is slower than AdamW8Bit or Adafactor.</span></span><br><span class="line"><span class="comment"># Choices: adamw, adamw8bit, adafactor, dadaptation</span></span><br><span class="line"><span class="comment"># export OPTIMIZER=&quot;adamw_bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> OPTIMIZER=<span class="string">&quot;adamw_bf16&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># EMA is a strong regularisation method that uses a lot of extra VRAM to hold two copies of the weights.</span></span><br><span class="line"><span class="comment"># This is worthwhile on large training runs, but not so much for smaller training runs.</span></span><br><span class="line"><span class="built_in">export</span> USE_EMA=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> EMA_DECAY=0.999</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TRAINER_EXTRA_ARGS=<span class="string">&quot;--lora_rank=768 --lora_alpha=768&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reproducible training. Set to -1 to disable.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SEED=42</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mixed precision is the best. You honestly might need to YOLO it in fp16 mode for Google Colab type setups.</span></span><br><span class="line"><span class="built_in">export</span> MIXED_PRECISION=<span class="string">&quot;bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> PURE_BF16=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This has to be changed if you&#x27;re training with multiple GPUs.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_PROCESSES=1</span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_MACHINES=1</span><br><span class="line"><span class="built_in">export</span> ACCELERATE_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># With Pytorch 2.1, you might have pretty good luck here.</span></span><br><span class="line"><span class="comment"># If you&#x27;re using aspect bucketing however, each resolution change will recompile. Seriously, just don&#x27;t do it.</span></span><br><span class="line"><span class="comment"># Well, then again... Pytorch 2.2 has support for dynamic shapes. Why not?</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TOKENIZERS_PARALLELISM=<span class="literal">false</span></span><br></pre></td></tr></table></figure></li></ul><p><a href="https://emojipedia.org/index-pointing-up">**☝️</a>** <code>LoRA</code> 순위&#x2F;알파는 <code>TRAINER_EXTRA_ARGS</code> 변수 내에서 변경될 수 있다는 점을 지적하고 싶습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> TRAINER_EXTRA_ARGS=<span class="string">&quot;--lora_rank=768 --lora_alpha=768&quot;</span></span><br></pre></td></tr></table></figure><p><a href="https://emojipedia.org/warning">**⚠️</a>** <code>.env</code> 형식을 사용하기로 결정한 경우 인라인 주석, 참조 변수 또는 계산이 없는지 확인하세요. 이것은 새로운 <code>SimpleTuner</code> <a href="https://github.com/bghira/SimpleTuner/blob/main/helpers/configuration/env_file.py#L94">env 도우미</a>가 작동하는 방식이므로 모든 것을 하드 코딩해야 합니다. . ****예를 들어:</p><p><strong>Failure case 1 (inline comments):</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LEARNING_RATE=1.05e-3 <span class="comment">#@param &#123;type:&quot;number&quot;&#125;</span></span><br></pre></td></tr></table></figure><p><strong>Failure case 2 (reference variable with <code>TRAINER_EXTRA_ARGS</code>):</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> TRAINER_EXTRA_ARGS=<span class="string">&quot;<span class="variable">$&#123;TRAINER_EXTRA_ARGS&#125;</span> --offset_noise --noise_offset=0.02&quot;</span></span><br></pre></td></tr></table></figure><p><strong>Failure case 3 (calculations)</strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LR_WARMUP_STEPS=$((MAX_NUM_STEPS / <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>원하는 경우 위의 하위 수준 <code>config.env</code>를 기본 참조로 사용할 수 있습니다. 하위 수준 <code>env</code> 파일을 사용하기로 결정한 경우 상위 수준 <code>config.env</code>에서 <code>CONFIG_BACKEND</code>를 <code>env</code>로 변경하는 것을 잊지 마세요.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_NUM_PROCESSES=1</span><br><span class="line">TRAINING_NUM_MACHINES=1</span><br><span class="line">TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line">MIXED_PRECISION=<span class="string">&#x27;bf16&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line"><span class="built_in">export</span> ENV=<span class="string">&quot;sd35_fantasy_art_lora&quot;</span></span><br></pre></td></tr></table></figure><h2 id="Training-process"><a href="#Training-process" class="headerlink" title="Training process"></a>Training process</h2><p>마지막으로 훈련 과정을 시작할 수 있습니다. 참고용으로 필요한 모든 파일을 여기에 가져오겠습니다.</p><ul><li><p>High-level <code>config.env</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_NUM_PROCESSES=1</span><br><span class="line">TRAINING_NUM_MACHINES=1</span><br><span class="line">TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line">MIXED_PRECISION=<span class="string">&#x27;bf16&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line"><span class="built_in">export</span> ENV=<span class="string">&quot;sd35_fantasy_art_lora&quot;</span></span><br></pre></td></tr></table></figure></li><li><p>Custom SD3.5 Large <code>config.json</code> for LoRA training</p>  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">60</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">1.05e-3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--instance_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4 &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--metadata_update_interval&quot;</span><span class="punctuation">:</span> <span class="number">65</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--vae_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_unwanted_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_problematic_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--training_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--inference_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--snr_gamma&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--enable_xformers_memory_efficient_attention&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--allow_tf32&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--use_ema&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--ema_decay&quot;</span><span class="punctuation">:</span> <span class="number">0.999</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_alpha&quot;</span><span class="punctuation">:</span> <span class="number">768</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lora_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li><li><p>Custom SD3.5 Large  <code>config.env</code> for LoRA training</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="built_in">export</span> MODEL_TYPE=<span class="string">&#x27;lora&#x27;</span></span><br><span class="line"><span class="built_in">export</span> MODEL_FAMILY=<span class="string">&#x27;sd3&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONTROLNET=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> USE_DORA=<span class="literal">false</span></span><br><span class="line"><span class="comment"># Restart where we left off. Change this to &quot;checkpoint-1234&quot; to start from a specific checkpoint.</span></span><br><span class="line"><span class="built_in">export</span> RESUME_CHECKPOINT=<span class="string">&quot;latest&quot;</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_STEPS=400</span><br><span class="line"><span class="comment"># This is how many checkpoints we will keep. Two is safe, but three is safer.</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_LIMIT=60</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is decided as a relatively conservative &#x27;constant&#x27; learning rate.</span></span><br><span class="line"><span class="comment"># Adjust higher or lower depending on how burnt your model becomes.</span></span><br><span class="line"><span class="built_in">export</span> LEARNING_RATE=1.05e-3</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using a Huggingface Hub model:</span></span><br><span class="line"><span class="built_in">export</span> MODEL_NAME=<span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make DEBUG_EXTRA_ARGS empty to disable wandb.</span></span><br><span class="line"><span class="built_in">export</span> DEBUG_EXTRA_ARGS=<span class="string">&quot;--report_to=wandb&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_PROJECT_NAME=<span class="string">&quot;sd35-training&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_RUN_NAME=<span class="string">&quot;simpletuner-fantasy-art-lora-01&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Max number of steps OR epochs can be used. Not both.</span></span><br><span class="line"><span class="built_in">export</span> MAX_NUM_STEPS=24000</span><br><span class="line"><span class="built_in">export</span> NUM_EPOCHS=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># A convenient prefix for all of your training paths.</span></span><br><span class="line"><span class="built_in">export</span> DATALOADER_CONFIG=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span></span><br><span class="line"><span class="built_in">export</span> OUTPUT_DIR=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span></span><br><span class="line"><span class="comment"># Set this to &quot;true&quot; to push your model to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_TO_HUB=<span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="comment"># If PUSH_TO_HUB and PUSH_CHECKPOINTS are both enabled, every saved checkpoint will be pushed to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_CHECKPOINTS=<span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="comment"># This will be the model name for your final hub upload, eg. &quot;yourusername/yourmodelname&quot;</span></span><br><span class="line"><span class="comment"># It defaults to the wandb project name, but you can override this here.</span></span><br><span class="line"><span class="comment"># export HUB_MODEL_NAME=$TRACKER_PROJECT_NAME</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, images will be resized so their SMALLER EDGE is 1024 pixels, maintaining aspect ratio.</span></span><br><span class="line"><span class="comment"># Setting this value to 768px might result in more reasonable training data sizes for SDXL.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION=1024</span><br><span class="line"><span class="comment"># If you want to have the training data resized by pixel area (Megapixels) rather than edge length,</span></span><br><span class="line"><span class="comment">#  set this value to &quot;area&quot; instead of &quot;pixel&quot;, and uncomment the next RESOLUTION declaration.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION_TYPE=<span class="string">&quot;pixel&quot;</span></span><br><span class="line"><span class="comment">#export RESOLUTION=1          # 1.0 Megapixel training sizes</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;pixel&quot;, the minimum resolution specifies the smaller edge length, measured in pixels. Recommended: 1024.</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;area&quot;, the minimum resolution specifies the total image area, measured in megapixels. Recommended: 1.</span></span><br><span class="line"><span class="built_in">export</span> MINIMUM_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># How many decimals to round aspect buckets to.</span></span><br><span class="line"><span class="comment">#export ASPECT_BUCKET_ROUNDING=2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use this to append an instance prompt to each caption, used for adding trigger words.</span></span><br><span class="line"><span class="comment"># This has not been tested in SDXL.</span></span><br><span class="line"><span class="built_in">export</span> INSTANCE_PROMPT=<span class="string">&quot;k4s4 &quot;</span></span><br><span class="line"><span class="comment"># If you also supply a user prompt library or `--use_prompt_library`, this will be added to those lists.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_PROMPT=<span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE=7.5</span><br><span class="line"><span class="comment"># You&#x27;ll want to set this to 0.7 if you are training a terminal SNR model.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE_RESCALE=0.0</span><br><span class="line"><span class="comment"># How frequently we will save and run a pipeline for validations.</span></span><br><span class="line"><span class="comment"># export VALIDATION_STEPS=200</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_STEPS=70</span><br><span class="line"><span class="built_in">export</span> VALIDATION_NUM_INFERENCE_STEPS=30</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> VALIDATION_NEGATIVE_PROMPT=<span class="string">&quot;blurry, cropped, ugly&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_SEED=42</span><br><span class="line"><span class="built_in">export</span> VALIDATION_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adjust this for your GPU memory size. This, and resolution, are the biggest VRAM killers.</span></span><br><span class="line"><span class="built_in">export</span> TRAIN_BATCH_SIZE=6</span><br><span class="line"><span class="comment"># Accumulate your update gradient over many steps, to save VRAM while still having higher effective batch size:</span></span><br><span class="line"><span class="comment"># effective batch size = ($TRAIN_BATCH_SIZE * $GRADIENT_ACCUMULATION_STEPS).</span></span><br><span class="line"><span class="built_in">export</span> GRADIENT_ACCUMULATION_STEPS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use any standard scheduler type. constant, polynomial, constant_with_warmup</span></span><br><span class="line"><span class="built_in">export</span> LR_SCHEDULE=<span class="string">&quot;cosine&quot;</span></span><br><span class="line"><span class="comment"># A warmup period allows the model and the EMA weights more importantly to familiarise itself with the current quanta.</span></span><br><span class="line"><span class="comment"># For the cosine or sine type schedules, the warmup period defines the interval between peaks or valleys.</span></span><br><span class="line"><span class="comment"># Use a sine schedule to simulate a warmup period, or a Cosine period to simulate a polynomial start.</span></span><br><span class="line"><span class="comment"># export LR_WARMUP_STEPS=$((MAX_NUM_STEPS / 10))</span></span><br><span class="line"><span class="built_in">export</span> LR_WARMUP_STEPS=2400</span><br><span class="line"></span><br><span class="line"><span class="comment"># Caption dropout probability. Set to 0.1 for 10% of captions dropped out. Set to 0 to disable.</span></span><br><span class="line"><span class="comment"># You may wish to disable dropout if you want to limit your changes strictly to the prompts you show the model.</span></span><br><span class="line"><span class="comment"># You may wish to increase the rate of dropout if you want to more broadly adopt your changes across the model.</span></span><br><span class="line"><span class="built_in">export</span> CAPTION_DROPOUT_PROBABILITY=0</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> METADATA_UPDATE_INTERVAL=65</span><br><span class="line"><span class="built_in">export</span> VAE_BATCH_SIZE=12</span><br><span class="line"></span><br><span class="line"><span class="comment"># If this is set, any images that fail to open will be DELETED to avoid re-checking them every time.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_ERRORED_IMAGES=0</span><br><span class="line"><span class="comment"># If this is set, any images that are too small for the minimum resolution size will be DELETED.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_SMALL_IMAGES=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bytedance recommends these be set to &quot;trailing&quot; so that inference and training behave in a more congruent manner.</span></span><br><span class="line"><span class="comment"># To follow the original SDXL training strategy, use &quot;leading&quot; instead, though results are generally worse.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"><span class="built_in">export</span> INFERENCE_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing this option or unsetting it uses vanilla training. Setting it reweights the loss by the position of the timestep in the noise schedule.</span></span><br><span class="line"><span class="comment"># A value &quot;5&quot; is recommended by the researchers. A value of &quot;20&quot; is the least impact, and &quot;1&quot; is the most impact.</span></span><br><span class="line"><span class="built_in">export</span> MIN_SNR_GAMMA=5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set this to an explicit value of &quot;false&quot; to disable Xformers. Probably required for AMD users.</span></span><br><span class="line"><span class="built_in">export</span> USE_XFORMERS=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># There&#x27;s basically no reason to unset this. However, to disable it, use an explicit value of &quot;false&quot;.</span></span><br><span class="line"><span class="comment"># This will save a lot of memory consumption when enabled.</span></span><br><span class="line"><span class="built_in">export</span> USE_GRADIENT_CHECKPOINTING=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Options below here may require a bit more complicated configuration, so they are not simple variables.</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TF32 is great on Ampere or Ada, not sure about earlier generations.</span></span><br><span class="line"><span class="built_in">export</span> ALLOW_TF32=<span class="literal">true</span></span><br><span class="line"><span class="comment"># AdamW 8Bit is a robust and lightweight choice. Adafactor might reduce memory consumption, and Dadaptation is slow and experimental.</span></span><br><span class="line"><span class="comment"># AdamW is the default optimizer, but it uses a lot of memory and is slower than AdamW8Bit or Adafactor.</span></span><br><span class="line"><span class="comment"># Choices: adamw, adamw8bit, adafactor, dadaptation</span></span><br><span class="line"><span class="comment"># export OPTIMIZER=&quot;adamw_bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> OPTIMIZER=<span class="string">&quot;adamw_bf16&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># EMA is a strong regularisation method that uses a lot of extra VRAM to hold two copies of the weights.</span></span><br><span class="line"><span class="comment"># This is worthwhile on large training runs, but not so much for smaller training runs.</span></span><br><span class="line"><span class="built_in">export</span> USE_EMA=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> EMA_DECAY=0.999</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TRAINER_EXTRA_ARGS=<span class="string">&quot;--lora_rank=768 --lora_alpha=768&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reproducible training. Set to -1 to disable.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SEED=42</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mixed precision is the best. You honestly might need to YOLO it in fp16 mode for Google Colab type setups.</span></span><br><span class="line"><span class="built_in">export</span> MIXED_PRECISION=<span class="string">&quot;bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> PURE_BF16=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This has to be changed if you&#x27;re training with multiple GPUs.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_PROCESSES=1</span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_MACHINES=1</span><br><span class="line"><span class="built_in">export</span> ACCELERATE_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># With Pytorch 2.1, you might have pretty good luck here.</span></span><br><span class="line"><span class="comment"># If you&#x27;re using aspect bucketing however, each resolution change will recompile. Seriously, just don&#x27;t do it.</span></span><br><span class="line"><span class="comment"># Well, then again... Pytorch 2.2 has support for dynamic shapes. Why not?</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TOKENIZERS_PARALLELISM=<span class="literal">false</span></span><br></pre></td></tr></table></figure></li><li><p>Default <a href="http://train.sh/">train.sh</a></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pull config from config.env</span></span><br><span class="line">[ -f <span class="string">&quot;config/config.env&quot;</span> ] &amp;&amp; <span class="built_in">source</span> config/config.env</span><br><span class="line"></span><br><span class="line"><span class="comment"># If the user has not provided VENV_PATH, we will assume $(pwd)/.venv</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;VENV_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="comment"># what if we have VIRTUAL_ENV? use that instead</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;VIRTUAL_ENV&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> VENV_PATH=<span class="string">&quot;<span class="variable">$&#123;VIRTUAL_ENV&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">export</span> VENV_PATH=<span class="string">&quot;<span class="subst">$(pwd)</span>/.venv&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;DISABLE_LD_OVERRIDE&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> NVJITLINK_PATH=<span class="string">&quot;<span class="subst">$(find <span class="string">&quot;<span class="variable">$&#123;VENV_PATH&#125;</span>&quot;</span> -name nvjitlink -type d)</span>/lib&quot;</span></span><br><span class="line">    <span class="comment"># if it&#x27;s not empty, we will add it to LD_LIBRARY_PATH at the front:</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;NVJITLINK_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">&quot;<span class="variable">$&#123;NVJITLINK_PATH&#125;</span>:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TOKENIZERS_PARALLELISM=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> PLATFORM</span><br><span class="line">PLATFORM=$(<span class="built_in">uname</span> -s)</span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$PLATFORM</span>&quot;</span> == <span class="string">&quot;Darwin&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> MIXED_PRECISION=<span class="string">&quot;no&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_EXTRA_ARGS&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_PROCESSES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Set custom env vars permanently in config/config.env:&quot;</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_NUM_PROCESSES not set, defaulting to 1.\n&quot;</span></span><br><span class="line">    TRAINING_NUM_PROCESSES=1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_MACHINES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_NUM_MACHINES not set, defaulting to 1.\n&quot;</span></span><br><span class="line">    TRAINING_NUM_MACHINES=1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;MIXED_PRECISION&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;MIXED_PRECISION not set, defaulting to bf16.\n&quot;</span></span><br><span class="line">    MIXED_PRECISION=bf16</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;TRAINING_DYNAMO_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;TRAINING_DYNAMO_BACKEND not set, defaulting to no.\n&quot;</span></span><br><span class="line">    TRAINING_DYNAMO_BACKEND=<span class="string">&quot;no&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;ENV&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;ENV not set, defaulting to default.\n&quot;</span></span><br><span class="line">    <span class="built_in">export</span> ENV=<span class="string">&quot;default&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">export</span> ENV_PATH=<span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$ENV</span>&quot;</span> != <span class="string">&quot;default&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> ENV_PATH=<span class="string">&quot;<span class="variable">$&#123;ENV&#125;</span>/&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;CONFIG_TYPE&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;<span class="variable">$&#123;CONFIG_TYPE&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="built_in">export</span> CONFIG_PATH=<span class="string">&quot;config/<span class="variable">$&#123;ENV_PATH&#125;</span>config&quot;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.json&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;json&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.toml&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;toml&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> [ -f <span class="string">&quot;<span class="variable">$&#123;CONFIG_PATH&#125;</span>.env&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">export</span> CONFIG_BACKEND=<span class="string">&quot;env&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Using <span class="variable">$&#123;CONFIG_BACKEND&#125;</span> backend: <span class="variable">$&#123;CONFIG_PATH&#125;</span>.<span class="variable">$&#123;CONFIG_BACKEND&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Update dependencies</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;DISABLE_UPDATES&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;Updating dependencies. Set DISABLE_UPDATES to prevent this.&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;pyproject.toml&quot;</span> ] &amp;&amp; [ -f <span class="string">&quot;poetry.lock&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        nvidia-smi 2&gt; /dev/null &amp;&amp; poetry install</span><br><span class="line">        <span class="built_in">uname</span> -s | grep -q Darwin &amp;&amp; poetry install -C install/apple</span><br><span class="line">        rocm-smi 2&gt; /dev/null &amp;&amp; poetry install -C install/rocm</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment"># Run the training script.</span></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_CONFIG_PATH=<span class="string">&quot;<span class="variable">$&#123;HOME&#125;</span>/.cache/huggingface/accelerate/default_config.yaml&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Using Accelerate config file: <span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span></span><br><span class="line">    accelerate launch --config_file=<span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> train.py</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Accelerate config file not found: <span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>. Using values from config.env.&quot;</span></span><br><span class="line">    accelerate launch <span class="variable">$&#123;ACCELERATE_EXTRA_ARGS&#125;</span> --mixed_precision=<span class="string">&quot;<span class="variable">$&#123;MIXED_PRECISION&#125;</span>&quot;</span> --num_processes=<span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_PROCESSES&#125;</span>&quot;</span> --num_machines=<span class="string">&quot;<span class="variable">$&#123;TRAINING_NUM_MACHINES&#125;</span>&quot;</span> --dynamo_backend=<span class="string">&quot;<span class="variable">$&#123;TRAINING_DYNAMO_BACKEND&#125;</span>&quot;</span> train.py</span><br><span class="line"></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure></li></ul><h3 id="Possible-accelerate-issues"><a href="#Possible-accelerate-issues" class="headerlink" title="Possible accelerate issues"></a>Possible <code>accelerate</code> issues</h3><p>여기서는 훈련을 시작하는 데 방해가 될 수 있는 한 가지 작은 사항을 언급하고 싶습니다. 끝 부분에 있는 기본 <code>train.sh</code> 안에는 훈련을 실행하는 명령이 있습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch --config_file=<span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> train.py</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run the training script.</span></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">&quot;$&#123;ACCELERATE_CONFIG_PATH&#125;&quot;</span> ]]; then</span><br><span class="line">    ACCELERATE_CONFIG_PATH=<span class="string">&quot;$&#123;HOME&#125;/.cache/huggingface/accelerate/default_config.yaml&quot;</span></span><br><span class="line">fi</span><br><span class="line"><span class="keyword">if</span> [ -f <span class="string">&quot;$&#123;ACCELERATE_CONFIG_PATH&#125;&quot;</span> ]; then</span><br><span class="line">    echo <span class="string">&quot;Using Accelerate config file: $&#123;ACCELERATE_CONFIG_PATH&#125;&quot;</span></span><br><span class="line">    accelerate launch --config_file=<span class="string">&quot;$&#123;ACCELERATE_CONFIG_PATH&#125;&quot;</span> train.py</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    echo <span class="string">&quot;Accelerate config file not found: $&#123;ACCELERATE_CONFIG_PATH&#125;. Using values from config.env.&quot;</span></span><br><span class="line">    accelerate launch $&#123;ACCELERATE_EXTRA_ARGS&#125; --mixed_precision=<span class="string">&quot;$&#123;MIXED_PRECISION&#125;&quot;</span> --num_processes=<span class="string">&quot;$&#123;TRAINING_NUM_PROCESSES&#125;&quot;</span> --num_machines=<span class="string">&quot;$&#123;TRAINING_NUM_MACHINES&#125;&quot;</span> --dynamo_backend=<span class="string">&quot;$&#123;TRAINING_DYNAMO_BACKEND&#125;&quot;</span> train.py</span><br><span class="line"></span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>이것이 처음으로 훈련 저장소를 설치하는 것이라면 아마도 오류 없이 실행될 것입니다. 그러나 다른 저장소에서 <code>accelerate</code>를 사용한 경우 <code>default_config.yaml</code>을 이미 구성했을 가능성이 높습니다. 훈련에서 오류가 발생하는 경우 일반 훈련을 위해 여기에 자체 <code>config.yaml</code>을 제공했습니다. 또한 ‘LoRA’ 교육이 아닌 완전한 미세 조정을 시도하려는 경우 ‘DeepSpeed’ ‘config.yaml’을 제공했습니다.</p><p>‘DeepSpeed’는 GPU VRAM이 충분하지 않을 때 내부의 특수 기술을 사용하여 최적화 상태, 그라데이션 및 기타 매개변수를 CPU 메모리(RAM)로 오프로드합니다. ‘80GB’ VRAM과 ‘128GB’ CPU RAM을 갖춘 단일 ‘H100’ GPU에서는 ‘SD3.5 Large’로 완전한 미세 조정을 수행할 수 있었습니다. VRAM이 부족하고 CPU RAM으로 오프로드해야 할 때마다 이 <code>config.yaml</code>을 사용할 수도 있습니다.</p><ul><li><p>Custom general use <code>base_config.yaml</code></p>  <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">debug:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">&#x27;NO&#x27;</span></span><br><span class="line"><span class="attr">downcast_bf16:</span> <span class="string">&#x27;no&#x27;</span></span><br><span class="line"><span class="attr">enable_cpu_affinity:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">gpu_ids:</span> <span class="string">all</span></span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">bf16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">rdzv_backend:</span> <span class="string">static</span></span><br><span class="line"><span class="attr">same_network:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">tpu_env:</span> []</span><br><span class="line"><span class="attr">tpu_use_cluster:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">tpu_use_sudo:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></li><li><p>Custom <code>DeepSpeed 2</code> <code>deepspeed_config.yaml</code></p>  <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">debug:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">gradient_accumulation_steps:</span> <span class="number">8</span></span><br><span class="line">  <span class="attr">gradient_clipping:</span> <span class="number">1.0</span></span><br><span class="line">  <span class="attr">offload_optimizer_device:</span> <span class="string">cpu</span></span><br><span class="line">  <span class="attr">offload_param_device:</span> <span class="string">cpu</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">zero_stage:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">downcast_bf16:</span> <span class="string">&#x27;no&#x27;</span></span><br><span class="line"><span class="attr">enable_cpu_affinity:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">bf16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">rdzv_backend:</span> <span class="string">static</span></span><br><span class="line"><span class="attr">same_network:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">tpu_env:</span> []</span><br><span class="line"><span class="attr">tpu_use_cluster:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">tpu_use_sudo:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></li></ul><p>이러한 <code>yaml</code> 파일을 배치할 위치를 선택할 때마다 <code>train.sh</code> 코드에서 해당 파일을 올바르게 참조해야 합니다. 예를 들어, 저는 <code>SimpleTuner</code> 디렉토리의 루트에 파일을 배치합니다. 따라서 코드의 ‘ACCELERATE_CONFIG_PATH’ 부분이 그에 따라 수정됩니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run the training script with base config.</span></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_CONFIG_PATH=<span class="string">&quot;<span class="variable">$&#123;HOME&#125;</span>/SimpleTuner/base_config.yaml&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>or</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run the training script with DeepSpeed config.</span></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">&quot;<span class="variable">$&#123;ACCELERATE_CONFIG_PATH&#125;</span>&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    ACCELERATE_CONFIG_PATH=<span class="string">&quot;<span class="variable">$&#123;HOME&#125;</span>/SimpleTuner/deepspeed_config.yaml&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>결국 <code>DeepSpeed</code> 지원 훈련을 시도하게 된다면, 이에 따라 사용할 수 있는 하위 수준 <code>config.env</code> 샘플이 있습니다.</p><ul><li><p>Custom SD3.5 Large <code>full</code> fine-tune<code>config.json</code></p>  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;full&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--model_family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd3&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resume_from_checkpoint&quot;</span><span class="punctuation">:</span> <span class="string">&quot;latest&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">100</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">100</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">5e-5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--pretrained_model_name_or_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--report_to&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wandb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_project_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-full-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--max_train_steps&quot;</span><span class="punctuation">:</span> <span class="number">24000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--num_train_epochs&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--data_backend_config&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--output_dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--push_checkpoints_to_hub&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--hub_model_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sd35-training&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--resolution_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pixel&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--minimum_image_size&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--instance_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4 &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance&quot;</span><span class="punctuation">:</span> <span class="number">7.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_guidance_rescale&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_steps&quot;</span><span class="punctuation">:</span> <span class="number">25</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_num_inference_steps&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_negative_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blurry, cropped, ugly&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--validation_resolution&quot;</span><span class="punctuation">:</span> <span class="number">1024</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--train_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_scheduler&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cosine&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--lr_warmup_steps&quot;</span><span class="punctuation">:</span> <span class="number">2400</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--caption_dropout_probability&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--metadata_update_interval&quot;</span><span class="punctuation">:</span> <span class="number">65</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--vae_batch_size&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_unwanted_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--delete_problematic_images&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--training_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--inference_scheduler_timestep_spacing&quot;</span><span class="punctuation">:</span> <span class="string">&quot;trailing&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--snr_gamma&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--enable_xformers_memory_efficient_attention&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--gradient_checkpointing&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--allow_tf32&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--optimizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;adamw_bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--use_ema&quot;</span><span class="punctuation">:</span> <span class="keyword">false</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--ema_decay&quot;</span><span class="punctuation">:</span> <span class="number">0.999</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--seed&quot;</span><span class="punctuation">:</span> <span class="number">42</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--mixed_precision&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bf16&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li><li><p>Custom SD3.5 Large <code>full</code> fine-tune<code>config.env</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="built_in">export</span> MODEL_TYPE=<span class="string">&#x27;full&#x27;</span></span><br><span class="line"><span class="built_in">export</span> MODEL_FAMILY=<span class="string">&#x27;sd3&#x27;</span></span><br><span class="line"><span class="built_in">export</span> CONTROLNET=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> USE_DORA=<span class="literal">false</span></span><br><span class="line"><span class="comment"># Restart where we left off. Change this to &quot;checkpoint-1234&quot; to start from a specific checkpoint.</span></span><br><span class="line"><span class="built_in">export</span> RESUME_CHECKPOINT=<span class="string">&quot;latest&quot;</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_STEPS=100</span><br><span class="line"><span class="comment"># This is how many checkpoints we will keep. Two is safe, but three is safer.</span></span><br><span class="line"><span class="built_in">export</span> CHECKPOINTING_LIMIT=100</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is decided as a relatively conservative &#x27;constant&#x27; learning rate.</span></span><br><span class="line"><span class="comment"># Adjust higher or lower depending on how burnt your model becomes.</span></span><br><span class="line"><span class="built_in">export</span> LEARNING_RATE=5e-5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using a Huggingface Hub model:</span></span><br><span class="line"><span class="built_in">export</span> MODEL_NAME=<span class="string">&quot;stabilityai/stable-diffusion-3.5-large&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make DEBUG_EXTRA_ARGS empty to disable wandb.</span></span><br><span class="line"><span class="built_in">export</span> DEBUG_EXTRA_ARGS=<span class="string">&quot;--report_to=wandb&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_PROJECT_NAME=<span class="string">&quot;sd35-training&quot;</span></span><br><span class="line"><span class="built_in">export</span> TRACKER_RUN_NAME=<span class="string">&quot;simpletuner-fantasy-art-full-01&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Max number of steps OR epochs can be used. Not both.</span></span><br><span class="line"><span class="built_in">export</span> MAX_NUM_STEPS=24000</span><br><span class="line"><span class="built_in">export</span> NUM_EPOCHS=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># A convenient prefix for all of your training paths.</span></span><br><span class="line"><span class="built_in">export</span> DATALOADER_CONFIG=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_full_L_01/datasets/multidatabackend.json&quot;</span></span><br><span class="line"><span class="built_in">export</span> OUTPUT_DIR=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_full_L_01/datasets/models&quot;</span></span><br><span class="line"><span class="comment"># Set this to &quot;true&quot; to push your model to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_TO_HUB=<span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="comment"># If PUSH_TO_HUB and PUSH_CHECKPOINTS are both enabled, every saved checkpoint will be pushed to Hugging Face Hub.</span></span><br><span class="line"><span class="built_in">export</span> PUSH_CHECKPOINTS=<span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="comment"># This will be the model name for your final hub upload, eg. &quot;yourusername/yourmodelname&quot;</span></span><br><span class="line"><span class="comment"># It defaults to the wandb project name, but you can override this here.</span></span><br><span class="line"><span class="comment"># export HUB_MODEL_NAME=$TRACKER_PROJECT_NAME</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, images will be resized so their SMALLER EDGE is 1024 pixels, maintaining aspect ratio.</span></span><br><span class="line"><span class="comment"># Setting this value to 768px might result in more reasonable training data sizes for SDXL.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION=1024</span><br><span class="line"><span class="comment"># If you want to have the training data resized by pixel area (Megapixels) rather than edge length,</span></span><br><span class="line"><span class="comment">#  set this value to &quot;area&quot; instead of &quot;pixel&quot;, and uncomment the next RESOLUTION declaration.</span></span><br><span class="line"><span class="built_in">export</span> RESOLUTION_TYPE=<span class="string">&quot;pixel&quot;</span></span><br><span class="line"><span class="comment">#export RESOLUTION=1          # 1.0 Megapixel training sizes</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;pixel&quot;, the minimum resolution specifies the smaller edge length, measured in pixels. Recommended: 1024.</span></span><br><span class="line"><span class="comment"># If RESOLUTION_TYPE=&quot;area&quot;, the minimum resolution specifies the total image area, measured in megapixels. Recommended: 1.</span></span><br><span class="line"><span class="built_in">export</span> MINIMUM_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># How many decimals to round aspect buckets to.</span></span><br><span class="line"><span class="comment">#export ASPECT_BUCKET_ROUNDING=2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use this to append an instance prompt to each caption, used for adding trigger words.</span></span><br><span class="line"><span class="comment"># This has not been tested in SDXL.</span></span><br><span class="line"><span class="built_in">export</span> INSTANCE_PROMPT=<span class="string">&quot;k4s4 &quot;</span></span><br><span class="line"><span class="comment"># If you also supply a user prompt library or `--use_prompt_library`, this will be added to those lists.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_PROMPT=<span class="string">&quot;k4s4, a waist up view of a beautiful blonde woman, green eyes&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE=7.5</span><br><span class="line"><span class="comment"># You&#x27;ll want to set this to 0.7 if you are training a terminal SNR model.</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_GUIDANCE_RESCALE=0.0</span><br><span class="line"><span class="comment"># How frequently we will save and run a pipeline for validations.</span></span><br><span class="line"><span class="comment"># export VALIDATION_STEPS=200</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_STEPS=25</span><br><span class="line"><span class="built_in">export</span> VALIDATION_NUM_INFERENCE_STEPS=30</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> VALIDATION_NEGATIVE_PROMPT=<span class="string">&quot;blurry, cropped, ugly&quot;</span></span><br><span class="line"><span class="built_in">export</span> VALIDATION_SEED=42</span><br><span class="line"><span class="built_in">export</span> VALIDATION_RESOLUTION=1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adjust this for your GPU memory size. This, and resolution, are the biggest VRAM killers.</span></span><br><span class="line"><span class="built_in">export</span> TRAIN_BATCH_SIZE=6</span><br><span class="line"><span class="comment"># Accumulate your update gradient over many steps, to save VRAM while still having higher effective batch size:</span></span><br><span class="line"><span class="comment"># effective batch size = ($TRAIN_BATCH_SIZE * $GRADIENT_ACCUMULATION_STEPS).</span></span><br><span class="line"><span class="built_in">export</span> GRADIENT_ACCUMULATION_STEPS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use any standard scheduler type. constant, polynomial, constant_with_warmup</span></span><br><span class="line"><span class="built_in">export</span> LR_SCHEDULE=<span class="string">&quot;cosine&quot;</span></span><br><span class="line"><span class="comment"># A warmup period allows the model and the EMA weights more importantly to familiarise itself with the current quanta.</span></span><br><span class="line"><span class="comment"># For the cosine or sine type schedules, the warmup period defines the interval between peaks or valleys.</span></span><br><span class="line"><span class="comment"># Use a sine schedule to simulate a warmup period, or a Cosine period to simulate a polynomial start.</span></span><br><span class="line"><span class="comment"># export LR_WARMUP_STEPS=$((MAX_NUM_STEPS / 10))</span></span><br><span class="line"><span class="built_in">export</span> LR_WARMUP_STEPS=2400</span><br><span class="line"></span><br><span class="line"><span class="comment"># Caption dropout probability. Set to 0.1 for 10% of captions dropped out. Set to 0 to disable.</span></span><br><span class="line"><span class="comment"># You may wish to disable dropout if you want to limit your changes strictly to the prompts you show the model.</span></span><br><span class="line"><span class="comment"># You may wish to increase the rate of dropout if you want to more broadly adopt your changes across the model.</span></span><br><span class="line"><span class="built_in">export</span> CAPTION_DROPOUT_PROBABILITY=0</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> METADATA_UPDATE_INTERVAL=65</span><br><span class="line"><span class="built_in">export</span> VAE_BATCH_SIZE=12</span><br><span class="line"></span><br><span class="line"><span class="comment"># If this is set, any images that fail to open will be DELETED to avoid re-checking them every time.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_ERRORED_IMAGES=0</span><br><span class="line"><span class="comment"># If this is set, any images that are too small for the minimum resolution size will be DELETED.</span></span><br><span class="line"><span class="built_in">export</span> DELETE_SMALL_IMAGES=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bytedance recommends these be set to &quot;trailing&quot; so that inference and training behave in a more congruent manner.</span></span><br><span class="line"><span class="comment"># To follow the original SDXL training strategy, use &quot;leading&quot; instead, though results are generally worse.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"><span class="built_in">export</span> INFERENCE_SCHEDULER_TIMESTEP_SPACING=<span class="string">&quot;trailing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing this option or unsetting it uses vanilla training. Setting it reweights the loss by the position of the timestep in the noise schedule.</span></span><br><span class="line"><span class="comment"># A value &quot;5&quot; is recommended by the researchers. A value of &quot;20&quot; is the least impact, and &quot;1&quot; is the most impact.</span></span><br><span class="line"><span class="built_in">export</span> MIN_SNR_GAMMA=5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set this to an explicit value of &quot;false&quot; to disable Xformers. Probably required for AMD users.</span></span><br><span class="line"><span class="built_in">export</span> USE_XFORMERS=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># There&#x27;s basically no reason to unset this. However, to disable it, use an explicit value of &quot;false&quot;.</span></span><br><span class="line"><span class="comment"># This will save a lot of memory consumption when enabled.</span></span><br><span class="line"><span class="built_in">export</span> USE_GRADIENT_CHECKPOINTING=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Options below here may require a bit more complicated configuration, so they are not simple variables.</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TF32 is great on Ampere or Ada, not sure about earlier generations.</span></span><br><span class="line"><span class="built_in">export</span> ALLOW_TF32=<span class="literal">true</span></span><br><span class="line"><span class="comment"># AdamW 8Bit is a robust and lightweight choice. Adafactor might reduce memory consumption, and Dadaptation is slow and experimental.</span></span><br><span class="line"><span class="comment"># AdamW is the default optimizer, but it uses a lot of memory and is slower than AdamW8Bit or Adafactor.</span></span><br><span class="line"><span class="comment"># Choices: adamw, adamw8bit, adafactor, dadaptation</span></span><br><span class="line"><span class="comment"># export OPTIMIZER=&quot;adamw_bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> OPTIMIZER=<span class="string">&quot;adamw_bf16&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># EMA is a strong regularisation method that uses a lot of extra VRAM to hold two copies of the weights.</span></span><br><span class="line"><span class="comment"># This is worthwhile on large training runs, but not so much for smaller training runs.</span></span><br><span class="line"><span class="built_in">export</span> USE_EMA=<span class="literal">false</span></span><br><span class="line"><span class="built_in">export</span> EMA_DECAY=0.999</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TRAINER_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reproducible training. Set to -1 to disable.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_SEED=42</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mixed precision is the best. You honestly might need to YOLO it in fp16 mode for Google Colab type setups.</span></span><br><span class="line"><span class="built_in">export</span> MIXED_PRECISION=<span class="string">&quot;bf16&quot;</span></span><br><span class="line"><span class="built_in">export</span> PURE_BF16=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This has to be changed if you&#x27;re training with multiple GPUs.</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_PROCESSES=1</span><br><span class="line"><span class="built_in">export</span> TRAINING_NUM_MACHINES=1</span><br><span class="line"><span class="built_in">export</span> ACCELERATE_EXTRA_ARGS=<span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># With Pytorch 2.1, you might have pretty good luck here.</span></span><br><span class="line"><span class="comment"># If you&#x27;re using aspect bucketing however, each resolution change will recompile. Seriously, just don&#x27;t do it.</span></span><br><span class="line"><span class="comment"># Well, then again... Pytorch 2.2 has support for dynamic shapes. Why not?</span></span><br><span class="line"><span class="built_in">export</span> TRAINING_DYNAMO_BACKEND=<span class="string">&#x27;no&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> TOKENIZERS_PARALLELISM=<span class="literal">false</span></span><br></pre></td></tr></table></figure></li></ul><p>Changed parameters</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;--model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;full&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;--checkpointing_steps&quot;</span><span class="punctuation">:</span> <span class="number">100</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--checkpoints_total_limit&quot;</span><span class="punctuation">:</span> <span class="number">100</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--learning_rate&quot;</span><span class="punctuation">:</span> <span class="number">5e-5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;--tracker_run_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;simpletuner-fantasy-art-full-01&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>특히 학습률이 ‘5e-5’로 감소했습니다.</p><p>모든 것이 정상이면 계속해서 훈련을 시작하십시오.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash train.sh</span><br></pre></td></tr></table></figure><h3 id="Memory-usage"><a href="#Memory-usage" class="headerlink" title="Memory usage"></a>Memory usage</h3><p>텍스트 인코더를 훈련하지 않는 경우(우리는 그렇지 않습니다) ‘SimpleTuner’를 사용하면 약 ‘10.4GB’의 VRAM을 절약할 수 있습니다.</p><p>‘배치 크기’를 ‘6’으로 설정하고 ‘lora 순위&#x2F;알파’를 ‘768’로 설정하면 훈련에서 약 ‘32GB’의 VRAM을 소비합니다.</p><p>당연히 이는 소비자 ‘24GB’ VRAM GPU의 범위를 벗어납니다. 그래서 <code>batch size</code>를 <code>1</code>, <code>lora Rank/alpha</code>를 <code>128</code>로 사용하여 메모리 비용을 줄이려고 했습니다.</p><p>잠정적으로 VRAM 비용을 약 ‘19.65GB’ VRAM으로 낮출 수 있었습니다.</p><p>그러나 유효성 검사 프롬프트에 대한 추론을 실행하면 VRAM이 최대 ‘23.37GB’까지 급증합니다.</p><p>안전을 위해 ‘lora 순위&#x2F;알파’를 ‘64’로 더욱 줄여야 할 수도 있습니다. 그렇다면 훈련 중에 약 ‘18.83GB’의 VRAM을 소비하게 됩니다.</p><p>검증 추론 중에는 최대 약 ‘21.50GB’의 VRAM이 사용됩니다. 이 정도면 충분히 안전해 보입니다.</p><p>‘배치 크기’ ‘6’ 및 ‘lora 순위&#x2F;알파’ ‘768’의 더 높은 사양 교육을 사용하기로 결정한 경우 [위](https:&#x2F;&#x2F; <a href="http://www.notion.so/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6?pvs=21">www.notion.so/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6?pvs=21</a>) GPU VRAM이 부족하고 CPU RAM이 충분한 경우.</p><h3 id="Monitoring-the-training"><a href="#Monitoring-the-training" class="headerlink" title="Monitoring the training"></a>Monitoring the training</h3><p>훈련 과정에서 검증 이미지가 픽셀화되거나 검게 변하는 경우가 있을 수 있습니다. 이는 ‘1.05e-3’이라는 매우 공격적인 학습률을 사용하고 있기 때문입니다. 더 안전하게 플레이하고 싶다면 ‘9.5e-4’를 사용하면 픽셀화 문제가 거의 발생하지 않습니다. 그럼에도 불구하고 두 손실 곡선은 결국 훌륭하게 수렴했습니다.</p><p>하지만 우려사항을 해소하기 위해 어떤 모습일지 몇 가지 예를 보여드리고 싶습니다.</p><h3 id="Observing-training-loss"><a href="#Observing-training-loss" class="headerlink" title="Observing training loss"></a>Observing training loss</h3><h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a><code>LoRA</code></h3><p>판타지 아트 ‘LoRA’ 수련을 통해 얻은 피규어들입니다. 손실이 감소하고 있으며 아직 수렴되지 않았습니다. 그러나 확산 모델을 미세 조정한 경험이 있는 경우 손실 최소화는 미적 극대화와 거의 관련이 없습니다. 또한 높은 학습률을 사용하는 경우 손실 곡선의 최고점 근처에서 검증 이미지의 픽셀화 또는 품질 저하가 발생할 수 있음을 확인했습니다. 훈련이 모델 가중치가 만족스럽지 않은 학습 속도에 도달하면 이는 의미가 있습니다.</p><p>학습률이 높으면 열차 손실도 최고점에 달합니다.</p><h2 id="Evaluating-the-results"><a href="#Evaluating-the-results" class="headerlink" title="Evaluating the results"></a>Evaluating the results</h2><h3 id="How-to-actually-get-the-LoRA-models-into-ComfyUI"><a href="#How-to-actually-get-the-LoRA-models-into-ComfyUI" class="headerlink" title="How to actually get the LoRA models into ComfyUI"></a>How to actually get the LoRA models into ComfyUI</h3><p>이제 모델이 모두 훈련되었으므로 <code>ComfyUI</code>를 사용하여 테스트할 차례입니다. 그러나 SimpleTuner가 모델을 저장하는 방식으로 인해 ‘ComfyUI&#x2F;models&#x2F;loras’ 디렉터리로 가져오기가 약간 어렵습니다.</p><p>모델을 저장한 디렉터리로 이동하면 해당 형식이 이 형식인 것을 볼 수 있습니다.</p><p>각 디렉토리에서 원하는 파일은 <code>pytorch_lora_weights.safetensors</code> 파일입니다. 이러한 파일을 <code>ComfyUI</code>로 가져오는 프로세스를 간소화하기 위해 다음 스크립트를 작성했습니다.</p><ul><li><p><code>create_symlinks_lora.sh</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Source directory where the models are stored</span></span><br><span class="line">SOURCE_DIR=<span class="string">&quot;/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Target directory for symlinks</span></span><br><span class="line">TARGET_DIR=<span class="string">&quot;/weka2/home-yeo/ComfyUI/models/loras/sd35_large/fantasy_art&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ensure target directory exists or create it</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="string">&quot;<span class="variable">$&#123;TARGET_DIR&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over each checkpoint directory</span></span><br><span class="line"><span class="keyword">for</span> CHECKPOINT_DIR <span class="keyword">in</span> <span class="variable">$&#123;SOURCE_DIR&#125;</span>/checkpoint-*; <span class="keyword">do</span></span><br><span class="line">    <span class="comment"># Check if it&#x27;s indeed a directory</span></span><br><span class="line">    <span class="keyword">if</span> [ -d <span class="string">&quot;<span class="variable">$&#123;CHECKPOINT_DIR&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="comment"># Extract the checkpoint number from the directory name</span></span><br><span class="line">        CHECKPOINT_NAME=$(<span class="built_in">basename</span> <span class="variable">$&#123;CHECKPOINT_DIR&#125;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Define the source file path</span></span><br><span class="line">        SOURCE_FILE=<span class="string">&quot;<span class="variable">$&#123;CHECKPOINT_DIR&#125;</span>/pytorch_lora_weights.safetensors&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Define the symlink name with &#x27;lora&#x27; added before &#x27;safetensors&#x27;</span></span><br><span class="line">        LINK_NAME=<span class="string">&quot;<span class="variable">$&#123;TARGET_DIR&#125;</span>/<span class="variable">$&#123;CHECKPOINT_NAME&#125;</span>_lora.safetensors&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if the source file exists</span></span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$&#123;SOURCE_FILE&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="comment"># Create a symlink in the target directory</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;Creating symlink from <span class="variable">$&#123;SOURCE_FILE&#125;</span> to <span class="variable">$&#123;LINK_NAME&#125;</span>&quot;</span></span><br><span class="line">            <span class="built_in">ln</span> -s <span class="string">&quot;<span class="variable">$&#123;SOURCE_FILE&#125;</span>&quot;</span> <span class="string">&quot;<span class="variable">$&#123;LINK_NAME&#125;</span>&quot;</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;Symlink created for <span class="variable">$&#123;CHECKPOINT_NAME&#125;</span>&quot;</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;File not found: <span class="variable">$&#123;SOURCE_FILE&#125;</span>&quot;</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;Not a directory: <span class="variable">$&#123;CHECKPOINT_DIR&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Symlinking complete.&quot;</span></span><br></pre></td></tr></table></figure></li></ul><p>위의 쉘 스크립트가 수행할 작업은 <code>SimpleTuner</code>에서 <code>SOURCE_DIR</code>을 반복한 다음 <em><strong>만</strong></em> <code>pytorch_lora_weights.safetensors</code> 파일을 <code>TARGET_DIR</code>에 심볼릭 링크하는 것입니다. 이 파일은 <code>ComfyUI 내부 디렉토리여야 합니다. /모델/로라스</code>. 파일을 추적하기 위해 파일 이름 안에 해당 체크포인트 번호가 포함되도록 이름도 변경했습니다.</p><h3 id="Determining-the-best-checkpoint"><a href="#Determining-the-best-checkpoint" class="headerlink" title="Determining the best checkpoint"></a>Determining the best checkpoint</h3><p>제가 사용하고 있는 기본적인 ‘SD3.5 Large’ 워크플로는 이것이었습니다.</p><p>가장 좋은 체크포인트를 결정하는 방법은 특정 프롬프트에 대해 x축에 체크포인트 번호를 표시하는 것입니다. 그래서 저는 다음과 같은 단일 스트립을 얻습니다.</p><p>판타지 아트 ‘LoRA’</p><p>Prompt</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a three fourth perspective waist up portrait view of a young woman with messy long blonde hair and light purple eyes, looking at viewer with a closed mouth smile, wearing tight black dress, a faded pink simple background during golden hour</span><br></pre></td></tr></table></figure><p>이를 위해 <code>ComfyUI</code> 워크플로의 <code>api</code> 버전에 로드되는 사용자 정의 스크립트를 사용합니다. 저장(API 형식) 버튼을 클릭하면 모든 워크플로우를 ‘API’ 형식으로 저장할 수 있습니다. 귀하가 사용할 수 있도록 이미 위 버전을 저장했습니다. ‘ComfyUI’ API 사용에 대한 더 심층적인 비디오 가이드를 원하시면 제가 작년에 <a href="https://youtu.be/WwsJ_QIgsG8">여기</a>를 만들었습니다.</p><p><code>ComfyUI</code>가 실행 중인지 확인한 후 아래 스크립트를 실행하세요. 또한 스크립트를 실행하는 동일한 위치에 <code>.env</code> 파일을 설정해야 합니다.</p><ul><li><p><code>API script</code></p><p>This is my custom <code>python</code> script:</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import json</span><br><span class="line">import random</span><br><span class="line">from urllib import request</span><br><span class="line">import datetime</span><br><span class="line">from PIL import Image, ImageDraw, ImageFont</span><br><span class="line">import time</span><br><span class="line">import re</span><br><span class="line">import urllib.error</span><br><span class="line"></span><br><span class="line">from dotenv import load_dotenv</span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"># Configuration</span><br><span class="line">api_workflow_dir = os.getenv(&quot;API_WORKFLOW_DIR&quot;)</span><br><span class="line">lora_dir = os.getenv(&quot;LORA_DIR&quot;)</span><br><span class="line"></span><br><span class="line">api_workflow_file = os.getenv(&quot;API_WORKFLOW_FILE&quot;)</span><br><span class="line">api_endpoint = os.getenv(&quot;API_ENDPOINT&quot;)</span><br><span class="line">image_output_dir = os.getenv(&quot;IMAGE_OUTPUT_DIR&quot;)</span><br><span class="line">font_ttf_path = os.getenv(&quot;FONT_TTF_PATH&quot;)</span><br><span class="line"></span><br><span class="line">comfyui_output_dir = os.getenv(&quot;COMFYUI_OUTPUT_DIR&quot;)</span><br><span class="line"></span><br><span class="line">api_endpoint = f&quot;http://&#123;api_endpoint&#125;/prompt&quot;</span><br><span class="line"></span><br><span class="line">workflow_file_path = os.path.join(api_workflow_dir, api_workflow_file)</span><br><span class="line">workflow = json.load(open(workflow_file_path))</span><br><span class="line"></span><br><span class="line">current_datetime = datetime.datetime.now().strftime(&quot;%Y-%m-%d_%H-%M-%S&quot;)</span><br><span class="line">relative_output_path = current_datetime</span><br><span class="line"></span><br><span class="line">directory_creation_timeout = 3000  # Timeout for directory creation in seconds</span><br><span class="line">image_generation_timeout = 30000  # Timeout for image generation in seconds</span><br><span class="line"></span><br><span class="line">def get_checkpoint_number(filename):</span><br><span class="line">    match = re.search(r&#x27;checkpoint-(\d+)&#x27;, filename)</span><br><span class="line">    if match:</span><br><span class="line">        return int(match.group(1))</span><br><span class="line">    match = re.search(r&#x27;/checkpoint-(\d+)/&#x27;, filename)</span><br><span class="line">    if match:</span><br><span class="line">        return int(match.group(1))</span><br><span class="line">    return None</span><br><span class="line"></span><br><span class="line">def get_most_recent_output_folder(base_dir):</span><br><span class="line">    folders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]</span><br><span class="line">    if not folders:</span><br><span class="line">        return None</span><br><span class="line">    return max(folders, key=lambda f: os.path.getctime(os.path.join(base_dir, f)))</span><br><span class="line"></span><br><span class="line">def process_loras(lora_dir, workflow):</span><br><span class="line">    print(f&quot;Scanning directory: &#123;lora_dir&#125;&quot;)</span><br><span class="line">    </span><br><span class="line">    # Extract the last two directories from LORA_DIR</span><br><span class="line">    lora_path_parts = lora_dir.split(&#x27;/&#x27;)</span><br><span class="line">    dynamic_lora_path = &#x27;/&#x27;.join(lora_path_parts[-2:])</span><br><span class="line">    </span><br><span class="line">    all_items = os.listdir(lora_dir)</span><br><span class="line">    </span><br><span class="line">    lora_items = [f for f in all_items if f.endswith(&#x27;_lora.safetensors&#x27;)]</span><br><span class="line">    </span><br><span class="line">    lora_items.sort(key=lambda x: int(x.split(&#x27;-&#x27;)[1].split(&#x27;_&#x27;)[0]))</span><br><span class="line">    </span><br><span class="line">    print(f&quot;Found items: &#123;lora_items&#125;&quot;)</span><br><span class="line">    </span><br><span class="line">    for item in lora_items:</span><br><span class="line">        checkpoint_num = item.split(&#x27;-&#x27;)[1].split(&#x27;_&#x27;)[0]</span><br><span class="line">        </span><br><span class="line">        print(f&quot;Processing: &#123;item&#125;&quot;)</span><br><span class="line"></span><br><span class="line">        # Update the LoRA loader node</span><br><span class="line">        lora_loader_node = workflow[&quot;276&quot;]</span><br><span class="line">        lora_loader_node[&quot;inputs&quot;][&quot;lora_name&quot;] = f&quot;&#123;dynamic_lora_path&#125;/&#123;item&#125;&quot;</span><br><span class="line"></span><br><span class="line">        save_image = workflow[&quot;314&quot;]</span><br><span class="line">        filename_prefix = f&quot;checkpoint-&#123;checkpoint_num&#125;&quot;</span><br><span class="line">        save_image[&quot;inputs&quot;][&quot;output_path&quot;] = relative_output_path</span><br><span class="line">        save_image[&quot;inputs&quot;][&quot;filename_prefix&quot;] = filename_prefix</span><br><span class="line"></span><br><span class="line">        success = queue_prompt(workflow)</span><br><span class="line">        if not success:</span><br><span class="line">            print(f&quot;Failed to queue prompt for checkpoint &#123;checkpoint_num&#125;&quot;)</span><br><span class="line">        else:</span><br><span class="line">            print(f&quot;Successfully queued prompt for checkpoint &#123;checkpoint_num&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    if not lora_items:</span><br><span class="line">        print(&quot;No LoRA files found in the directory.&quot;)</span><br><span class="line">    </span><br><span class="line">    return len(lora_items)</span><br><span class="line"></span><br><span class="line">def create_image_strip(lora_dir, image_folder, output_filename):</span><br><span class="line">    lora_files = [f for f in os.listdir(lora_dir) if f.endswith(&#x27;_lora.safetensors&#x27;)]</span><br><span class="line">    lora_files.sort(key=get_checkpoint_number)</span><br><span class="line">    checkpoints = [get_checkpoint_number(f) for f in lora_files if get_checkpoint_number(f) is not None]</span><br><span class="line"></span><br><span class="line">    images = []</span><br><span class="line">    for checkpoint in checkpoints:</span><br><span class="line">        filename = f&quot;checkpoint-&#123;checkpoint&#125;_0001.png&quot;</span><br><span class="line">        filepath = os.path.join(image_folder, filename)</span><br><span class="line">        if os.path.exists(filepath):</span><br><span class="line">            try:</span><br><span class="line">                img = Image.open(filepath)</span><br><span class="line">                images.append(img)</span><br><span class="line">            except IOError as e:</span><br><span class="line">                print(f&quot;Cannot open image: &#123;filepath&#125;&quot;)</span><br><span class="line">                print(f&quot;Error: &#123;e&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    if not images:</span><br><span class="line">        print(&quot;No valid images found.&quot;)</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    img_width, img_height = images[0].size</span><br><span class="line">    strip_width = img_width * len(images)</span><br><span class="line">    label_height = 50  # Space for labels</span><br><span class="line">    strip_height = img_height + label_height</span><br><span class="line"></span><br><span class="line">    strip_image = Image.new(&#x27;RGB&#x27;, (strip_width, strip_height), &#x27;white&#x27;)</span><br><span class="line">    draw = ImageDraw.Draw(strip_image)</span><br><span class="line">    font = ImageFont.truetype(font_ttf_path, 20)</span><br><span class="line"></span><br><span class="line">    for i, (img, checkpoint) in enumerate(zip(images, checkpoints)):</span><br><span class="line">        strip_image.paste(img, (i * img_width, label_height))</span><br><span class="line">        </span><br><span class="line">        label = f&quot;checkpoint-&#123;checkpoint&#125;&quot;</span><br><span class="line">        label_width = draw.textlength(label, font=font)</span><br><span class="line">        label_x = i * img_width + (img_width - label_width) // 2</span><br><span class="line">        draw.text((label_x, 10), label, fill=&quot;black&quot;, font=font)</span><br><span class="line"></span><br><span class="line">    strip_image.save(output_filename)</span><br><span class="line">    print(f&quot;Image strip saved to: &#123;output_filename&#125;&quot;)</span><br><span class="line"></span><br><span class="line">def queue_prompt(workflow):</span><br><span class="line">    p = &#123;&quot;prompt&quot;: workflow&#125;</span><br><span class="line">    data = json.dumps(p).encode(&#x27;utf-8&#x27;)</span><br><span class="line">    req = request.Request(api_endpoint, data=data, headers=&#123;&#x27;Content-Type&#x27;: &#x27;application/json&#x27;&#125;)</span><br><span class="line">    try:</span><br><span class="line">        with request.urlopen(req) as response:</span><br><span class="line">            print(f&quot;API request successful. Status code: &#123;response.getcode()&#125;&quot;)</span><br><span class="line">            return True</span><br><span class="line">    except urllib.error.URLError as e:</span><br><span class="line">        if hasattr(e, &#x27;reason&#x27;):</span><br><span class="line">            print(f&quot;Failed to reach the server. Reason: &#123;e.reason&#125;&quot;)</span><br><span class="line">        elif hasattr(e, &#x27;code&#x27;):</span><br><span class="line">            print(f&quot;The server couldn&#x27;t fulfill the request. Error code: &#123;e.code&#125;&quot;)</span><br><span class="line">        print(f&quot;API endpoint: &#123;api_endpoint&#125;&quot;)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        print(f&quot;An error occurred: &#123;str(e)&#125;&quot;)</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">def wait_for_directory_creation(directory, timeout):</span><br><span class="line">    print(f&quot;Waiting for directory &#123;directory&#125; to be created...&quot;)</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    while time.time() - start_time &lt; timeout:</span><br><span class="line">        if os.path.exists(directory):</span><br><span class="line">            print(f&quot;Directory &#123;directory&#125; found.&quot;)</span><br><span class="line">            return True</span><br><span class="line">        time.sleep(5)  # Check every 5 seconds</span><br><span class="line">    print(f&quot;Timeout waiting for directory &#123;directory&#125; to be created.&quot;)</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">def wait_for_images(image_folder, expected_count, timeout):</span><br><span class="line">    print(&quot;Waiting for images to be generated...&quot;)</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    while time.time() - start_time &lt; timeout:</span><br><span class="line">        if os.path.exists(image_folder):</span><br><span class="line">            image_files = [f for f in os.listdir(image_folder) if f.endswith(&#x27;.png&#x27;)]</span><br><span class="line">            if len(image_files) &gt;= expected_count:</span><br><span class="line">                print(f&quot;Found all &#123;expected_count&#125; images.&quot;)</span><br><span class="line">                return True</span><br><span class="line">        time.sleep(5)  # Check every 5 seconds</span><br><span class="line">    print(&quot;Timeout waiting for images to be generated.&quot;)</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    print(f&quot;LoRA directory: &#123;lora_dir&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    # Generate images</span><br><span class="line">    expected_image_count = process_loras(lora_dir, workflow)</span><br><span class="line"></span><br><span class="line">    absolute_output_path = os.path.join(comfyui_output_dir, current_datetime)</span><br><span class="line">    print(f&quot;Absolute output path: &#123;absolute_output_path&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    # Create the image strip</span><br><span class="line">    if wait_for_directory_creation(absolute_output_path, directory_creation_timeout):</span><br><span class="line">        print(f&quot;Expected image count: &#123;expected_image_count&#125;&quot;)</span><br><span class="line">        if wait_for_images(absolute_output_path, expected_image_count, image_generation_timeout):</span><br><span class="line">            output_strip_filename = os.path.join(absolute_output_path, &quot;output_image_strip.png&quot;)</span><br><span class="line">            create_image_strip(lora_dir, absolute_output_path, output_strip_filename)</span><br><span class="line">        else:</span><br><span class="line">            print(&quot;Failed to generate all images in time.&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;Output directory was not created.&quot;)</span><br></pre></td></tr></table></figure></li><li><p>sample <code>.env</code> file</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">API_WORKFLOW_DIR=/weka2/home-yeo/workflows</span><br><span class="line">COMFYUI_OUTPUT_DIR = /weka2/home-yeo/ComfyUI/output/</span><br><span class="line">LORA_DIR=/admin/home-yeo/workspace/ComfyUI/models/loras/sd35_large/fantasy_art_01</span><br><span class="line">API_WORKFLOW_FILE=sd35_fantasy_art_02_api.json</span><br><span class="line">API_ENDPOINT=127.0.0.1:8188</span><br><span class="line">FONT_TTF_PATH=/weka2/home-yeo/fonts/arial.ttf</span><br><span class="line">BOLD_FONT_TTF_PATH=/weka2/home-yeo/fonts/arialbd.ttf</span><br></pre></td></tr></table></figure></li></ul><p>Fantasy Art <code>LoRA</code></p><p>Prompt</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a three fourth perspective waist up portrait view of a young woman with messy long blonde hair and light purple eyes, looking at viewer with a closed mouth smile, wearing tight black dress, a faded pink simple background during golden hour</span><br></pre></td></tr></table></figure><p>결국 ‘24,000’ 단계에서 거의 마지막에 체크포인트를 선택하게 되었습니다.</p><p>나는 또한 건전성 확인을 위해 수행한 다른 모든 훈련에 대해 동일한 실험을 실행했습니다.</p><p>Cinema Photo <code>LoRA</code></p><p>Prompt</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a few hooded figures walking on an empty road in the rain, desolate, high skyscrapers</span><br></pre></td></tr></table></figure><p>John Singer Sargent <code>LoRA</code></p><p>Prompt</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">an abandoned beach with a lighthouse</span><br></pre></td></tr></table></figure><p>Underexposed Photography <code>LoRA</code></p><p>Prompt</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">waist up view of a woman posing on a runway, streetwear in the style of alexander mcqueen</span><br></pre></td></tr></table></figure><p>전문적인 이유로 원래 그리드의 특정 부분이 생략되었습니다. 전체 그리드에는 모든 청중에게 적합하지 않을 수 있는 콘텐츠가 포함되어 있으므로 기술적인 측면에 초점을 맞추기 위해 잘린 버전이 표시됩니다.</p><p>Pixel Art <code>LoRA</code></p><p>Prompt</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a plush chibi mythical creature</span><br></pre></td></tr></table></figure><p>Ethnic Paint <code>LoRA</code></p><p>Prompt</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a skyline view of a futuristic maritime village floating above ground, <span class="keyword">in</span> the clouds, towering skyscrapers, golden hour, day time lighting</span><br></pre></td></tr></table></figure><h2 id="A-x2F-B-evaluation"><a href="#A-x2F-B-evaluation" class="headerlink" title="A&#x2F;B evaluation"></a>A&#x2F;B evaluation</h2><h3 id="Improving-x2F-tuning-generations-with-APG-scaling"><a href="#Improving-x2F-tuning-generations-with-APG-scaling" class="headerlink" title="Improving&#x2F;tuning generations with APG scaling"></a>Improving&#x2F;tuning generations with APG scaling</h3><p>최고의 미적 결과를 제공하는 ‘LoRA’ 체크포인트를 찾았으면 ‘APG’ 스케일링을 통해 이를 더욱 향상시킬 수 있습니다. ‘APG’ 스케일링은 적응형 예측 지침을 의미합니다.</p><p><a href="https://arxiv.org/abs/2410.02416">APG 논문</a> 초록의 핵심 부분</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Our approach, termed adaptive projected guidance (APG), retains the quality-boosting advantages of CFG while enabling the use of higher guidance scales without oversaturation. APG is easy to implement and introduces practically no additional computational overhead to the sampling process.</span><br></pre></td></tr></table></figure><p>이것이 이 샘플 워크플로에 포함된 <a href="https://github.com/logtd/ComfyUI-APGScaling">ComfyUI 노드</a>입니다. 세 가지 다른 이미지를 생성합니다. 하나는 기본 이미지, 하나는 ***<code>APG</code> 스케일링 없이 ***<code>LoRA</code> 적용, 세 번째 이미지는 ***<code>사용***</code> LoRA<code> 적용 APG</code> 스케일링.</p><p>The parameters for APG are:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">eta</span><br><span class="line">norm<span class="emphasis">_threshold</span></span><br><span class="line"><span class="emphasis">use_</span>momentum</span><br><span class="line">momentum</span><br></pre></td></tr></table></figure><p>이 노드에 대해 그렇게 많이 심층 분석하지는 않았지만 이미지 품질이 좋든 나쁘든 변경됩니다.</p><h3 id="Before-and-after-comparison"><a href="#Before-and-after-comparison" class="headerlink" title="Before and after comparison"></a>Before and after comparison</h3><p>Fantasy Art</p><p>Prompt</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a three fourth perspective waist up portrait view of a young woman with messy long blonde hair and light purple eyes, perfect face, looking at viewer with a closed mouth smile, wearing loose black dress, a faded pink simple background during golden hour</span><br></pre></td></tr></table></figure><p><code>Base model</code></p><p><code>LoRA</code></p><p><code>LoRA</code> + <code>APG</code></p><p>Cinema Photo</p><p>Prompt</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a wide view of a figure looking up at a meteor breaking apart</span><br></pre></td></tr></table></figure><p><code>Base model</code></p><p><code>LoRA</code></p><p><code>LoRA</code> + <code>APG</code></p><p>John Singer Sargent</p><p>Prompt</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">an abandoned beach with a lighthouse</span><br></pre></td></tr></table></figure><p><code>Base model</code></p><p><code>LoRA</code></p><p><code>LoRA</code> + <code>APG</code></p><p>Underexposed Photography</p><p>Prompt</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">waist up view of a woman posing on a runway, streetwear in the style of alexander mcqueen</span><br></pre></td></tr></table></figure><p><code>Base model</code></p><p><code>LoRA</code></p><p><code>LoRA</code> + <code>APG</code></p><p>Pixel Art</p><p>Prompt</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a sci-fi venetian town near the water</span><br></pre></td></tr></table></figure><p><code>Base model</code></p><p><code>LoRA</code></p><p><code>LoRA</code> + <code>APG</code></p><p>Ethnic Paint</p><p>Prompt</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a man in his late 30s to early 40s, rendered in a dark, moody style, The subject is depicted from the shoulders up, facing the viewer directly, He has a full, thick beard and mustache, which is dark and well-groomed, with a few strands of gray, His hair is short and neatly combed, with a few strands falling over his forehead, His eyes are dark and piercing, with a slight hint of sadness or introspection, </span><br></pre></td></tr></table></figure><p><code>Base model</code></p><p><code>LoRA</code></p><p><code>LoRA</code> + <code>APG</code></p><p><code>APG</code> 는 그 말에 충실한 것 같습니다. 채도를 줄여줍니다. 개인적으로 나는 바랜 색상을 선호하지 않지만 밋밋한 “RAW” 같은 이미지를 얻을 수 있는 좋은 방법이 될 수 있습니다.</p><h2 id="Other-fine-tuning-tools-x2F-libraries-for-SD3-5"><a href="#Other-fine-tuning-tools-x2F-libraries-for-SD3-5" class="headerlink" title="Other fine-tuning tools&#x2F;libraries for SD3.5"></a>Other fine-tuning tools&#x2F;libraries for SD3.5</h2><p>Hugging Face의 <a href="https://huggingface.co/blog/sd3-5#training-loras-with-sd35-large-with-Quantization">이 스크립트 및 구성</a>을 참조하세요. 이는 사용하기가 더 간단하지만 결과는 약간 더 나쁠 수 있습니다.</p><h2 id="Conclusion-amp-Feedback"><a href="#Conclusion-amp-Feedback" class="headerlink" title="Conclusion &amp; Feedback"></a>Conclusion &amp; Feedback</h2><p>여기 있는 모든 정보가 출시일에 SD3.5 Large를 미세 조정하는 데 도움이 되기를 바랍니다. ‘DiT’ 아키텍처는 여전히 상대적으로 새로운 것이기 때문에 우리는 구성, 질감 및 전체적인 미학 측면에서 최고의 이미지 품질을 달성하기 위해 다양한 방법을 시도했습니다. 최상의 결과를 얻지 못하는 문제가 발생하는 경우 훈련 중에 보다 세부적인 레이어 조작을 적극 권장합니다.</p><h2 id="Two-cents-from-Dango"><a href="#Two-cents-from-Dango" class="headerlink" title="Two cents from Dango"></a>Two cents from Dango</h2><p>따라서 SD3.5 시리즈의 주요 설계자 중 하나인 Dango의 추가 정보는 다음과 같습니다.</p><p><a href="https://huggingface.co/Dango233">Dango’s Hugging Face profile</a></p><h3 id="Diving-into-SD3-5-Large-Architecture"><a href="#Diving-into-SD3-5-Large-Architecture" class="headerlink" title="Diving into SD3.5 Large Architecture"></a>Diving into SD3.5 Large Architecture</h3><p>SD 3.5 Large의 큰 그림을 이해하기 위해 먼저 아키텍처를 인쇄해 보겠습니다.</p><p>모델을 로컬 디렉터리에 다운로드하는 경우 <code>stable-diffusion-3-medium-diffusers</code>와 유사한 파일 구조를 가져야 합니다.</p><p>SD3.5 Large의 경우 다음과 같습니다.</p><p>키를 나열하려고 하면 샤딩된 디퓨저 형식의 기본 모델에서 오류가 발생하므로 이를 단일 모델로 병합하는 코드입니다. 이 시점에서 나는 모델의 로컬 버전으로 작업하고 있었지만 <code>.cache</code>에 다운로드한 Hugging Face 버전과 동일합니다.</p><p>Example path:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home-kasukanra/.cache/huggingface/hub/models--stabilityai--stable-diffusion-3.5-large/snapshots/1a43aa3b9bb52ead637f9693a228092aa802a5dd/transformer</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import safetensors.torch</span><br><span class="line"></span><br><span class="line">shards = [</span><br><span class="line">    &quot;/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/test_convert/transformer/diffusion_pytorch_model-00001-of-00002.safetensors&quot;,</span><br><span class="line">    &quot;/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/test_convert/transformer/diffusion_pytorch_model-00002-of-00002.safetensors&quot;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># Initialize an empty state dictionary</span><br><span class="line">combined_state_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"># Load each shard and merge into combined_state_dict</span><br><span class="line">for shard in shards:</span><br><span class="line">    ckpt = safetensors.torch.load_file(shard)</span><br><span class="line">    combined_state_dict.update(ckpt)</span><br><span class="line"></span><br><span class="line"># Specify the output path for the combined model</span><br><span class="line">output_path = &quot;/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/merged/combined_model.safetensors&quot;</span><br><span class="line"></span><br><span class="line"># Save the combined state dictionary to a single .safetensors file</span><br><span class="line">safetensors.torch.save_file(combined_state_dict, output_path)</span><br><span class="line">print(f&quot;Combined model saved successfully at &#123;output_path&#125;&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>제 경우에는 병합된 모델(<code>combined_model.safetensors</code>)이 있으면 이 스크립트를 실행하여 아키텍처를 텍스트 파일에 저장하세요. 스크립트는 변환기 모델의 일반적인 순차 흐름을 출력합니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import safetensors.torch</span><br><span class="line">import re</span><br><span class="line">import json</span><br><span class="line">from collections import defaultdict</span><br><span class="line"></span><br><span class="line">def group_keys(keys):</span><br><span class="line">    groups = defaultdict(list)</span><br><span class="line">    for key in keys:</span><br><span class="line">        if &#x27;transformer_blocks&#x27; in key:</span><br><span class="line">            block_num = int(re.search(r&#x27;transformer_blocks\.(\d+)&#x27;, key).group(1))</span><br><span class="line">            groups[f&#x27;transformer_block_&#123;block_num&#125;&#x27;].append(key)</span><br><span class="line">        elif &#x27;embed&#x27; in key:</span><br><span class="line">            groups[&#x27;embedding&#x27;].append(key)</span><br><span class="line">        elif &#x27;pos_embed&#x27; in key:</span><br><span class="line">            groups[&#x27;positional_embedding&#x27;].append(key)</span><br><span class="line">        elif &#x27;time_text_embed&#x27; in key:</span><br><span class="line">            groups[&#x27;time_text_embedding&#x27;].append(key)</span><br><span class="line">        elif &#x27;norm_out&#x27; in key:</span><br><span class="line">            groups[&#x27;output_normalization&#x27;].append(key)</span><br><span class="line">        elif &#x27;proj_out&#x27; in key:</span><br><span class="line">            groups[&#x27;output_projection&#x27;].append(key)</span><br><span class="line">        else:</span><br><span class="line">            groups[&#x27;other&#x27;].append(key)</span><br><span class="line">    return groups</span><br><span class="line"></span><br><span class="line">def order_groups(groups):</span><br><span class="line">    order = [</span><br><span class="line">        &#x27;embedding&#x27;,</span><br><span class="line">        &#x27;positional_embedding&#x27;,</span><br><span class="line">        &#x27;time_text_embedding&#x27;,</span><br><span class="line">    ] + [f&#x27;transformer_block_&#123;i&#125;&#x27; for i in range(38)] + [</span><br><span class="line">        &#x27;output_normalization&#x27;,</span><br><span class="line">        &#x27;output_projection&#x27;,</span><br><span class="line">        &#x27;other&#x27;</span><br><span class="line">    ]</span><br><span class="line">    return &#123;k: groups[k] for k in order if k in groups&#125;</span><br><span class="line"></span><br><span class="line">def pretty_print_and_save(ckpt, output_file):</span><br><span class="line">    keys_list = list(ckpt.keys())</span><br><span class="line">    grouped_keys = group_keys(keys_list)</span><br><span class="line">    ordered_groups = order_groups(grouped_keys)</span><br><span class="line">    </span><br><span class="line">    output = []</span><br><span class="line">    for group, keys in ordered_groups.items():</span><br><span class="line">        output.append(f&quot;\n&#123;group.upper()&#125;:&quot;)</span><br><span class="line">        output.extend(sorted(keys))</span><br><span class="line">    </span><br><span class="line">    pretty_output = &#x27;\n&#x27;.join(output)</span><br><span class="line">    </span><br><span class="line">    with open(output_file, &#x27;w&#x27;) as f:</span><br><span class="line">        f.write(pretty_output)</span><br><span class="line">    </span><br><span class="line">    print(f&quot;Grouped keys have been saved to &#123;output_file&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># Load the checkpoint</span><br><span class="line">checkpoint_path = &quot;/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/merged/combined_model.safetensors&quot;</span><br><span class="line">ckpt = safetensors.torch.load_file(checkpoint_path)</span><br><span class="line"></span><br><span class="line"># Pretty-print and save the grouped keys to a file</span><br><span class="line">output_file = &quot;ckpt_keys_grouped_output.txt&quot;</span><br><span class="line">pretty_print_and_save(ckpt, output_file)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr><ul><li><a href="https://stabilityai.notion.site/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6">Stable Diffusion 3.5 Large Fine-tuning Tutorial</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2024/10/2024-10-25-Stable%20Diffusion_3_5_Large_Fine-tuning_Tutorial/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CHAPTER 2 리스트와 딕셔너리</title>
      <link>https://sejoung.github.io/2024/10/2024-10-18-Effective_Python_CHAPTER_2/</link>
      <guid>https://sejoung.github.io/2024/10/2024-10-18-Effective_Python_CHAPTER_2/</guid>
      <pubDate>Fri, 18 Oct 2024 12:27:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;2장-리스트와-딕셔너리&quot;&gt;&lt;a href=&quot;#2장-리스트와-딕셔너리&quot; class=&quot;headerlink&quot; title=&quot;2장 리스트와 딕셔너리&quot;&gt;&lt;/a&gt;2장 리스트와 딕셔너리&lt;/h1&gt;&lt;p&gt;리스트는 아주 간편하며 다양한 문제를 해결하는데 사용&lt;
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="2장-리스트와-딕셔너리"><a href="#2장-리스트와-딕셔너리" class="headerlink" title="2장 리스트와 딕셔너리"></a>2장 리스트와 딕셔너리</h1><p>리스트는 아주 간편하며 다양한 문제를 해결하는데 사용<br>리스트를 자연스럽게 보완할수 있는 타입이 딕셔너리 타입이다</p><p>딕셔너리 타입은 일반적으로 해시 테이블이나 연관 배열이라고 부르는 데이터 구조 안에 값을 저장</p><h2 id="Better-way-11-시퀀스를-슬라이싱하는-방법을-익혀라"><a href="#Better-way-11-시퀀스를-슬라이싱하는-방법을-익혀라" class="headerlink" title="Better way 11 시퀀스를 슬라이싱하는 방법을 익혀라"></a>Better way 11 시퀀스를 슬라이싱하는 방법을 익혀라</h2><ul><li>슬라이싱 구문의 기본 형태는 리스트[시작:끝] 이다</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;]</span><br><span class="line">print(&#x27;가운데 2개:&#x27;, a[3:5])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">assert a[:5] == a[0:5]</span><br><span class="line">assert a[5:] == a[5:len(a)]</span><br></pre></td></tr></table></figure><ul><li><p>슬라이싱한 결과는 완전히 새로운 리스트 원래 리스트는 그대로 유지 된다.</p></li><li><p>슬라이싱한 리스트를 변경해도 원래 리스트는 변경되지 않는다</p></li><li><p>슬라이싱 할때는 간결하게 하라 시작 인덱스에 0을 넣거나, 끝 인덱스에 시퀀스 길이를 넣지 말라</p></li></ul><h2 id="Better-way-12-스트라이드와-슬라이스를-한-식에-함께-사용하지-말라"><a href="#Better-way-12-스트라이드와-슬라이스를-한-식에-함께-사용하지-말라" class="headerlink" title="Better way 12 스트라이드와 슬라이스를 한 식에 함께 사용하지 말라"></a>Better way 12 스트라이드와 슬라이스를 한 식에 함께 사용하지 말라</h2><ul><li><p>파이썬은 리스트[시작:끝:증가값]으로 일정한 간격을 두고 슬라이싱을 할 수 있는 특별한 구문을 제공한다</p></li><li><p>슬라이스에 시작, 끝, 증가값을 함께 지정하면 코드의 의미를 혼동하기 쉽다</p></li><li><p>시작이나 끝 인덱스가 없는 슬라이스를 만들 때는 양수 증가값을 사용하라 가급적 음수 증가값을 피하라</p></li><li><p>한슬라이스 안에서 시작, 끝, 증가값을 함께 사용하지 말라 세 파라미터를 모두 써야 하는 경우 두번 대입을<br>  사용하거나 itertools.islice를 사용하라</p></li></ul><h2 id="Better-way-13-슬라이싱보다는-나머지를-모두-잡아내는-언패킹을-사용하라"><a href="#Better-way-13-슬라이싱보다는-나머지를-모두-잡아내는-언패킹을-사용하라" class="headerlink" title="Better way 13 슬라이싱보다는 나머지를 모두 잡아내는 언패킹을 사용하라"></a>Better way 13 슬라이싱보다는 나머지를 모두 잡아내는 언패킹을 사용하라</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">car_ages = [<span class="number">0</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">20</span>, <span class="number">19</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">15</span>]</span><br><span class="line">car_ages_descending = <span class="built_in">sorted</span>(car_ages, reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>아래 코드는 시작적으로 노이즈가 많다<br>그래서 인덱스에 대한 오류를 만들어 내기 쉽다</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">oldest = car_ages_descending[<span class="number">0</span>]</span><br><span class="line">second_oldest = car_ages_descending[<span class="number">1</span>]</span><br><span class="line">ohers = car_ages_descending[<span class="number">2</span>:]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>이런 상황을 더 잘 다룰 수 있도록 파이썬은 별표식을 사용해 모든 값을 담는 언패킹을 할 수 있게 지원 한다</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oldest, second_oldest, *others = car_ages_descending</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>별표식은 다른 위치에도 쓸수 있다</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oldest, *others, youngest = car_ages_descending</span><br><span class="line">*others, second_youngest, youngest = car_ages_descending</span><br></pre></td></tr></table></figure><p>한 수준의 언패킹에 별표식 2개 이상 쓸수 없다</p><p>별표식은 항상 list 인스턴스가 된다</p><h2 id="Better-way-14-복잡한-기준을-사용해-정렬할-때는-key-파라미터를-사용하라"><a href="#Better-way-14-복잡한-기준을-사용해-정렬할-때는-key-파라미터를-사용하라" class="headerlink" title="Better way 14 복잡한 기준을 사용해 정렬할 때는 key 파라미터를 사용하라"></a>Better way 14 복잡한 기준을 사용해 정렬할 때는 key 파라미터를 사용하라</h2><ul><li>리스트 타입에 들어 있는 sort 메서드를 사용하면 원소 타입이 문자열 정수 튜플 등로가 같은 내장 타입인 경우 자연스러운 순서로 리스트의 원소를 정렬할 수 있다</li><li>원소 타입에 특별 메서드를 통해 자연스러운 순서가 정의되 있지 않으면 sort 메서드는 예외를 일으킨다</li><li>sort 메서드에서 key 파라미터를 사용하면 리스트의 각 원소 대신 비교에 사용할 객체를 반환하는 도우미 함수를 제공할 수 있다</li><li>key 함수에서 튜플을 반환하면 여러 정렬 기준을 하나로 역을수 있다</li></ul><h2 id="Better-way-15-딕셔너리-삽입-순서에-의존할-때는-조심하라"><a href="#Better-way-15-딕셔너리-삽입-순서에-의존할-때는-조심하라" class="headerlink" title="Better way 15 딕셔너리 삽입 순서에 의존할 때는 조심하라"></a>Better way 15 딕셔너리 삽입 순서에 의존할 때는 조심하라</h2><ul><li>파이썬 3.6 이전에는 딕셔너리의 순서가 삽입 순서에 의존하지 않는다는 사실을 명시적으로 문서화하지 않았다</li><li>파이썬 3.7부터는 딕셔너리의 순서가 삽입 순서에 의존한다는 사실을 문서화했다</li><li>파이썬은 dict는 아니지만 딕셔너리와 비슷한 객체를 쉽게 만들 수 있게 해준다</li><li>딕셔너리와 비슷한 클래스를 조심스럽게 다루는 방법은 <ul><li>dict 인스턴스의 삽입 순서에 의존하지 않는 방법</li><li>실행 시점에 명시적으로 타입검사를 하는것</li><li>타입 애너테이션과 정적 분석을 사용해 dict 값을 요구하는 법</li></ul></li></ul><h2 id="Better-way-16-in을-사용하고-딕셔너리-키가-없을-때-KeyError를-처리하기보다는-get을-사용하라"><a href="#Better-way-16-in을-사용하고-딕셔너리-키가-없을-때-KeyError를-처리하기보다는-get을-사용하라" class="headerlink" title="Better way 16 in을 사용하고 딕셔너리 키가 없을 때 KeyError를 처리하기보다는 get을 사용하라"></a>Better way 16 in을 사용하고 딕셔너리 키가 없을 때 KeyError를 처리하기보다는 get을 사용하라</h2><ul><li>딕셔너리가 없는 경우를 처리하는 방법으로 in식을 사용하는 방법 keyError를 처리하는 방법으로 get을 사용하는 방법이 있다</li><li>카운터와 같이 기본적인 타입의 값이 들어가는 딕셔너리를 다룰 때는 get 메서드를 사용하는 것이 더 깔끔하다</li><li>setdefault 메서드 대신 defaultdict를 사용하면 더 간결하게 딕셔너리를 다룰 수 있다</li></ul><h2 id="Better-way-17-내부-상태에서-원소가-없는-경우를-처리할-때는-setdefault보다-defaultdict를-사용하라"><a href="#Better-way-17-내부-상태에서-원소가-없는-경우를-처리할-때는-setdefault보다-defaultdict를-사용하라" class="headerlink" title="Better way 17 내부 상태에서 원소가 없는 경우를 처리할 때는 setdefault보다 defaultdict를 사용하라"></a>Better way 17 내부 상태에서 원소가 없는 경우를 처리할 때는 setdefault보다 defaultdict를 사용하라</h2><ul><li>키로 어떤 값이 들어올지 모르는 딕셔너리를 관리해야 하는데 collections.defaultdict를 사용하면 편리하다</li><li>임의의 키가 들어 있는 딕셔너리가 여러분에게 전달됐고 그 딕셔너리가 어떻게 생성됐는지 모르는 경우, defaultdict를 사용하면 편리하다</li></ul><h2 id="Better-way-18-missing-을-사용해-키에-따라-다른-디폴트-값을-생성하는-방법을-알아두라"><a href="#Better-way-18-missing-을-사용해-키에-따라-다른-디폴트-값을-생성하는-방법을-알아두라" class="headerlink" title="Better way 18 __missing__을 사용해 키에 따라 다른 디폴트 값을 생성하는 방법을 알아두라"></a>Better way 18 __missing__을 사용해 키에 따라 다른 디폴트 값을 생성하는 방법을 알아두라</h2><ul><li>디폴드 값을 만드는 계산 비용이 높거나 만드는 과정에서 예외가 발생할 수 있는 상황에서는 dict의 setdefault 메서드를 사용하지 말라</li><li>디폴트 키를 만들때 어떤 키를 사용했는지 반드시 알아야 하는 상황이라면 직접 dict의 하위 클래스와 <strong>missing</strong> 메서드를 정의하라</li></ul><h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr><ul><li><a href="https://www.yes24.com/Product/Goods/94197582">Effective Python 2nd 파이썬 코딩의 기술(개정2판) 똑똑하게 코딩하는 법</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2024/10/2024-10-18-Effective_Python_CHAPTER_2/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CHAPTER 1 파이썬다운 생각</title>
      <link>https://sejoung.github.io/2024/10/2024-10-15-Effective_Python_CHAPTER_1/</link>
      <guid>https://sejoung.github.io/2024/10/2024-10-15-Effective_Python_CHAPTER_1/</guid>
      <pubDate>Tue, 15 Oct 2024 13:40:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;1장-파이썬답게-생각하기&quot;&gt;&lt;a href=&quot;#1장-파이썬답게-생각하기&quot; class=&quot;headerlink&quot; title=&quot;1장 파이썬답게 생각하기&quot;&gt;&lt;/a&gt;1장 파이썬답게 생각하기&lt;/h1&gt;&lt;p&gt;가장 파이썬 다운 방식을 알아야 된다&lt;/p&gt;
&lt;
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="1장-파이썬답게-생각하기"><a href="#1장-파이썬답게-생각하기" class="headerlink" title="1장 파이썬답게 생각하기"></a>1장 파이썬답게 생각하기</h1><p>가장 파이썬 다운 방식을 알아야 된다</p><h2 id="Better-way-1-사용-중인-파이썬의-버전을-알아두라"><a href="#Better-way-1-사용-중인-파이썬의-버전을-알아두라" class="headerlink" title="Better way 1 사용 중인 파이썬의 버전을 알아두라"></a>Better way 1 사용 중인 파이썬의 버전을 알아두라</h2><p>파이썬 2는 수명이 다됨 백포팅이 이루어지지 않음</p><ul><li>backporting: 새로운 기능을 이전 버전에서도 사용할 수 있게 하는 것</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">print(sys.version_info)</span><br><span class="line">print(sys.version)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Better-way-2-PEP-8-스타일-가이드를-따르라"><a href="#Better-way-2-PEP-8-스타일-가이드를-따르라" class="headerlink" title="Better way 2 PEP 8 스타일 가이드를 따르라"></a>Better way 2 PEP 8 스타일 가이드를 따르라</h2><p>파이썬 개선 제안(Python Enhancement Proposal) 8은 파이썬 코드를 어떻게 구성할지에 대한 스타일 가이드를 제공한다.</p><h3 id="공백-Whitespace"><a href="#공백-Whitespace" class="headerlink" title="공백(Whitespace)"></a>공백(Whitespace)</h3><ul><li>탭 대신 스페이스를 사용하라</li><li>문법적으로 의미 있는 들여쓰기는 4개의 스페이스로 한다</li><li>한 줄의 문자 길이가 79자 이하여야 한다</li><li>긴 식을 다음 줄에 이어서 쓸 때는 일반적인 들여쓰기 보다 4개의 스페이스를 더 들여써야 한다</li><li>파일에서 함수와 클래스는 빈 줄 두개로 구분해야 한다</li><li>딕셔너리(dictionsry)에서 키와 콜론(:) 사이에는 공백을 넣지 않고 한 줄 안에 키와 값을 같이 넣는 경우에는 콜론 다음에 스페이스를 하나 넣는다</li><li>변수 대입에서 &#x3D; 전후에는 스페이스를 하나씩만 넣는다</li><li>타입 표기를 덧붙이는 경우에는 변수 이름과 콜론사이에 공백을 넣지 않도록 주의하고 콜론과 타입 정보 사이에는 스페이스를 하나 넣어라</li></ul><h3 id="명명-Naming"><a href="#명명-Naming" class="headerlink" title="명명(Naming)"></a>명명(Naming)</h3><ul><li>함수, 변수, 속성은 lowercase_underscore 형식을 따른다</li><li>보호(protected) 인스턴스 속성은 _leading_underscore 형식을 따른다</li><li>비공개(private) 인스턴스 속성은 __double_leading_underscore 형식을 따른다</li><li>클래스와 예외는 CapitalizedWord 형식을 따른다</li><li>모듈 수준 상수는 ALL_CAPS 형식을 따른다</li><li>클래스의 인스턴스 메서드에서 첫 번째 파라미터의 이름은 self를 사용한다</li><li>클래스 메서드에서 첫 번째 파라미터의 이름은 cls를 사용한다</li></ul><h3 id="표현식과-문장-Expressions-and-Statements"><a href="#표현식과-문장-Expressions-and-Statements" class="headerlink" title="표현식과 문장(Expressions and Statements)"></a>표현식과 문장(Expressions and Statements)</h3><ul><li>긍정적인 표현식을 부정하지 말고(if not a is b) 부정을 내부에 넣어라 (if a is not b)</li><li>빈 컨테이너(container)나 시퀀스(sequence)를 검사할 때 길이를 (if len(somelist)&#x3D;&#x3D;0)와 비교하지 말고 if not somelist 와 같이 사용하라<ul><li>빈 값은 암시적으로 False로 평가된다</li></ul></li><li>비어 있지 않을때로 길이로 비교하지 말라<ul><li>비어 있지 않은 값은 암시적으로 True로 평가된다</li></ul></li><li>한줄로 된 if 문, for와 while 루프, except 복합문을 쓰지 말고 여러 줄로 나눠서 쓰라</li><li>식을 한 줄에 넣을 때는 식을 괄호로 둘러싸고 줄바꿈과 들여쓰기를 추가해서 읽기 쉽게 만들어라</li><li>여러줄에 걸쳐 식을 쓸 때는 줄이 계속된다는 표시로 \ 문자보단 괄호를 사용하라</li></ul><h3 id="임포트-Imports"><a href="#임포트-Imports" class="headerlink" title="임포트(Imports)"></a>임포트(Imports)</h3><ul><li>import 문은 항상 파일 맨 위에 위치해야 한다</li><li>모듈을 임포트할 때는 항상 모듈의 절대 이름을 사용하고 현재 모듈의 경로를 기준으로 상대 경로로 된 이름을 사용하지 않는다<ul><li>from bar import foo 라고 해야 되며 import foo 라고 하면 안된다</li></ul></li><li>반듯이 상대적인 경로로 임포트해야 하는 경우에는 from . import foo 라고 해야 한다</li><li>임포트는 ‘표준 라이브러리 모듈, 서드파티 모듈, 자신이 만든 모듈’ 순으로 구분해야 한다</li></ul><h2 id="Better-way-3-bytes와-str의-차이를-알아두라"><a href="#Better-way-3-bytes와-str의-차이를-알아두라" class="headerlink" title="Better way 3 bytes와 str의 차이를 알아두라"></a>Better way 3 bytes와 str의 차이를 알아두라</h2><p>문자열 데이터의 시퀀스를 표현하는 두 가지 타입</p><ul><li>bytes: 8비트 값을 연속된 시퀀스로 나타낸다</li><li>str: 유니코드 문자를 연속된 시퀀스로 나타낸다</li></ul><p>str 인스턴스에는 직접 대응하는 이진 인코딩이 없고 bytes 인스턴스에는 직접 대응하는 텍스트 인코딩이 없다</p><p>유니코드 샌드위치 : 유니코드 데이터를 인코딩 하거나 디코딩하는 부분을 인터페이스의 가장 먼 경계에 두는 것</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def to_str(bytes_or_str):</span><br><span class="line">    if isinstance(bytes_or_str, bytes):</span><br><span class="line">        value = bytes_or_str.decode(&#x27;utf-8&#x27;)</span><br><span class="line">    else:</span><br><span class="line">        value = bytes_or_str</span><br><span class="line">    return value</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Better-way-4-C-스타일-형식-문자열을-str-format과-쓰기보다는-f-문자열을-통한-인터폴레이션을-사용하라"><a href="#Better-way-4-C-스타일-형식-문자열을-str-format과-쓰기보다는-f-문자열을-통한-인터폴레이션을-사용하라" class="headerlink" title="Better way 4 C 스타일 형식 문자열을 str.format과 쓰기보다는 f-문자열을 통한 인터폴레이션을 사용하라"></a>Better way 4 C 스타일 형식 문자열을 str.format과 쓰기보다는 f-문자열을 통한 인터폴레이션을 사용하라</h2><ul><li>f-문자열: 문자열 앞에 f를 붙이면 문자열 안에 중괄호로 변수를 감싸면 변수의 값을 참조할 수 있다</li><li>f-문자열은 가독성이 좋고 str.format보다 간결하다</li><li>% 연산자를 사용한 C 스타일 형식 문자열은 가독성이 떨어지고 f-문자열보다 더 복잡하다</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">place = 3</span><br><span class="line">number = 1.23456</span><br><span class="line"></span><br><span class="line">print(f&#x27;내가 고른 숫자? &#123;number:.&#123;place&#125;f&#125;&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Better-way-5-복잡한-식을-쓰는-대신-도우미-함수를-작성하라"><a href="#Better-way-5-복잡한-식을-쓰는-대신-도우미-함수를-작성하라" class="headerlink" title="Better way 5 복잡한 식을 쓰는 대신 도우미 함수를 작성하라"></a>Better way 5 복잡한 식을 쓰는 대신 도우미 함수를 작성하라</h2><ul><li>복잡한 식을 작성할 때는 식을 작성하는 도우미 함수를 작성하라</li><li>boolean 연산자나 or, and를 식에 사용하는 것 보다 if&#x2F;else 문을 사용하는 것이 가독성이 좋다</li></ul><h2 id="Better-way-6-인덱스를-사용하는-대신-대입을-사용해-데이터를-언패킹하라"><a href="#Better-way-6-인덱스를-사용하는-대신-대입을-사용해-데이터를-언패킹하라" class="headerlink" title="Better way 6 인덱스를 사용하는 대신 대입을 사용해 데이터를 언패킹하라"></a>Better way 6 인덱스를 사용하는 대신 대입을 사용해 데이터를 언패킹하라</h2><p>파이썬에는 값으로 이뤄진 불변 순서쌍을 만들어낼 수 있는 tuple이 있다</p><p>튜플이 만들어지면 인덱스를 통해 새 값을 대입해서 튜플을 변경할수는 없다</p><p>언패킹</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">item = (&#x27;apple&#x27;, 5)</span><br><span class="line">name, count = item</span><br><span class="line">print(name, count)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def bubble_sort(a):</span><br><span class="line">    for _ in range(len(a)):</span><br><span class="line">        for i in range(1, len(a)):</span><br><span class="line">            if a[i] &lt; a[i-1]:</span><br><span class="line">                a[i-1], a[i] = a[i], a[i-1]</span><br><span class="line">                </span><br><span class="line">names = [&#x27;pretzels&#x27;, &#x27;carrots&#x27;, &#x27;arugula&#x27;, &#x27;bacon&#x27;]</span><br><span class="line">bubble_sort(names)</span><br><span class="line">print(names)</span><br><span class="line"># [&#x27;arugula&#x27;, &#x27;bacon&#x27;, &#x27;carrots&#x27;, &#x27;pretzels&#x27;]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for rank, (name, count) in enumerate(results, 1):</span><br><span class="line">    print(f&#x27;&#123;rank&#125;: &#123;name&#125; -&gt; &#123;count&#125;&#x27;)</span><br></pre></td></tr></table></figure><h2 id="Better-way-7-range보다는-enumerate를-사용하라"><a href="#Better-way-7-range보다는-enumerate를-사용하라" class="headerlink" title="Better way 7 range보다는 enumerate를 사용하라"></a>Better way 7 range보다는 enumerate를 사용하라</h2><p>range 함수는 어떤 정수의 집합을 이터레이션 하는 루프가 필요 할 때 유용하다</p><p>enumerate 함수는 이터레이터를 감싸서 이터레이터가 생성하는 각 아이템의 값과 인덱스를 함께 반환한다</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">flavors = [&#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;]</span><br><span class="line"></span><br><span class="line">it = enumerate(flavors)</span><br><span class="line">print(next(it))</span><br><span class="line">print(next(it))</span><br><span class="line">print(next(it))</span><br><span class="line"></span><br><span class="line">for i, flavor in enumerate(flavors):</span><br><span class="line">    print(f&#x27;&#123;i+1&#125;: &#123;flavor&#125;&#x27;)</span><br><span class="line">    </span><br><span class="line">for i, flavor in enumerate(flavors, 1):</span><br><span class="line">    print(f&#x27;&#123;i&#125;: &#123;flavor&#125;&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Better-way-8-여러-이터레이터에-대해-나란히-루프를-수행하려면-zip을-사용하라"><a href="#Better-way-8-여러-이터레이터에-대해-나란히-루프를-수행하려면-zip을-사용하라" class="headerlink" title="Better way 8 여러 이터레이터에 대해 나란히 루프를 수행하려면 zip을 사용하라"></a>Better way 8 여러 이터레이터에 대해 나란히 루프를 수행하려면 zip을 사용하라</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for name, count in zip(names, counts):</span><br><span class="line">    print(name, count)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>zip 제너레이션은 각 이터레이터의 다음값이 있는 튜플을 반환한다</p><p>zip은 입력 이터레이터들 중 가장 짧은 이터레이터가 끝날 때 멈춘다</p><h2 id="Better-way-9-for나-while-루프-뒤에-else-블록을-사용하지-말라"><a href="#Better-way-9-for나-while-루프-뒤에-else-블록을-사용하지-말라" class="headerlink" title="Better way 9 for나 while 루프 뒤에 else 블록을 사용하지 말라"></a>Better way 9 for나 while 루프 뒤에 else 블록을 사용하지 말라</h2><p>파이썬은 루프가 완전히 실행되고 나면 else 블록을 실행한다</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i in range(3):</span><br><span class="line">    print(&#x27;Loop&#x27;, i)</span><br><span class="line">else:</span><br><span class="line">    print(&#x27;Else block!&#x27;)    </span><br></pre></td></tr></table></figure><p>하지만 break 문을 사용하면 else 블록은 실행되지 않는다</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for i in range(3):</span><br><span class="line">    print(&#x27;Loop&#x27;, i)</span><br><span class="line">    if i == 1:</span><br><span class="line">        break</span><br><span class="line">else:</span><br><span class="line">    print(&#x27;Else block!&#x27;)</span><br><span class="line">    </span><br></pre></td></tr></table></figure><p>또 빈 시퀀스에 대한 루프를 실행할 때도 else 블록이 실행된다</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">for x in []:</span><br><span class="line">    print(&#x27;Never runs&#x27;)</span><br><span class="line">else:</span><br><span class="line">    print(&#x27;For Else block!&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>while 루프에도 값이 false면 바로 else 블록을 실행한다</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while False:</span><br><span class="line">    print(&#x27;Never runs&#x27;)</span><br><span class="line">else:</span><br><span class="line">    print(&#x27;While Else block!&#x27;)</span><br></pre></td></tr></table></figure><p>위처럼 동작이 직관적이지 않고 혼동을 야기 할수 있다</p><h2 id="Better-way-10-대입식을-사용해-반복을-피하라"><a href="#Better-way-10-대입식을-사용해-반복을-피하라" class="headerlink" title="Better way 10 대입식을 사용해 반복을 피하라"></a>Better way 10 대입식을 사용해 반복을 피하라</h2><p>assignment expression이며 walrus operator라고도 한다</p><ul><li>대입식에서는 왈러스 연산자(:&#x3D;)를 사용해서 변수에 값을 대입하면서 동시에 이 값을 평가할 수 있고 중복을 줄일수 있다</li><li>대입식이 더 큰 식의 일부분으로 쓰일 때는 괄호로 둘러싸야 한다</li><li>파이썬에서는 switch&#x2F;case 문이나 do&#x2F;while 루프가 없지만 대입식을 활용해서 이방법을 깔끔하게 흉내 낼수 있다</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bottles = []</span><br><span class="line">while fresh_fruit := pick_fruit():</span><br><span class="line">    bottles.append(fresh_fruit.make_juice())</span><br></pre></td></tr></table></figure><h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr><ul><li><a href="https://www.yes24.com/Product/Goods/94197582">Effective Python 2nd 파이썬 코딩의 기술(개정2판) 똑똑하게 코딩하는 법</a></li><li><a href="https://peps.python.org/pep-0008/">PEP 8 – Style Guide for Python Code</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2024/10/2024-10-15-Effective_Python_CHAPTER_1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Flux Lora 훈련하기</title>
      <link>https://sejoung.github.io/2024/10/2024-10-11-train_lora_flux/</link>
      <guid>https://sejoung.github.io/2024/10/2024-10-11-train_lora_flux/</guid>
      <pubDate>Fri, 11 Oct 2024 01:30:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Flux-Lora-훈련하기&quot;&gt;&lt;a href=&quot;#Flux-Lora-훈련하기&quot; class=&quot;headerlink&quot; title=&quot;Flux Lora 훈련하기&quot;&gt;&lt;/a&gt;Flux Lora 훈련하기&lt;/h1&gt;&lt;p&gt;관찮은 글 모음&lt;/p&gt;
&lt;h1 id=&quot;참
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Flux-Lora-훈련하기"><a href="#Flux-Lora-훈련하기" class="headerlink" title="Flux Lora 훈련하기"></a>Flux Lora 훈련하기</h1><p>관찮은 글 모음</p><h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr><ul><li><a href="https://github.com/bghira/SimpleTuner">SimpleTuner</a></li><li><a href="https://github.com/ostris/ai-toolkit">ai-toolkit</a></li><li><a href="https://www.youtube.com/watch?v=se3qpLkJnrk">Kasucast #26 - FLUX.1 [dev]: Training LoRA with SimpleTuner and AI-Toolkit</a></li><li><a href="https://stability.ai/learning-hub/stable-diffusion-3-medium-fine-tuning-tutorial">Stable Diffusion 3 Medium Fine-tuning Tutorial</a></li><li><a href="https://github.com/Nerogar/OneTrainer">OneTrainer</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2024/10/2024-10-11-train_lora_flux/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Lora 훈련하기</title>
      <link>https://sejoung.github.io/2024/09/2024-09-09-train_lora/</link>
      <guid>https://sejoung.github.io/2024/09/2024-09-09-train_lora/</guid>
      <pubDate>Mon, 09 Sep 2024 04:51:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Lora-훈련하기&quot;&gt;&lt;a href=&quot;#Lora-훈련하기&quot; class=&quot;headerlink&quot; title=&quot;Lora 훈련하기&quot;&gt;&lt;/a&gt;Lora 훈련하기&lt;/h1&gt;&lt;p&gt;괜찮은 스레드들의 모음&lt;/p&gt;
&lt;h1 id=&quot;참조&quot;&gt;&lt;a href=&quot;#참조&quot;
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Lora-훈련하기"><a href="#Lora-훈련하기" class="headerlink" title="Lora 훈련하기"></a>Lora 훈련하기</h1><p>괜찮은 스레드들의 모음</p><h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr><ul><li><a href="https://civitai.com/articles/3921/this-is-how-i-create-and-train-loras">This is how I create and train LoRAs</a></li><li><a href="https://civitai.com/articles/6824/training-a-flux-character-lora-on-civitai">Training a Flux Character LoRA on Civitai</a></li><li><a href="https://civitai.com/articles/6792/flux-style-captioning-differences-training-diary">Flux Style Captioning Differences - Training Diary</a></li><li><a href="https://civitai.com/articles/7097/flux-complete-lora-settings-and-dataset-guide-post-mortem-of-two-weeks-of-learning">Flux complete Lora settings and dataset guide - post-mortem of two weeks of learning</a></li><li><a href="https://civitai.com/articles/6868/flux-character-caption-differences-training-diary">Flux Character Caption Differences - Training Diary</a></li><li><a href="https://civitai.com/articles/7146/flux-style-captioning-differences-pt2-4-new-caption-tools-training-diary">Flux Style Captioning Differences pt2. - 4 new caption tools - Training Diary</a></li><li></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2024/09/2024-09-09-train_lora/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CHAPTER 19 Critique: 구글의 코드 리뷰 도구</title>
      <link>https://sejoung.github.io/2024/08/2024-08-22-Software_Engineering_at_Google_CHAPTER_19/</link>
      <guid>https://sejoung.github.io/2024/08/2024-08-22-Software_Engineering_at_Google_CHAPTER_19/</guid>
      <pubDate>Thu, 22 Aug 2024 00:21:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Part-IV-도구&quot;&gt;&lt;a href=&quot;#Part-IV-도구&quot; class=&quot;headerlink&quot; title=&quot;Part IV 도구&quot;&gt;&lt;/a&gt;Part IV 도구&lt;/h1&gt;&lt;h2 id=&quot;CHAPTER-19-Critique-구글의-코드-리뷰-도구&quot;
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Part-IV-도구"><a href="#Part-IV-도구" class="headerlink" title="Part IV 도구"></a>Part IV 도구</h1><h2 id="CHAPTER-19-Critique-구글의-코드-리뷰-도구"><a href="#CHAPTER-19-Critique-구글의-코드-리뷰-도구" class="headerlink" title="CHAPTER 19 Critique: 구글의 코드 리뷰 도구"></a>CHAPTER 19 Critique: 구글의 코드 리뷰 도구</h2><p>코드 리뷰는 소프트웨어 개발에서 없어서는 안 될 요소입니다.<br>특히 성장하기 위해 매우 중요한 요소입니다.</p><h3 id="코드-리뷰-도구-원칙"><a href="#코드-리뷰-도구-원칙" class="headerlink" title="코드 리뷰 도구 원칙"></a>코드 리뷰 도구 원칙</h3><ul><li>간결성<ul><li>가장 큰 영향을 준 원칙</li></ul></li><li>신뢰 제공</li><li>익숙한 소통 방식</li><li>워크플로 통합</li></ul><h3 id="코드-리뷰-흐름"><a href="#코드-리뷰-흐름" class="headerlink" title="코드 리뷰 흐름"></a>코드 리뷰 흐름</h3><ul><li>변경 생성</li><li>리뷰 요청</li><li>변경 이해하고 댓글 달기</li><li>변경 수정 및 댓글에 답하기</li><li>변경 승인</li><li>변경 커밋</li></ul><h3 id="1단계-변경-생성"><a href="#1단계-변경-생성" class="headerlink" title="1단계: 변경 생성"></a>1단계: 변경 생성</h3><ul><li>디프, 차이점 보여주기</li><li>분석 결과</li><li>긴밀한 도구 통합<h3 id="2단계-리뷰-요청"><a href="#2단계-리뷰-요청" class="headerlink" title="2단계: 리뷰 요청"></a>2단계: 리뷰 요청</h3>리뷰 요청 할때 리뷰어 선정이 어려운데 별칭으로 리뷰어를 선정하는 방법이 있다.</li></ul><h3 id="3-4단계-변경-이해하고-댓글-달기"><a href="#3-4단계-변경-이해하고-댓글-달기" class="headerlink" title="3~4단계: 변경 이해하고 댓글 달기"></a>3~4단계: 변경 이해하고 댓글 달기</h3><ul><li><p>댓글달기</p></li><li><p>변경의 상태 이해하기</p><ul><li>누구 차례 기능</li><li>대시보드 와 검색 시스템<h3 id="5단계-변경-승인-변경에-점수-매기기"><a href="#5단계-변경-승인-변경에-점수-매기기" class="headerlink" title="5단계: 변경 승인(변경에 점수 매기기)"></a>5단계: 변경 승인(변경에 점수 매기기)</h3>구글에서 변경에 점수를 매길 때 고려하는 요소</li></ul></li><li><p>LGTM</p></li><li><p>승인</p></li><li><p>미해결 댓글 개수</p></li></ul><h3 id="6단계-변경-커밋"><a href="#6단계-변경-커밋" class="headerlink" title="6단계: 변경 커밋"></a>6단계: 변경 커밋</h3><ul><li>커밋 후 : 뱐걍 이력 추적</li></ul><h1 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h1><hr><ul><li><a href="https://www.yes24.com/Product/Goods/109182479">구글 엔지니어는 이렇게 일한다</a></li></ul>]]></content:encoded>
      
      <comments>https://sejoung.github.io/2024/08/2024-08-22-Software_Engineering_at_Google_CHAPTER_19/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
