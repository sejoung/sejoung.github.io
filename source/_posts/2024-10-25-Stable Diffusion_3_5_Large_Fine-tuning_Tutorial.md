---
layout: post
title: "Stable Diffusion 3.5 Large Fine-tuning Tutorial"
date: 2024-10-25 15:30 +0900
comments: true
tags: [ "LORA","Lora", "í›ˆë ¨", "í•™ìŠµ" ,"SD 3.5", "Large", "Fine-tuning", "Tutorial","stable diffusion"]
categories: [ "Machine Learning" ]
sitemap:
    changefreq: daily
    priority: 1.0
---
# Stable Diffusion 3.5 Large Fine-tuning Tutorial

ì´ê¸€ì€ [Stable Diffusion 3.5 Large Fine-tuning Tutorial](https://stabilityai.notion.site/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6)
ê¸€ì„ ë²ˆì—­í•œ ê¸€ì…ë‹ˆë‹¤ ì´ë¯¸ì§€ëŠ” ë”°ë¡œ ì²¨ë¶€ í•˜ì§€ ì•Šìœ¼ë©° í•„ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” ë¶€ë¶„ë§Œ ë²ˆì—­í•©ë‹ˆë‹¤

# ëŒ€ìƒ : ë¯¸ì„¸ ì¡°ì •ì— ëŒ€í•œ ìµœì†Œí•œì˜ ê¸°ë³¸ ì§€ì‹ì„ ê°–ì¶˜ ì—”ì§€ë‹ˆì–´ ë˜ëŠ” ê¸°ìˆ  ì¸ë ¥

ëª©ì : SD1.5/SDXLê³¼ Stable Diffusion 3 Medium/Large(SD3.5M/L) ë¯¸ì„¸ ì¡°ì • ê°„ì˜ ì°¨ì´ì ì„ ì´í•´í•˜ê³  ë” ë§ì€ ì‚¬ìš©ìê°€ ë‘ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

## Tools

[SimpleTuner](https://github.com/bghira/SimpleTuner) toolkit

## Environment Setup

í™˜ê²½ ì„¤ì •ì€ ì—¬ì „íˆ ì´ì „ê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, ì´ì „ ê²Œì‹œë¬¼ ì´í›„ SimpleTunerì˜ êµ¬ì„±ì—ëŠ” **ë§ì€** ë³€ê²½ì´ ìˆì—ˆìŠµë‹ˆë‹¤. 
ê°€ëŠ¥í•œ í•œ ì´ ì‘ì—…ì„ ê°„ì†Œí™”í•˜ë ¤ê³  ë…¸ë ¥í•˜ê² ì§€ë§Œ ì´ì „ `config.env` íŒŒì¼ê³¼ ìƒˆë¡œìš´ `config.env` ë° `config.json`ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜í–ˆìŠµë‹ˆë‹¤.
[ì—¬ê¸°](https://github.com/bghira/SimpleTuner/blob/main)ì— ì§€ì •ëœ [configure.py](https://github.com/bghira/SimpleTuner/blob/main/configure.py) ë°©ë²•ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. /documentation/quickstart/SD3.md)ë¥¼ ì°¸ì¡°í•˜ì—¬ ê²°ê³¼ íŒŒì¼ì´ ë¬´ì—‡ì„ ì œê³µí•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

[**âš ï¸](https://emojipedia.org/warning)** Just a note of warning, if youâ€™d like to use your [old](https://www.notion.so/17f90df74bce4c62a295849f0dc8fb7e?pvs=21) `config.env` files, youâ€™ll have to do some slight tweaking. Iâ€™ll cover it later in this [section](https://www.notion.so/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6?pvs=21).

If you want to see the full list of options available, you can check the [OPTIONS.MD](https://github.com/bghira/SimpleTuner/blob/main/OPTIONS.md#environment-configuration-variables) file.

- Sample `.json` generated with `configure.py` (used as a reference)

```json
    {
        "--resume_from_checkpoint": "latest",
        "--data_backend_config": "config/multidatabackend.json",
        "--aspect_bucket_rounding": 2,
        "--seed": 42,
        "--minimum_image_size": 0,
        "--disable_benchmark": false,
        "--output_dir": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/",
        "--lora_type": "standard",
        "--lora_rank": 256,
        "--max_train_steps": 24000,
        "--num_train_epochs": 0,
        "--checkpointing_steps": 400,
        "--checkpoints_total_limit": 60,
        "--tracker_project_name": "sd35-training",
        "--tracker_run_name": "simpletuner-sd35-large-fantasy-art-01",
        "--report_to": "wandb",
        "--model_type": "lora",
        "--pretrained_model_name_or_path": "stabilityai/stable-diffusion-3.5-large",
        "--model_family": "sd3",
        "--train_batch_size": 6,
        "--gradient_checkpointing": "true",
        "--caption_dropout_probability": 0.0,
        "--resolution_type": "pixel_area",
        "--resolution": "1024",
        "--validation_seed": "42",
        "--validation_steps": "35",
        "--validation_resolution": "1024x1024",
        "--validation_guidance": "7.5",
        "--validation_guidance_rescale": "0.0",
        "--validation_num_inference_steps": "35",
        "--validation_prompt": "k4s4, a waist up view of a beautiful blonde woman, green eyes",
        "--mixed_precision": "bf16",
        "--optimizer": "adamw_bf16",
        "--learning_rate": "1.05e-3",
        "--lr_scheduler": "polynomial",
        "--lr_warmup_steps": "2400",
        "--validation_torch_compile": "false"
    }

```



ë˜í•œ, [ë¦´ë¦¬ìŠ¤ ë¸Œëœì¹˜](https://github.com/bghira/SimpleTuner/tree/release) ëŒ€ì‹  `SimpleTuner`ì˜ ìµœì‹  ë©”ì¸ ë¸Œëœì¹˜ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê°€ëŠ¥í•œ í•œ í˜„ì¬ê¹Œì§€. ì»¤ë°‹ í•´ì‹œ(2024ë…„ 10ì›” 15ì¼)ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
```
694784083c70bf81086bb3ceba86262b7b22757d
```

### Python Dependencies

ì¢…ì†ì„±ì„ ì„¤ì¹˜í•˜ë ¤ë©´ ì €ì¥ì†Œ í˜ì´ì§€ì—ì„œ SD3ìš© [ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ](https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/SD3.md)ë¥¼ ë”°ë¥´ì„¸ìš”. ì—¬ê¸°ì—ì„œë„ ì‚´í´ë³´ê³  ëŒ€ì²´ ì„¤ì¹˜ ë°©ë²•ë„ ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤. `SimpleTuner`(`12.4+`)ì™€ ì¼ì¹˜í•˜ëŠ” `CUDA` ë²„ì „ì´ ìˆëŠ” ê²½ìš° ì¢…ì†ì„± ì„¤ì¹˜ê°€ ë§¤ìš° ê°„ë‹¨í•  ìˆ˜ ìˆì§€ë§Œ `CUDA`ì˜ ì´ì „ ë²„ì „ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì¡°ê¸ˆ ë” ë³µì¡í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìš°ì„  ì €ì¥ì†Œë¥¼ `git clone`í•©ë‹ˆë‹¤.

```
git clone https://github.com/bghira/SimpleTuner.git
```

```
cd SimpleTuner
```


ë§ˆì§€ë§‰ìœ¼ë¡œ ìœ„ì—ì„œ ì–¸ê¸‰í•œ ì»¤ë°‹ í•´ì‹œë¥¼ í™•ì¸í•˜ì„¸ìš”. ë””ë²„ê¹…ì„ í•˜ê³  ì‹¶ë‹¤ë©´ ê³„ì†í•´ì„œ ë¶„ê¸°ë¥¼ ìƒì„±í•´ ë³´ê² ìŠµë‹ˆë‹¤('base_branch'ë¼ëŠ” ì´ë¦„, ììœ ë¡­ê²Œ ì´ë¦„ì„ ë°”ê¾¸ì„¸ìš”).

```
git checkout -b base_branch 694784083c70bf81086bb3ceba86262b7b22757d
```


```
git branch
```

ìƒˆ ì§€ì ì— ìˆìœ¼ë©´ Python ê°€ìƒ í™˜ê²½ì„ ë§Œë“¤ ì°¨ë¡€ì…ë‹ˆë‹¤. ì¢…ì†ì„±ì„ ì„¤ì¹˜í•  ë•Œ `python 3.11`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ê°ê° ë‹¤ìŒ ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ `OS` ë° `CUDA` í™˜ê²½ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
uname -a
```

```bash
nvcc --version
```

```bash
python --version
```


`SimpleTuner` ë””ë ‰í„°ë¦¬ì˜ ë£¨íŠ¸ì— ì´ ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ `virtualenv`ë¥¼ ë§Œë“­ë‹ˆë‹¤.

```
python -m venv .venv
```

Activate it with:

```
source .venv/bin/activate
```

ì™„ë£Œë˜ë©´ `poetry`(`pip` ë˜ëŠ” `uv`ì™€ ìœ ì‚¬í•œ ì¢…ì†ì„± ê´€ë¦¬ì)ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```
pip install -U poetry pip
```

`bghira`ëŠ” ì•ˆì „ì„ ìœ„í•´ ì´ ëª…ë ¹ì„ ì‹¤í–‰í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤:

```
poetry config virtualenvs.create false
```

ì €ëŠ” `Linux`ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒ ë‹¨ê³„ëŠ” ë‹¤ìŒ ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì¢…ì†ì„±ì„ ì„¤ì¹˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

```jsx
poetry install
```


ê·¸ëŸ¬ë‚˜ SD3.5 LargeëŠ” `diffusers`ì˜ íŠ¹ì • ì»¤ë°‹ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤(ì•„ë§ˆë„ ìµœì‹  ë²„ì „ë„ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤). ì´ [ì»¤ë°‹](https://github.com/huggingface/diffusers/commit/e2d037bbf1388fdc172458bed7a8a58b34fc6f84) ì´ìƒì´ í¬í•¨ëœ ë²„ì „ì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

```markdown
e2d037bbf1388fdc172458bed7a8a58b34fc6f84
```


ì´ëŠ” 'bghira'ë¡œ ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë©° ê·¸ì˜ íŒ€ì€ SimpleTuner ì €ì¥ì†Œë¥¼ ë§¤ìš° ë¹ ë¥´ê²Œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ë²„ì „ì˜ `diffusers`ë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´ `SimpleTuner` ë””ë ‰í„°ë¦¬ì˜ `pyproject.toml` íŒŒì¼ì„ ë³€ê²½í•˜ì—¬ ì˜¬ë°”ë¥¸ ì»¤ë°‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.


```toml
    [tool.poetry]
    name = "simpletuner"
    version = "1.1.0"
    description = "Stable Diffusion 2.x and XL tuner."
    authors = ["bghira"]
    license = "AGPLv3"
    readme = "README.md"
    package-mode = false
    
    [tool.poetry.dependencies]
    python = ">=3.10,<3.12"
    torch = { version = "2.4.1+cu124", source = "pytorch" }
    torchvision = { version = ">0.19", source = "pytorch" }
    diffusers = {git = "https://github.com/huggingface/diffusers", rev = "e2d037b"}
    transformers = "^4.45.1"
    datasets = "^3.0.1"
    bitsandbytes = "^0.44.1"
    wandb = "^0.18.2"
    requests = "^2.32.3"
    pillow = "^10.4.0"
    opencv-python = "^4.10.0.84"
    deepspeed = "^0.15.1"
    accelerate = "^0.34.2"
    safetensors = "^0.4.5"
    compel = "^2.0.1"
    clip-interrogator = "^0.6.0"
    open-clip-torch = "^2.26.1"
    iterutils = "^0.1.6"
    scipy = "^1.11.1"
    boto3 = "^1.35.24"
    pandas = "^2.2.3"
    botocore = "^1.35.24"
    urllib3 = "<1.27"
    torchaudio = "^2.4.1"
    triton-library = "^1.0.0rc4"
    torchsde = "^0.2.5"
    torchmetrics = "^1.1.1"
    colorama = "^0.4.6"
    numpy = "1.26"
    peft = "^0.12.0"
    tensorboard = "^2.17.1"
    triton = {version = "^3.0.0", source = "pytorch"}
    sentencepiece = "^0.2.0"
    optimum-quanto = {git = "https://github.com/huggingface/optimum-quanto"}
    lycoris-lora = {git = "https://github.com/kohakublueleaf/lycoris", rev = "dev"}
    torch-optimi = "^0.2.1"
    toml = "^0.10.2"
    fastapi = {extras = ["standard"], version = "^0.115.0"}
    torchao = {version = "^0.5.0+cu124", source = "pytorch"}
    lm-eval = "^0.4.4"
    nvidia-cudnn-cu12 = "*"
    nvidia-nccl-cu12 = "*"
    
    [build-system]
    requires = ["poetry-core", "setuptools", "wheel", "torch"]
    build-backend = "poetry.core.masonry.api"
    
    [[tool.poetry.source]]
    priority = "supplemental"
    name = "pytorch"
    url = "https://download.pytorch.org/whl/cu124"
    
```



ë³€ê²½ ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

Old

```toml
diffusers = {git = "https://github.com/huggingface/diffusers", rev = "quantization-config"}
```

New

```toml
diffusers = {git = "https://github.com/huggingface/diffusers", rev = "e2d037b"}
```

ì´ì œ í•„ìš”í•œ `SimpleTuner` ì¢…ì†ì„±ì´ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
- 
- [**ğŸš¨](https://emojipedia.org/police-car-light)** ì»´í“¨í„° í™˜ê²½ì—ì„œ 'CUDA 12.4' ì´ìƒì´ ì•„ë‹Œ ê²½ìš° 'SimpleTuner'ê°€ 'CUDA 12.4' ì´ìƒì´ë¼ëŠ” ê°€ì •í•˜ì— ì‘ë™í•˜ë¯€ë¡œ CUDA ì¢…ì†ì„± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ì„œ ì•Œì•„ì°¨ë¦¬ì…¨ë‹¤ë©´ ì €ëŠ” `CUDA 12.2`ë¥¼ ì‚¬ìš© ì¤‘ì´ì—ˆê³  `poetry install` ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
    - ì´ ë‹¨ë½ì„ í¼ì¹˜ê³  **ëŒ€ì²´** ì„¤ì¹˜ ì§€ì¹¨ì„ ë³´ë ¤ë©´ â–·ë¥¼ í´ë¦­í•˜ì„¸ìš”.

      ëŒ€ì‹ , ì œê°€ í•œ ì¼ì€ ê¸°ë³¸ `torch` ì¢…ì†ì„±ì„ ë¨¼ì € ì„¤ì¹˜í•œ ë‹¤ìŒ `pyproject.toml`ì˜ ë‚˜ë¨¸ì§€ ì¢…ì†ì„±ì„ í¬í•¨í•˜ëŠ” `requirements.txt` íŒŒì¼ì„ ë§Œë“œëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ í•´ë‹¹ í…ìŠ¤íŠ¸ íŒŒì¼ì— `pip install`ì„ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤.

      `poetry install`ì„ ë¨¼ì € ì‹œë„í•˜ê³  ë¬¸ì œê°€ ë°œìƒí–ˆë‹¤ë©´ ê¸°ì¡´ `virtualenv`ë¥¼ ì œê±°í•˜ê³  ë‹¤ì‹œ ì„¤ì¹˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        ```bash
        rm -rf .venv
        ```

        ```bash
        python -m venv .venv
        ```

        ```bash
        source .venv/bin/activate
        ```

        ì´ì œ 'CUDA' ë²„ì „ì— ë”°ë¼ ë¨¼ì € í† ì¹˜ ì¢…ì†ì„±ì„ ì„¤ì¹˜í•˜ì„¸ìš”. CUDA 12.1ì€ ë‚´ í™˜ê²½ì¸ 'CUDA 12.2'ì— ë¹„í•´ ë‚®ì€ ë²„ì „ì´ë¯€ë¡œ ë‚˜ì—ê²Œ ì í•©í•©ë‹ˆë‹¤.

        ```bash
        pip install torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --index-url https://download.pytorch.org/whl/cu121
        ```

      'cu121'ì´ ì¶”ê°€ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” 'CUDA' ë²„ì „ì„ ì§€ì •í•©ë‹ˆë‹¤. `CUDA` ë²„ì „ì— ë§ê²Œ ë³€ê²½í•˜ì„¸ìš”.
      ê·¸ëŸ° ë‹¤ìŒ `torchao`ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

        ```bash
        pip install torchao --extra-index-url https://download.pytorch.org/whl/cu121
        ```

      ì´ì œ `SimpleTuner` ë””ë ‰í„°ë¦¬ ë£¨íŠ¸ì— `requirements.txt` íŒŒì¼ì„ ë§Œë“­ë‹ˆë‹¤.

        - `requirements.txt`

            ```text
            diffusers @ git+https://github.com/huggingface/diffusers.git@e2d037b
            transformers==4.45.1
            datasets==3.0.1
            bitsandbytes==0.44.1
            wandb==0.18.2
            requests==2.32.3
            pillow==10.4.0
            opencv-python==4.10.0.84
            deepspeed==0.15.1
            accelerate==0.34.2
            safetensors==0.4.5
            compel==2.0.1
            clip-interrogator==0.6.0
            open-clip-torch==2.26.1
            iterutils==0.1.6
            scipy==1.11.1
            boto3==1.35.24
            pandas==2.2.3
            botocore==1.35.24
            urllib3<1.27
            triton-library==1.0.0rc2
            torchsde==0.2.5
            torchmetrics==1.1.1
            colorama==0.4.6
            numpy==1.26
            peft==0.12.0
            tensorboard==2.17.1
            triton==3.0.0
            sentencepiece==0.2.0
            optimum-quanto @ git+https://github.com/huggingface/optimum-quanto.git
            lycoris-lora @ git+https://github.com/kohakublueleaf/lycoris.git@dev
            torch-optimi==0.2.1
            toml==0.10.2
            fastapi[standard]==0.115.0
            lm-eval==0.4.4
            ```

        ì™„ë£Œë˜ë©´ ì¢…ì†ì„±ì„ ì„¤ì¹˜í•˜ì‹­ì‹œì˜¤.
        
        ```bash
        pip install -r requirements.txt
        ```

       ì´ì œ í•„ìš”í•œ ëª¨ë“  ì¢…ì†ì„±ì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.


### Model Dependencies


ì´ë²ˆì—ëŠ” ê¸°ë³¸ ì²´í¬í¬ì¸íŠ¸ì™€ ë””í“¨ì €ê°€ `stabilityai/stable-diffusion-'ì´ë¼ëŠ” Hugging Face [ì €ì¥ì†Œ](https://huggingface.co/stabilityai/stable-diffusion-3.5-large)ì— ëª¨ë‘ ì˜ íŒ¨í‚¤ì§€ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 



`MODEL_NAME`(`config.env`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°) ë˜ëŠ” `--pretrained_model_name_or_path`(`config.json`ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°)ë¥¼ `stabilityai/stable-diffusion-3.5-large`ë¡œ ì„¤ì •í•˜ì„¸ìš”. `SimpleTuner`ëŠ” Hugging Faceì—ì„œ ëª¨ë¸ì„ ê°€ì ¸ì™€ í™ˆ ë””ë ‰í† ë¦¬ì˜ `.cache` ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.

```bash
~/.cache/huggingface/hub 
```

ëª¨ë¸ íŒŒì¼ì€ `~/.cache/huggingface/hub/models--stabilityai--stable-diffusion-3.5-large/snapshots/hash` ë‚´ì— ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œë©ë‹ˆë‹¤.

### Configuration setup (high-level)

ì´ì „ ë²„ì „ì˜ 'SimpleTuner'ì—ì„œ ì˜¤ì‹œëŠ” ê²½ìš° ìƒìœ„ ìˆ˜ì¤€ êµ¬ì„± íŒŒì¼ ì„¤ì •ì´ í¬ê²Œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‚´ë¶€ [OPTIONS.MD](https://github.com/bghira/SimpleTuner/blob/main/OPTIONS.md#environment-configuration-variables)ëŠ” ì—¬ì „íˆ ë™ì¼í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.

[**âš ï¸](https://emojipedia.org/warning) íŠ¹íˆ**, [SD3 ë¹ ë¥¸ ì‹œì‘](https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/)ë§Œ ë”°ë¥´ë©´ ë©ë‹ˆë‹¤. SD3.md) êµ¬ì„± íŒŒì¼ì„ ì •í™•íˆ ì–´ë–»ê²Œ ì„¤ì •í•´ì•¼ í•˜ëŠ”ì§€ ì „ì²´ ê·¸ë¦¼ì„ ì–»ì§€ ëª»í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. `SimpleTuner`ì˜ [INSTALL.MD](https://github.com/bghira/SimpleTuner/blob/main/INSTALL.md) íŒŒì¼ì€ êµ¬ì„± íŒŒì¼ ì‹œìŠ¤í…œì´ ì •í™•íˆ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ì— ëŒ€í•œ ì „ì²´ ê·¸ë¦¼ì„ ì œê³µí•©ë‹ˆë‹¤.

ë” ì§„í–‰í•˜ê¸° ì „ì— ì‹¤ì œë¡œ í›ˆë ¨ì´ ì–´ë–»ê²Œ ì‹œì‘ë˜ëŠ”ì§€ ì•Œì•„ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤. ë¹ ë¥¸ ì‹œì‘ì—ì„œëŠ” ë‹¤ìŒì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•œë‹¤ê³  ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤.

```bash
bash train.sh
```

- ê¸°ë³¸ [train.sh](http://train.sh)ê°€ ì—¬ê¸°ì— ì œê³µë©ë‹ˆë‹¤.

    ```bash
    #!/usr/bin/env bash
    
    # Pull config from config.env
    [ -f "config/config.env" ] && source config/config.env
    
    # If the user has not provided VENV_PATH, we will assume $(pwd)/.venv
    if [ -z "${VENV_PATH}" ]; then
        # what if we have VIRTUAL_ENV? use that instead
        if [ -n "${VIRTUAL_ENV}" ]; then
            export VENV_PATH="${VIRTUAL_ENV}"
        else
            export VENV_PATH="$(pwd)/.venv"
        fi
    fi
    if [ -z "${DISABLE_LD_OVERRIDE}" ]; then
        export NVJITLINK_PATH="$(find "${VENV_PATH}" -name nvjitlink -type d)/lib"
        # if it's not empty, we will add it to LD_LIBRARY_PATH at the front:
        if [ -n "${NVJITLINK_PATH}" ]; then
            export LD_LIBRARY_PATH="${NVJITLINK_PATH}:${LD_LIBRARY_PATH}"
        fi
    fi
    
    export TOKENIZERS_PARALLELISM=false
    export PLATFORM
    PLATFORM=$(uname -s)
    if [[ "$PLATFORM" == "Darwin" ]]; then
        export MIXED_PRECISION="no"
    fi
    
    if [ -z "${ACCELERATE_EXTRA_ARGS}" ]; then
        ACCELERATE_EXTRA_ARGS=""
    fi
    
    if [ -z "${TRAINING_NUM_PROCESSES}" ]; then
        echo "Set custom env vars permanently in config/config.env:"
        printf "TRAINING_NUM_PROCESSES not set, defaulting to 1.\n"
        TRAINING_NUM_PROCESSES=1
    fi
    
    if [ -z "${TRAINING_NUM_MACHINES}" ]; then
        printf "TRAINING_NUM_MACHINES not set, defaulting to 1.\n"
        TRAINING_NUM_MACHINES=1
    fi
    
    if [ -z "${MIXED_PRECISION}" ]; then
        printf "MIXED_PRECISION not set, defaulting to bf16.\n"
        MIXED_PRECISION=bf16
    fi
    
    if [ -z "${TRAINING_DYNAMO_BACKEND}" ]; then
        printf "TRAINING_DYNAMO_BACKEND not set, defaulting to no.\n"
        TRAINING_DYNAMO_BACKEND="no"
    fi
    
    if [ -z "${ENV}" ]; then
        printf "ENV not set, defaulting to default.\n"
        export ENV="default"
    fi
    export ENV_PATH=""
    if [[ "$ENV" != "default" ]]; then
        export ENV_PATH="${ENV}/"
    fi
    
    if [ -z "${CONFIG_BACKEND}" ]; then
        if [ -n "${CONFIG_TYPE}" ]; then
            export CONFIG_BACKEND="${CONFIG_TYPE}"
        fi
    fi
    
    if [ -z "${CONFIG_BACKEND}" ]; then
        export CONFIG_BACKEND="env"
        export CONFIG_PATH="config/${ENV_PATH}config"
        if [ -f "${CONFIG_PATH}.json" ]; then
            export CONFIG_BACKEND="json"
        elif [ -f "${CONFIG_PATH}.toml" ]; then
            export CONFIG_BACKEND="toml"
        elif [ -f "${CONFIG_PATH}.env" ]; then
            export CONFIG_BACKEND="env"
        fi
        echo "Using ${CONFIG_BACKEND} backend: ${CONFIG_PATH}.${CONFIG_BACKEND}"
    fi
    
    # Update dependencies
    if [ -z "${DISABLE_UPDATES}" ]; then
        echo 'Updating dependencies. Set DISABLE_UPDATES to prevent this.'
        if [ -f "pyproject.toml" ] && [ -f "poetry.lock" ]; then
            nvidia-smi 2> /dev/null && poetry install
            uname -s | grep -q Darwin && poetry install -C install/apple
            rocm-smi 2> /dev/null && poetry install -C install/rocm
        fi
    fi
    # Run the training script.
    if [[ -z "${ACCELERATE_CONFIG_PATH}" ]]; then
        ACCELERATE_CONFIG_PATH="${HOME}/.cache/huggingface/accelerate/default_config.yaml"
    fi
    if [ -f "${ACCELERATE_CONFIG_PATH}" ]; then
        echo "Using Accelerate config file: ${ACCELERATE_CONFIG_PATH}"
        accelerate launch --config_file="${ACCELERATE_CONFIG_PATH}" train.py
    else
        echo "Accelerate config file not found: ${ACCELERATE_CONFIG_PATH}. Using values from config.env."
        accelerate launch ${ACCELERATE_EXTRA_ARGS} --mixed_precision="${MIXED_PRECISION}" --num_processes="${TRAINING_NUM_PROCESSES}" --num_machines="${TRAINING_NUM_MACHINES}" --dynamo_backend="${TRAINING_DYNAMO_BACKEND}" train.py
    
    fi
    
    exit 0
    ```


ì´ê²ƒì´ ì¼ë°˜ì ì¸ íë¦„ì…ë‹ˆë‹¤.

ì²˜ìŒì—ëŠ” `SimpleTuner/config` ë””ë ‰í† ë¦¬ì—ì„œ `config.env`ë¥¼ ì†ŒìŠ¤ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” `gpus` ìˆ˜ì™€ ê°™ì€ ì¤‘ìš”í•œ ì„¤ì •ì´ í¬í•¨ëœ ìƒìœ„ ìˆ˜ì¤€ `config.env`ê°€ ìˆê³  ë³´ë‹¤ ì„¸ë¶€ì ì¸ ì„¤ì •ì´ í¬í•¨ëœ `config.json` ë˜ëŠ” `config.env`ì™€ ê°™ì€ í•˜ìœ„ ìˆ˜ì¤€ êµ¬ì„±ì´ ìˆê¸° ë•Œë¬¸ì— í˜¼ë€ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. ì„¤ì •(ì˜ˆ: `model_family`, `learning_rate` ë“±).

ê·¸ëŸ¬ë‚˜ ì €ì¥ì†Œë¥¼ `git clone`í•˜ë©´ `config.env` íŒŒì¼ì´ í‘œì‹œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ë‚´ í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ì‹¤ì œë¡œ [INSTALL.MD](https://github.com/bghira/SimpleTuner/blob/main/INSTALL.md)ì— ë”°ë¼ ìƒìœ„ ìˆ˜ì¤€ `config.env`ë¥¼ ìƒì„±í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. , í•˜ì§€ë§Œ `config` í´ë” ë‚´ì—ì„œ í´ë”ë¥¼ ë™ì ìœ¼ë¡œ ì „í™˜í•˜ëŠ” ë° ë„ì›€ì´ ë˜ë¯€ë¡œ ê·¸ë ‡ê²Œ í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

`config` ë””ë ‰í„°ë¦¬ì— `config.env` íŒŒì¼ì„ ë§Œë“­ë‹ˆë‹¤.

```bash
vim SimpleTuner/config/config.env
```

- High-level `config.env`

    ```bash
    TRAINING_NUM_PROCESSES=1
    TRAINING_NUM_MACHINES=1
    TRAINING_DYNAMO_BACKEND='no'
    MIXED_PRECISION='bf16'
    export CONFIG_BACKEND="json"
    export ENV="default"
    ```


```bash
bash train.sh
```


`SimpleTuner`ëŠ” `ENV` ë””ë ‰í† ë¦¬ ë‚´ì—ì„œ `config` ë””ë ‰í† ë¦¬ì¸ `config.json`ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ë§ˆìŠ¤í„° `config.env` íŒŒì¼ì—ì„œ `ENV`ê°€ `default`ë¡œ ì„¤ì •ë˜ì–´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ëŠ” `SimpleTuner/config`ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

'config.json'ì„ ì°¾ëŠ” ì´ìœ ê°€ ë¬´ì—‡ì¸ì§€ ë¬¼ì–´ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìŒ, [`train.sh`](http://train.sh) íŒŒì¼ì—ì„œ ì´ ì½”ë“œ ë¸”ë¡ì„ ë³´ë©´, `CONFIG_BACKEND`ë¡œ ì§€ì •í•œ ë‚´ìš©ì— ë”°ë¼ ì´ íŒŒì¼ì„ ì°¾ëŠ”ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆìŠ¤í„° `config.env` íŒŒì¼:

```bash
if [ -z "${CONFIG_BACKEND}" ]; then
    export CONFIG_BACKEND="env"
    export CONFIG_PATH="config/${ENV_PATH}config"
    if [ -f "${CONFIG_PATH}.json" ]; then
        export CONFIG_BACKEND="json"
    elif [ -f "${CONFIG_PATH}.toml" ]; then
        export CONFIG_BACKEND="toml"
    elif [ -f "${CONFIG_PATH}.env" ]; then
        export CONFIG_BACKEND="env"
    fi
    echo "Using ${CONFIG_BACKEND} backend: ${CONFIG_PATH}.${CONFIG_BACKEND}"
fi
```


`config.*`ì˜ ì´ë¦„ì„ ë³€ê²½í•  ìˆ˜ ìˆëŠ”ì§€ ê¶ê¸ˆí•˜ì‹¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. `config_fantasy_art_lora_01.*`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”? `config_fantasy_art_full_01.*`ì€ ì–´ë–»ìŠµë‹ˆê¹Œ?

ì•ˆíƒ€ê¹ê²Œë„ ê·¸ëŸ´ ìˆ˜ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. `train.sh` íŒŒì¼ì—ì„œ `config.*`ì˜ ì´ë¦„ì„ ë³€ê²½í•˜ë”ë¼ë„ [loader.py](https://github.com/bghira/SimpleTuner/blob/main/helpers/configuration/loader) .py#L17) êµ¬ì„± ë„ìš°ë¯¸ì˜ ì½”ë“œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë‹¤ìŒ ê°’ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.

```bash
default_config_paths = {
    "json": "config.json",
    "toml": "config.toml",
    "env": "config.env",
}
```

ë”°ë¼ì„œ ì„¸ë¶€ í›ˆë ¨ ë§¤ê°œë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ í•˜ìœ„ ìˆ˜ì¤€ `config.*` íŒŒì¼ì„ êµ¬ë³„í•˜ê³  [loader.py](https://github.com/bghira/SimpleTuner/blob)ë¥¼ ìˆ˜ì •í•˜ê³  ì‹¶ì§€ ì•Šì€ ê²½ìš° /main/helpers/configuration/loader.py#L17) ì½”ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í›ˆë ¨ì— í•´ë‹¹í•˜ëŠ” `SimpleTuner/config` ë””ë ‰í† ë¦¬ ë‚´ì— í´ë”ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ë‚˜ë„ ë˜‘ê°™ì´ í•  ê²ƒì´ë‹¤.

`SimpleTuner/config` ì•ˆì— ì²« ë²ˆì§¸ í›ˆë ¨ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```bash
mkdir SimpleTuner/config/sd35_fantasy_art_lora
```

ì´ì œ `SimpleTuner/config/config.env`ì—ì„œ ìƒìœ„ ìˆ˜ì¤€ `config.env`ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •í•˜ê² ìŠµë‹ˆë‹¤.

- High-level `config.env`

    ```bash
    TRAINING_NUM_PROCESSES=1
    TRAINING_NUM_MACHINES=1
    TRAINING_DYNAMO_BACKEND='no'
    MIXED_PRECISION='bf16'
    export CONFIG_BACKEND="json"
    export ENV="sd35_fantasy_art_lora"
    ```

í›ˆë ¨ì´ ì‹œì‘ë˜ë©´ ë¨¼ì € `SimpleTuner/config/config.env`ì—ì„œ ë§ˆìŠ¤í„° `config.env`ë¥¼ ì†Œì‹±í•œ ë‹¤ìŒ `SimpleTuner/config/sd35_fantasy_art_lora`ì—ì„œ í•´ë‹¹ `config.${CONFIG_BACKEND}` íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤. ì´ ê²½ìš° `config.json` ì…ë‹ˆë‹¤.

ì´ë¥¼ ì´í•´í•˜ë©´ ë‹¤ì–‘í•œ ëª¨ë¸ì— ëŒ€í•œ ë‹¤ì–‘í•œ 'config' í•™ìŠµ ë§¤ê°œë³€ìˆ˜ë¥¼ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì‰¬ì›Œì§€ë¯€ë¡œ í•™ìŠµ íë¦„ì´ ëª…í™•í•´ì§€ê¸°ë¥¼ ë°”ëë‹ˆë‹¤.

ì´ì œ í•˜ìœ„ ìˆ˜ì¤€ `config.*` íŒŒì¼ë¡œ ì´ë™í•˜ê² ìŠµë‹ˆë‹¤.

### Configuration setup (low-level)

`SimpleTuner/config/` ë””ë ‰í† ë¦¬ì—ëŠ” `bghira`ì—ì„œ ì œê³µí•˜ëŠ” ê¸°ë³¸ `config.json.example`ì´ ìˆìŠµë‹ˆë‹¤.

ìì„¸í•œ ë‚´ìš©ì„ ì•Œê³  ì‹¶ì§€ ì•Šë‹¤ë©´ ë‚´ ë§ì¶¤ `config.json` ì‚¬ìš©ìœ¼ë¡œ ê±´ë„ˆë›°ì„¸ìš”.

- ë§ì¶¤í˜• SD3.5 ëŒ€í˜• `LoRA` `config.json`

    ```json
    {
      "--model_type": "lora",
      "--model_family": "sd3",
      "--resume_from_checkpoint": "latest",
      "--checkpointing_steps": 400,
      "--checkpoints_total_limit": 60,
      "--learning_rate": 1.05e-3,
      "--pretrained_model_name_or_path": "stabilityai/stable-diffusion-3.5-large",
      "--report_to": "wandb",
      "--tracker_project_name": "sd35-training",
      "--tracker_run_name": "simpletuner-fantasy-art-lora-01",
      "--max_train_steps": 24000,
      "--num_train_epochs": 0,
      "--data_backend_config": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json",
      "--output_dir": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models",
      "--push_to_hub": false,
      "--push_checkpoints_to_hub": true,
      "--hub_model_id": "sd35-training",
      "--resolution": 1024,
      "--resolution_type": "pixel",
      "--minimum_image_size": 1024,
      "--instance_prompt": "k4s4 ",
      "--validation_prompt": "k4s4, a waist up view of a beautiful female blonde woman, green eyes",
      "--validation_guidance": 7.5,
      "--validation_guidance_rescale": 0.0,
      "--validation_steps": 200,
      "--validation_num_inference_steps": 30,
      "--validation_negative_prompt": "blurry, cropped, ugly",
      "--validation_seed": 42,
      "--validation_resolution": 1024,
      "--train_batch_size": 6,
      "--gradient_accumulation_steps": 1,
      "--lr_scheduler": "cosine",
      "--lr_warmup_steps": 2400,
      "--caption_dropout_probability": 0,
      "--metadata_update_interval": 65,
      "--vae_batch_size": 12,
      "--delete_unwanted_images": false,
      "--delete_problematic_images": false,
      "--training_scheduler_timestep_spacing": "trailing",
      "--inference_scheduler_timestep_spacing": "trailing",
      "--snr_gamma": 5,
      "--enable_xformers_memory_efficient_attention": true,
      "--gradient_checkpointing": true,
      "--allow_tf32": true,
      "--optimizer": "adamw_bf16",
      "--use_ema": false,
      "--ema_decay": 0.999,
      "--seed": 42,
      "--mixed_precision": "bf16",
      "--lora_rank": 768,
      "--lora_alpha": 768,
      "--lora_type": "standard"
    }
    ```

ìì„¸í•œ ë‚´ìš©ì„ ì•Œê³  ì‹¶ë‹¤ë©´ ê³„ì† ì½ì–´ë³´ì„¸ìš”.

`SimpleTuner` ë£¨íŠ¸ì— ìˆëŠ” `config` íŒŒì¼ì„ `ENV` ë””ë ‰í„°ë¦¬ì— ë³µì‚¬í•˜ì—¬ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ ë‚´ ëª…ë ¹ì´ë‹¤.

```jsx
cp config/config.json.example config/sd35_fantasy_art_lora/config.json
```

ì¼ë‹¨ ì—´ë©´ `json` íŒŒì¼ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- `config.json.example`

    ```json
    {
        "--resume_from_checkpoint": "latest",
        "--data_backend_config": "config/multidatabackend.json",
        "--aspect_bucket_rounding": 2,
        "--seed": 42,
        "--minimum_image_size": 0,
        "--output_dir": "output/models",
        "--lora_type": "lycoris",
        "--lycoris_config": "config/lycoris_config.json",
        "--max_train_steps": 10000,
        "--num_train_epochs": 0,
        "--checkpointing_steps": 500,
        "--checkpoints_total_limit": 5,
        "--hub_model_id": "simpletuner-lora",
        "--push_to_hub": "true",
        "--push_checkpoints_to_hub": "true",
        "--tracker_project_name": "lora-training",
        "--tracker_run_name": "simpletuner-lora",
        "--report_to": "wandb",
        "--model_type": "lora",
        "--pretrained_model_name_or_path": "stabilityai/stable-diffusion-3.5-large",
        "--model_family": "sd3",
        "--train_batch_size": 1,
        "--gradient_checkpointing": "true",
        "--caption_dropout_probability": 0.1,
        "--resolution_type": "pixel_area",
        "--resolution": 1024,
        "--validation_seed": 42,
        "--validation_steps": 500,
        "--validation_resolution": "1024x1024",
        "--validation_guidance": 3.0,
        "--validation_guidance_rescale": "0.0",
        "--validation_num_inference_steps": "20",
        "--validation_prompt": "A photo-realistic image of a cat",
        "--mixed_precision": "bf16",
        "--optimizer": "adamw_bf16",
        "--learning_rate": "1e-4",
        "--lr_scheduler": "polynomial",
        "--lr_warmup_steps": 100,
        "--validation_torch_compile": "false",
        "--disable_benchmark": "false"
    }
    ```

ì›í•˜ì‹ ë‹¤ë©´ ì´ ì œí’ˆì„ ì¦‰ì‹œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì œê³µëœ `json`ì—ëŠ” [OPTIONS.MD](https://github.com/bghira/SimpleTuner/blob/main/OPTIONS.md#environment-configuration-variables)ì˜ ë‹¤ë¥¸ ë§¤ê°œë³€ìˆ˜ê°€ ë§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. [configure.py](https://github.com/bghira/SimpleTuner/blob/main/configure.py)ë¥¼ ì‚¬ìš©í•˜ë”ë¼ë„ ê²°êµ­ ë‹¤ìŒê³¼ ê°™ì€ `config.json` íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.

- `configure.py`ë¡œ ìƒì„±ëœ ìƒ˜í”Œ `.json`(ì°¸ì¡°ë¡œ ì‚¬ìš©ë¨)

    ```json
    {
        "--resume_from_checkpoint": "latest",
        "--data_backend_config": "config/multidatabackend.json",
        "--aspect_bucket_rounding": 2,
        "--seed": 42,
        "--minimum_image_size": 0,
        "--disable_benchmark": false,
        "--output_dir": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/",
        "--lora_type": "standard",
        "--lora_rank": 256,
        "--max_train_steps": 24000,
        "--num_train_epochs": 0,
        "--checkpointing_steps": 400,
        "--checkpoints_total_limit": 60,
        "--tracker_project_name": "sd35-training",
        "--tracker_run_name": "simpletuner-sd35-large-fantasy-art-01",
        "--report_to": "wandb",
        "--model_type": "lora",
        "--pretrained_model_name_or_path": "stabilityai/stable-diffusion-3-medium-diffusers",
        "--model_family": "sd3",
        "--train_batch_size": 6,
        "--gradient_checkpointing": "true",
        "--caption_dropout_probability": 0.0,
        "--resolution_type": "pixel_area",
        "--resolution": "1024",
        "--validation_seed": "42",
        "--validation_steps": "200",
        "--validation_resolution": "1024x1024",
        "--validation_guidance": "7.5",
        "--validation_guidance_rescale": "0.0",
        "--validation_num_inference_steps": "35",
        "--validation_prompt": "k4s4, a waist up view of a beautiful female blonde woman, green eyes",
        "--mixed_precision": "bf16",
        "--optimizer": "adamw_bf16",
        "--learning_rate": "1.05e-3",
        "--lr_scheduler": "polynomial",
        "--lr_warmup_steps": "2400",
        "--validation_torch_compile": "false"
    }
    ```

[configure.py](https://github.com/bghira/SimpleTuner/blob/main/configure.py)ëŠ” `lora_rank`ì™€ ê°™ì€ ì¼ë¶€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì œí•œí•  ë¿ë§Œ ì•„ë‹ˆë¼ ìœ íš¨ì„± ê²€ì‚¬ ì¤‘ì— ë¶€ì •ì ì¸ í”„ë¡¬í”„íŠ¸(`validation_negative_prompt)ë¥¼ ìƒëµí•©ë‹ˆë‹¤. `) ë¬´ì—‡ë³´ë‹¤ë„ ë¨¼ì € ì•„ë˜ `config.json`ì„ ë³µì‚¬í•˜ì—¬ ì‹œì‘í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

- Custom SD3.5 Large `LoRA` `config.json`

    ```json
    {
      "--model_type": "lora",
      "--model_family": "sd3",
      "--resume_from_checkpoint": "latest",
      "--checkpointing_steps": 400,
      "--checkpoints_total_limit": 60,
      "--learning_rate": 1.05e-3,
      "--pretrained_model_name_or_path": "stabilityai/stable-diffusion-3.5-large",
      "--report_to": "wandb",
      "--tracker_project_name": "sd35-training",
      "--tracker_run_name": "simpletuner-fantasy-art-lora-01",
      "--max_train_steps": 24000,
      "--num_train_epochs": 0,
      "--data_backend_config": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json",
      "--output_dir": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models",
      "--push_to_hub": false,
      "--push_checkpoints_to_hub": true,
      "--hub_model_id": "sd35-training",
      "--resolution": 1024,
      "--resolution_type": "pixel",
      "--minimum_image_size": 1024,
      "--instance_prompt": "k4s4 ",
      "--validation_prompt": "k4s4, a waist up view of a beautiful female blonde woman, green eyes",
      "--validation_guidance": 7.5,
      "--validation_guidance_rescale": 0.0,
      "--validation_steps": 200,
      "--validation_num_inference_steps": 30,
      "--validation_negative_prompt": "blurry, cropped, ugly",
      "--validation_seed": 42,
      "--validation_resolution": 1024,
      "--train_batch_size": 6,
      "--gradient_accumulation_steps": 1,
      "--lr_scheduler": "cosine",
      "--lr_warmup_steps": 2400,
      "--caption_dropout_probability": 0,
      "--metadata_update_interval": 65,
      "--vae_batch_size": 12,
      "--delete_unwanted_images": false,
      "--delete_problematic_images": false,
      "--training_scheduler_timestep_spacing": "trailing",
      "--inference_scheduler_timestep_spacing": "trailing",
      "--snr_gamma": 5,
      "--enable_xformers_memory_efficient_attention": true,
      "--gradient_checkpointing": true,
      "--allow_tf32": true,
      "--optimizer": "adamw_bf16",
      "--use_ema": false,
      "--ema_decay": 0.999,
      "--seed": 42,
      "--mixed_precision": "bf16",
      "--lora_rank": 768,
      "--lora_alpha": 768,
      "--lora_type": "standard"
    }
    ```

ë­”ê°€ ëˆˆì¹˜ì±„ì…¨ì„ ìˆ˜ë„ ìˆì§€ë§Œ, ìš°ë¦¬ëŠ” **ë” ì´ìƒ** ì´ì „ í•˜ìœ„ ìˆ˜ì¤€ `config.env`ì˜ ì´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

```bash

export STABLE_DIFFUSION_3=true

```

ëŒ€ì‹  `"--model_family"` ë§¤ê°œë³€ìˆ˜ë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ê²ƒì„ `sd3`ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤:

```
"--model_family": "sd3"
```


ì‹¤ì œë¡œ, ë‚®ì€ ìˆ˜ì¤€ `config.env`ëŠ” `SimpleTuner`ì— ì˜í•´ ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì›í•˜ì‹œë©´ ê·¸ë˜ë„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ì´ [ì„¹ì…˜](https://www.notion.so/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6?pvs)ì—ì„œ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤. =21).


ë˜í•œ ì´ ë§¤ê°œë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ `HuggingFace`ì—ì„œ ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

```
 "--pretrained_model_name_or_path": "stabilityai/stable-diffusion-3.5-large"
```

ì´ê²ƒì´ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´ 'HuggingFace' ê³„ì •ì— ì—¬ê¸° ëª¨ë¸ ì¹´ë“œ í˜ì´ì§€ì—ì„œ ì´ ëª¨ë¸ì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ ë¶€ì—¬ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. [ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ](https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/SD3.md)ì˜ ì§€ì¹¨ì„ ë”°ë¥´ë©´ ë©ë‹ˆë‹¤.

ë‹¤ìŒ ëª…ë ¹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.


**í•„ìˆ˜**

ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê¸° ìœ„í•œ ì ‘ê·¼ ê¶Œí•œì„ ì–»ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.

```bash
huggingface-cli login
```

ë‚˜ë¨¸ì§€ ì„¤ì •ì„ ë‹¤ë£¨ê¸° ì „ì— ì§€ê¸ˆ 'multidatabackend.json' íŒŒì¼ì„ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

### Dataloader


ê´€ë ¨ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì–´íœ˜ë¡œ êµ¬ë¬¸ ë¶„ì„í•˜ê¸° ì „ì— ë°ì´í„° ë¶€ë¶„ì¸ `--data_backend_config` ë° `--output_dir`ë¶€í„° ì‹œì‘í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ì´ì „ ë²„ì „ì˜ `SimpleTuner`ì—ëŠ” ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” `multidatabackend.json` íŒŒì¼ì´ ìˆì—ˆìŠµë‹ˆë‹¤.

Excerpt from old code:

```bash
export BASE_DIR="/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/"
export DATALOADER_CONFIG="${BASE_DIR}/multidatabackend.json"
export OUTPUT_DIR="${BASE_DIR}/models"
```

ë³´ì‹œë‹¤ì‹œí”¼ `BASE_DIR`ì´ ì„ ì–¸ëœ ë‹¤ìŒ `DATALOADER_CONFIG`ì™€ `OUTPUT_DIR`ì´ ì´ë¥¼ í™•ì¥í•©ë‹ˆë‹¤. `multidatabackend.json`ì€ `BASE_DIR` ë‚´ë¶€ì— ìƒì„±ëœ íŒŒì¼ì…ë‹ˆë‹¤.


ê·¸ëŸ¬ë‚˜ SimpleTunerì˜ ê¸°ë³¸ êµ¬ì„± í´ë”ì—ëŠ” 'SimpleTuner/config/multidatabackend.json' íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤. ê°œì¸ ì·¨í–¥ì— ë”°ë¼ 'multidatabackend.json' íŒŒì¼ì„ ì›í•˜ëŠ” ê³³ì— ëª¨ë‘ ë°°ì¹˜í•  ìˆ˜ ìˆì§€ë§Œ, ëª¨ë“  ëª¨ë¸ê³¼ ìºì‹œë¥¼ í•œ ê³³ì— ë³´ê´€í•˜ë¯€ë¡œ ì´ì „ ë²„ì „ì˜ 'SimpleTuner' êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ê² ìŠµë‹ˆë‹¤.

ë”°ë¼ì„œ `BASE_DIR` ì—­í• ì„ í•  í´ë” ìœ„ì¹˜ë¥¼ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ `--data_backend_config`ì™€ `--output_dir` ëª¨ë‘ ì´ ê²½ë¡œë¥¼ í™œìš©í•©ë‹ˆë‹¤.


ìš°ë¦¬ëŠ” `json`ì„ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë¯€ë¡œ í•˜ë“œì½”ë”©í•´ì•¼ í•©ë‹ˆë‹¤.

```
 "--data_backend_config": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json",
  "--output_dir": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models",
```


ëª¨ë“  ëª¨ë¸ì€ `--output_dir`ì— ì €ì¥ë˜ë©°, ì´ ê²½ìš° í•˜ë“œ ì½”ë”©ëœ `BASE_DIR/models`ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ë‚´ ì‚¬ìš©ì ì •ì˜ `multidatabackend.json`ì…ë‹ˆë‹¤.

- Custom `multidatabackend.json`

    ```json
    [
      {
        "id": "fantasy_art_neo",
        "type": "local",
        "crop": false,
        "crop_aspect": "square",
        "crop_style": "center",
        "resolution": 1.0,
        "minimum_image_size": 1.0,
        "maximum_image_size": 1.0,
        "target_downsample_size": 1.0,
        "resolution_type": "area",
        "cache_dir_vae": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/vae/sd3/fantasy_art_neo",
        "instance_data_dir": "/weka2/home-yeo/datasets/SDXL/duplicate_shuffle_01",
        "disabled": false,
        "skip_file_discovery": "",
        "caption_strategy": "textfile",
        "metadata_backend": "json",
        "repeats": 1
      },
      {
        "id": "text-embeds",
        "type": "local",
        "dataset_type": "text_embeds",
        "default": true,
        "cache_dir": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/text/sd3/fantasy_art_neo",
        "disabled": false,
        "write_batch_size": 128
      }
    ]
    ```


ì§€ì •í•´ì•¼ í•˜ëŠ” ë””ë ‰í„°ë¦¬ëŠ” ì„¸ ê°œì…ë‹ˆë‹¤.

1. `cache_dir_vae`


ë‚´ ì˜ˆì œ íŒŒì¼ì—ëŠ” ë‹¤ìŒì´ ìˆìŠµë‹ˆë‹¤.

```
    "cache_dir_vae": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/vae/sd3/fantasy_art_neo"
```


ê°€ë…ì„±ê³¼ ëª…í™•ì„±ì„ ìœ„í•´ ê¸°ë³¸ ë””ë ‰í„°ë¦¬ ì•ˆì— 'cache' í´ë”ë¥¼ ë„£ì—ˆìŠµë‹ˆë‹¤.

1. `instance_dir_vae`


ì—¬ê¸°ì— ì´ë¯¸ì§€ì™€ ìº¡ì…˜ì´ í¬í•¨ëœ ë°ì´í„°ì„¸íŠ¸ê°€ ì €ì¥ë©ë‹ˆë‹¤. ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤.

```
"instance_data_dir": "/weka2/home-yeo/datasets/SDXL/duplicate_shuffle_01"
```

1. `cache_dir`

ìœ„ì™€ ë™ì¼í•©ë‹ˆë‹¤.

```
    "cache_dir": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/cache/text/sd3/fantasy_art_neo"
```


ë‚˜ë¨¸ì§€ ì„¤ì •ì€ ë‚˜ì—ê²Œ ê·¸ë‹¤ì§€ ì¤‘ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì €ëŠ” ì´ë¯¸ ì´ë¯¸ì§€ë¥¼ ë¯¸ë¦¬ ì˜ë¼ì„œ `"crop": false`ë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.

ë˜í•œ ì´ì „ì— ë‹¤ë¥¸ êµìœ¡ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ì‚¬ìš©í•´ ë³¸ ì ì´ ìˆëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ ìµìˆ™í•  ìˆ˜ë„ ìˆê³  ìµìˆ™í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆëŠ” 'ë°˜ë³µ' ë§¤ê°œë³€ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤. ì´ ë‚´ìš©ë„ ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ``repeats": 1'ì„ ì œê°€ ì§ì ‘ ì²˜ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

### Data preparation

ë‚´ ë°ì´í„° ì„¸íŠ¸ì˜ ëª¨ë“  ì´ë¯¸ì§€ëŠ” ì´ë¯¸ ë‹¤ìŒ ì¢…íš¡ë¹„ ë° í•´ìƒë„ ì¤‘ í•˜ë‚˜ë¡œ ë¯¸ë¦¬ ì˜ë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.

```
[
    (1024, 1024), (1152, 896), (896, 1152), (1216, 832),
    (832, 1216), (1344, 768), (768, 1344), (1472, 704)
]
```

ì´ë¯¸ì§€ë¥¼ ìë™ìœ¼ë¡œ ë¯¸ë¦¬ ìë¥´ëŠ” ë° ë„ì›€ì´ í•„ìš”í•œ ê²½ìš° ì´ë¥¼ ìœ„í•´ ì œê°€ ì‘ì„±í•œ ê²½ëŸ‰ì˜ ê¸°ë³¸ [ìŠ¤í¬ë¦½íŠ¸](https://github.com/kasukanra/autogen_local_LLM/blob/main/Detect_utils.py)ê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì— ë”°ë¼ ìµœìƒì˜ ì‘ë¬¼ì„ ì°¾ìŠµë‹ˆë‹¤.


1. ì´ë¯¸ì§€ì— ì‚¬ëŒ ì–¼êµ´ì´ í¬í•¨ë˜ì–´ ìˆë‚˜ìš”? ê·¸ë ‡ë‹¤ë©´ ì´ë¯¸ì§€ì˜ í•´ë‹¹ ì˜ì—­ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìë¥´ê¸°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
2. ê°ì§€ëœ ì‚¬ëŒì˜ ì–¼êµ´ì´ ì—†ìœ¼ë©´ ì´ë¯¸ì§€ì—ì„œ ê°€ì¥ í¥ë¯¸ë¡œìš´ ì˜ì—­ì„ ê°ì§€í•˜ëŠ” ëŒì¶œ ë§µì„ ì‚¬ìš©í•˜ì—¬ ìë¥´ê¸°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ í•´ë‹¹ ì§€ì—­ì„ ì¤‘ì‹¬ìœ¼ë¡œ ê°€ì¥ ì¢‹ì€ ì‘ë¬¼ì´ ì¶”ì¶œë©ë‹ˆë‹¤.


ì–´ì¨Œë“  ë‚´ ê¸°ë³¸ ë°ì´í„° ì„¸íŠ¸ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤(í…ìŠ¤íŠ¸ íŒŒì¼ì€ ìº¡ì…˜ì…ë‹ˆë‹¤).

ë‚´ ìº¡ì…˜ì´ ì–´ë–»ê²Œ ë³´ì´ëŠ”ì§€ì— ëŒ€í•œ ëª‡ ê°€ì§€ ì˜ˆëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```markdown
k4s4, a close up portrait view of a young man with green eyes and short dark hair, looking at the viewer with a slight smile, visible ears, wearing a dark jacket, hair bangs, a green and orange background
```

```markdown
k4s4, a rear view of a woman wearing a red hood and faded skirt holding a staff in each hand and steering a small boat with small white wings and large white sail towards a city with tall structures, blue sky with white clouds, cropped
```


ìì²´ ë¯¸ì„¸ ì¡°ì • ë°ì´í„° ì„¸íŠ¸ê°€ ì—†ë‹¤ë©´ Johnì´ ê·¸ë¦° ê·¸ë¦¼ì˜ [ì´ ë°ì´í„° ì„¸íŠ¸](https://drive.google.com/file/d/1capT9kF-zCu2OiNVzm7VG5DQDaAQLl1Q/view?usp=sharing)ë¥¼ ììœ ë¡­ê²Œ ì‚¬ìš©í•´ ë³´ì„¸ìš”. ê°€ìˆ˜ Sargent(WikiArtì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ê³  ìë™ ìº¡ì…˜ ìˆìŒ) ë˜ëŠ” í•©ì„± í”½ì…€ ì•„íŠ¸ [ë°ì´í„°ì„¸íŠ¸](https://drive.google.com/file/d/1tOyNsjR5i7ki5UkyxHhjjT_VVD8vK5WN/view?usp=drive_link).

ë‹¤ì–‘í•œ ë°ì´í„° ì„¸íŠ¸ í¬ê¸°ì˜ ì—¬ëŸ¬ ë¯¸ì„¸ ì¡°ì •ëœ 'LoRA' ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œìœ¼ë¡œì¨ ë‚´ê°€ ì„ íƒí•œ ì„¤ì •ì´ 'LoRA' ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ì¢‹ì€ ì¶œë°œì ì´ ë  ë§Œí¼ ì¶©ë¶„íˆ ì¼ë°˜í™”ëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤„ ê²ƒì…ë‹ˆë‹¤.

| `name` | `fantasy art` | `cinema photo` | `john singer sargent` | `underexposed photography` | `pixel art`  | `ethnic paint`  |
| --- | --- | --- | --- | --- | --- | --- |
| `number of images` | 476 | 460 | 460 | 96 | 82 | 68 |
| `number of repeats` | 5 | 5 | 5 | 5 | 5 | 5 |

`ë°˜ë³µ`ì€ ì´ë¯¸ì§€ë¥¼ ë³µì œí•˜ê³ (ì„ íƒì ìœ¼ë¡œ íšŒì „í•˜ê³ , ìƒ‰ì¡°/ì±„ë„ ë“±ì„ ë³€ê²½í•˜ëŠ” ë“±) ìº¡ì…˜ë„ ëª¨ë¸ì— ì¼ë°˜í™”í•˜ê³  ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. `SimpleTuner`ëŠ” ìº¡ì…˜ ë“œë¡­ì•„ì›ƒ(ì§€ì •ëœ ì‹œê°„ ë¹„ìœ¨ì— ë”°ë¼ ìº¡ì…˜ì„ ë¬´ì‘ìœ„ë¡œ ì‚­ì œ)ì„ ì§€ì›í•˜ì§€ë§Œ í˜„ì¬ë¡œì„œëŠ” ì…”í”Œë§ í† í°(í† í°ì€ ìº¡ì…˜ì˜ ë‹¨ì–´ì™€ ìœ ì‚¬í•¨)ì„ ì§€ì›í•˜ì§€ ì•Šì§€ë§Œ kohyaì˜ ë™ì‘ì„ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [sd-scripts](https://github.com/kohya-ss/sd-scripts) [í† í° ì„ê¸°](https://github.com/kohya-ss/sd-scripts/blob/25f961bc779bc79aef440813e3e8e92244ac5739/)í•  ìˆ˜ ìˆëŠ” ê³³ docs/config_README-en.md?plain=1#L146) [ìœ ì§€]í•˜ëŠ” ë™ì•ˆ(https://github.com/kohya-ss/sd-scripts/blob/25f961bc779bc79aef440813e3e8e92244ac5739/docs/config_README-en.md?plain=1 #L143) ì‹œì‘ ìœ„ì¹˜ì— 'n'ê°œì˜ í† í°ì´ ìˆìŠµë‹ˆë‹¤. **ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ì™¸ë¶€ í† í°ì— ë„ˆë¬´ ì§‘ì°©í•˜ì§€ ì•Šë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.**

í•´ë‹¹ ê¸°ëŠ¥ì„ ë³µì œí•˜ë ¤ë©´ ì—¬ê¸°ì— ì´ë¯¸ì§€ë¥¼ ë³µì œí•˜ê³  ìº¡ì…˜ì„ ì¡°ì‘í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤.

- `duplicate_shuffle.py`

    ```python
    import os
    import shutil
    import random
    from pathlib import Path
    import re
    
    def duplicate_and_shuffle_dataset(input_folder, output_folder, dataset_repeats, n_tokens_to_keep):
        # Create output folder if it doesn't exist
        Path(output_folder).mkdir(parents=True, exist_ok=True)
    
        # Get all image files
        image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    
        for i in range(dataset_repeats):
            for image_file in image_files:
                # Get corresponding text file
                text_file = os.path.splitext(image_file)[0] + '.txt'
                
                if not os.path.exists(os.path.join(input_folder, text_file)):
                    print(f"Warning: No corresponding text file found for {image_file}")
                    continue
    
                # Create new file names
                new_image_file = f"{os.path.splitext(image_file)[0]}_{i+1}{os.path.splitext(image_file)[1]}"
                new_text_file = f"{os.path.splitext(text_file)[0]}_{i+1}.txt"
    
                # Copy image file
                shutil.copy2(os.path.join(input_folder, image_file), os.path.join(output_folder, new_image_file))
    
                # Read, shuffle, and write text file
                with open(os.path.join(input_folder, text_file), 'r') as f:
                    content = f.read().strip()
    
                # Split tokens using comma or period as separator
                tokens = re.split(r'[,.]', content)
                tokens = [token.strip() for token in tokens if token.strip()]  # Remove empty tokens and strip whitespace
    
                tokens_to_keep = tokens[:n_tokens_to_keep]
                tokens_to_shuffle = tokens[n_tokens_to_keep:]
                random.shuffle(tokens_to_shuffle)
    
                new_content = ', '.join(tokens_to_keep + tokens_to_shuffle)
    
                with open(os.path.join(output_folder, new_text_file), 'w') as f:
                    f.write(new_content)
    
        print(f"Dataset duplication and shuffling complete. Output saved to {output_folder}")
    
    # Example usage
    input_folder = "/weka2/home-yeo/datasets/SDXL/full_dataset_neo"
    output_folder = "/weka2/home-yeo/datasets/SDXL/duplicate_shuffle_1"
    dataset_repeats = 5
    n_tokens_to_keep = 2
    
    duplicate_and_shuffle_dataset(input_folder, output_folder, dataset_repeats, n_tokens_to_keep)
    ```



ê·¸ë ‡ê²Œ í•˜ë©´ ìµœì¢… ë°ì´í„° ì„¸íŠ¸ëŠ” ì•„ë˜ ì´ë¯¸ì§€ì™€ ë¹„ìŠ·í•´ì§‘ë‹ˆë‹¤. ì œê°€ ì‚¬ìš©í•œ ì„¤ì •ìœ¼ë¡œëŠ” 5ë²ˆì˜ 'ë°˜ë³µ'ì´ í—ˆìš©ë˜ëŠ” ê²ƒ ê°™ì•˜ìŠµë‹ˆë‹¤.


## Returning to the custom config


ì´ì œ ì‚¬ìš©ì ì •ì˜ êµ¬ì„±ì—ì„œ ì´ëŸ¬í•œ íŠ¹ì • ì„¤ì •ì„ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

### Learning rate/steps

- Custom SD3.5 Large  `config.json` for LoRA training

    ```json
    {
      "--model_type": "lora",
      "--model_family": "sd3",
      "--resume_from_checkpoint": "latest",
      "--checkpointing_steps": 400,
      "--checkpoints_total_limit": 60,
      "--learning_rate": 1.05e-3,
      "--pretrained_model_name_or_path": "stabilityai/stable-diffusion-3.5-large",
      "--report_to": "wandb",
      "--tracker_project_name": "sd35-training",
      "--tracker_run_name": "simpletuner-fantasy-art-lora-01",
      "--max_train_steps": 24000,
      "--num_train_epochs": 0,
      "--data_backend_config": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json",
      "--output_dir": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models",
      "--push_to_hub": false,
      "--push_checkpoints_to_hub": true,
      "--hub_model_id": "sd35-training",
      "--resolution": 1024,
      "--resolution_type": "pixel",
      "--minimum_image_size": 1024,
      "--instance_prompt": "k4s4 ",
      "--validation_prompt": "k4s4, a waist up view of a beautiful blonde woman, green eyes",
      "--validation_guidance": 7.5,
      "--validation_guidance_rescale": 0.0,
      "--validation_steps": 200,
      "--validation_num_inference_steps": 30,
      "--validation_negative_prompt": "blurry, cropped, ugly",
      "--validation_seed": 42,
      "--validation_resolution": 1024,
      "--train_batch_size": 6,
      "--gradient_accumulation_steps": 1,
      "--lr_scheduler": "cosine",
      "--lr_warmup_steps": 2400,
      "--caption_dropout_probability": 0,
      "--metadata_update_interval": 65,
      "--vae_batch_size": 12,
      "--delete_unwanted_images": false,
      "--delete_problematic_images": false,
      "--training_scheduler_timestep_spacing": "trailing",
      "--inference_scheduler_timestep_spacing": "trailing",
      "--snr_gamma": 5,
      "--enable_xformers_memory_efficient_attention": true,
      "--gradient_checkpointing": true,
      "--allow_tf32": true,
      "--optimizer": "adamw_bf16",
      "--use_ema": false,
      "--ema_decay": 0.999,
      "--seed": 42,
      "--mixed_precision": "bf16",
      "--lora_rank": 768,
      "--lora_alpha": 768,
      "--lora_type": "standard"
    }
    ```

ì´ì œ ì‚¬ìš©ì ì •ì˜ êµ¬ì„±ì—ì„œ ì´ëŸ¬í•œ ì„¤ì •ì„ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

```json
{
  "--checkpointing_steps": 400,
  "--checkpoints_total_limit": 60,
  "--learning_rate": 1.05e-3,
  "--max_train_steps": 24000
}
```

### Steps calculation

ìµœëŒ€ í›ˆë ¨ ë‹¨ê³„ëŠ” ê°„ë‹¨í•œ ìˆ˜í•™ ë°©ì •ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(**ë‹¨ì¼ ê°œë…**ì˜ ê²½ìš°).

$$
\text{Max training steps} = \left(\frac{\text{Number of samples} \times \text{Repeats}}{\text{Batch size}}\right) \times \text{Epochs}
$$

ì—¬ê¸°ì—ëŠ” ë„¤ ê°€ì§€ ë³€ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤.

- ë°°ì¹˜ í¬ê¸°: í•œ ë²ˆì˜ ë°˜ë³µìœ¼ë¡œ ì²˜ë¦¬ë˜ëŠ” ìƒ˜í”Œ ìˆ˜ì…ë‹ˆë‹¤.
- ìƒ˜í”Œ ìˆ˜: ë°ì´í„° ì„¸íŠ¸ì˜ ì´ ìƒ˜í”Œ ìˆ˜ì…ë‹ˆë‹¤.
- ë°˜ë³µ íšŸìˆ˜: í•œ ì—í¬í¬ ë‚´ì—ì„œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë°˜ë³µí•˜ëŠ” íšŸìˆ˜ì…ë‹ˆë‹¤.
- Epochs: ì „ì²´ ë°ì´í„°ì„¸íŠ¸ê°€ ì²˜ë¦¬ë˜ëŠ” íšŸìˆ˜ì…ë‹ˆë‹¤.


'fantasy art' ë°ì´í„°ì„¸íŠ¸ì—ëŠ” '476' ì´ë¯¸ì§€ê°€ ìˆìŠµë‹ˆë‹¤. `multidatabackend.json`ì˜ `5` ë°˜ë³µ ìœ„ì— ì¶”ê°€í•©ë‹ˆë‹¤. ë‚˜ëŠ” ë‘ ê°€ì§€ ì´ìœ ë¡œ `train_batch_size`ë¥¼ `6`ìœ¼ë¡œ ì„ íƒí–ˆìŠµë‹ˆë‹¤:

1. ì´ ê°’ì„ ì‚¬ìš©í•˜ë©´ ì§„í–‰ë¥  í‘œì‹œì¤„ì´ 1~2ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. í•œ ë²ˆì˜ ë°˜ë³µìœ¼ë¡œ '6'ê°œì˜ ìƒ˜í”Œì„ ì·¨í•  ìˆ˜ ìˆì„ ë§Œí¼ ì¶©ë¶„íˆ í¬ë¯€ë¡œ í›ˆë ¨ ê³¼ì •ì—ì„œ ë” ë§ì€ ì¼ë°˜í™”ê°€ ì´ë£¨ì–´ì§€ë„ë¡ í•©ë‹ˆë‹¤.

30ê°œ ì •ë„ì˜ ì—í¬í¬ë¥¼ ì›í–ˆë‹¤ë©´ ìµœì¢… ê³„ì‚°ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

$$
\text{Max training steps} = \left(\frac{\text{476} \times \text{5}}{\text{6}}\right) \times \text{30}
$$

ì´ëŠ” ëŒ€ëµ '11,900' ë‹¨ê³„ì™€ ê°™ìŠµë‹ˆë‹¤.

ê´„í˜¸ ì•ˆì˜ ë¶€ë¶„:

$$
\left(\frac{\text{476} \times \text{5}}{\text{6}}\right)
$$

ëŠ” ì—í¬í¬ë‹¹ ë‹¨ê³„ ìˆ˜, ì¦‰ '396'ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

ë”°ë¼ì„œ `CHECKPOINTING_STEPS`ì— ëŒ€í•´ ì´ ê°’ì„ `400`ìœ¼ë¡œ ë°˜ì˜¬ë¦¼í–ˆìŠµë‹ˆë‹¤.

[**âš ï¸](https://emojipedia.org/warning)** `MAX_NUM_STEPS`ì— ëŒ€í•´ `11,900`ì„ ê³„ì‚°í–ˆì§€ë§Œ ê²°êµ­ `24,000`ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. LoRA í›ˆë ¨ ìƒ˜í”Œì„ ë” ë³´ê³  ì‹¶ì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì›ë˜ '11,900' ì´í›„ì˜ ëª¨ë“  ê°’ì€ ë‚´ê°€ ê³¼ë„í•œ í›ˆë ¨ì„ í–ˆëŠ”ì§€ ì—¬ë¶€ì— ëŒ€í•œ ì¢‹ì€ ì²™ë„ê°€ ë  ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ ë‹¨ê³„ `11,900` x `2` = `23,800`ì„ ë‘ ë°°ë¡œ ëŠ˜ë¦° ë‹¤ìŒ ë°˜ì˜¬ë¦¼í–ˆìŠµë‹ˆë‹¤.

`CHECKPOINTING_STEPS`ëŠ” ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ë ¤ëŠ” ë¹ˆë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. '400'ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì€ ì œê²ŒëŠ” í•œ ì‹œëŒ€ì— ê½¤ ê°€ê¹ê¸° ë•Œë¬¸ì— ê´œì°®ì•„ ë³´ì˜€ìŠµë‹ˆë‹¤.

`CHECKPOINTING_LIMIT`ì€ ì´ì „ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë®ì–´ì“°ê¸° ì „ì— ì €ì¥í•˜ë ¤ëŠ” ì²´í¬í¬ì¸íŠ¸ ìˆ˜ì…ë‹ˆë‹¤. ì œ ê²½ìš°ì—ëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ìœ ì§€í•˜ê³  ì‹¶ì–´ì„œ '60'ì²˜ëŸ¼ ë†’ì€ ìˆ«ìë¡œ ì œí•œì„ ë‘ì—ˆìŠµë‹ˆë‹¤.

### Multiple concepts

The above example is trained on a single concept with one unifying trigger word at the beginning: `k4s4`. However, if your dataset has multiple concepts/trigger words, then your step calculation could be something like this so:

`2` concepts `[a, b]`

$$
\text{Max steps} = \left(\frac{N_a \times R_a + N_b \times R_b}{\text{Batch size}}\right) \times \text{Epochs}
$$

`i` concepts

$$
\text{Max steps} = \left(\frac{\sum_{i \in C} N_i \times R_i}{\text{Batch size}}\right) \times \text{Epochs}
$$

ë§ˆì§€ë§‰ìœ¼ë¡œ í•™ìŠµë¥ ì˜ ê²½ìš° '1.5e-3'ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. ë” ë†’ì„ìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ ë‹¤ìŒê³¼ ê°™ì´ í­ë°œí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ë‹¤ë¥¸ ê´€ë ¨ ì„¤ì •ì€ 'LoRA'ì™€ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤.

```json
{
  "--lora_rank": 768,
  "--lora_alpha": 768,
  "--lora_type": "standard"
}
```


ê°œì¸ì ìœ¼ë¡œëŠ” ì¢€ ë” ë†’ì€ 'LoRA' ë­í¬ì™€ ì•ŒíŒŒë¥¼ ì‚¬ìš©í•´ ì•„ì£¼ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ë‚´ YouTube [ì±„ë„](https://youtube.com/@kasukanra)ì—ì„œ 'LoRA' ìˆœìœ„ë¥¼ ë†’ì¼ìˆ˜ë¡ ì´ë¯¸ì§€ ì¶©ì‹¤ë„ê°€ ì–´ë–»ê²Œ ì¦ê°€í•˜ëŠ”ì§€ì— ëŒ€í•œ ë³´ë‹¤ ì •í™•í•œ ê²½í—˜ì  ë¶„ì„ì„ ë³´ë ¤ë©´ ìµœì‹  ë™ì˜ìƒì„ ì‹œì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. .

ì–´ì¨Œë“  VRAM, ì €ì¥ ìš©ëŸ‰ ë˜ëŠ” ê·¸ë ‡ê²Œ ë†’ì•„ì§ˆ ì‹œê°„ì´ ì—†ë‹¤ë©´ '256' ë˜ëŠ” '128'ê³¼ ê°™ì´ ë” ë‚®ì€ ê°’ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

`lora_type`ì— ê´€í•´ì„œëŠ”, ë‚˜ëŠ” ì‹œë„ë˜ê³  ì§„ì‹¤ëœ `standard`ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. 'LoRA'ì˜ 'lycoris' ìœ í˜•ì— ëŒ€í•œ ë˜ ë‹¤ë¥¸ ì˜µì…˜ì´ ìˆì§€ë§Œ ì•„ì§ì€ ë§¤ìš° ì‹¤í—˜ì ì´ë©° ì˜ íƒìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‚˜ëŠ” 'lycoris'ì— ëŒ€í•´ ì§ì ‘ ì‹¬ì¸µ ë¶„ì„í–ˆì§€ë§Œ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ì˜¬ë°”ë¥¸ ì„¤ì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

### Custom `config.json` miscellaneous

ì‚¶ì˜ ì§ˆì„ ìœ„í•´ ë³€ê²½í•  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ì¶”ê°€ ì„¤ì •ì´ ìˆìŠµë‹ˆë‹¤.

```json
{
  "--validation_prompt": "k4s4, a waist up view of a beautiful blonde woman, green eyes",
  "--validation_guidance": 7.5,
  "--validation_steps": 200,
  "--validation_num_inference_steps": 30,
  "--validation_negative_prompt": "blurry, cropped, ugly",
  "--validation_seed": 42,
  "--lr_scheduler": "cosine",
  "--lr_warmup_steps": 2400,
}
```

`"--validation_prompt": "k4s4, a waist up view of a beautiful blonde woman, green eyes"`

`"--validation_guidance": 7.5`
`"--validation_steps": 200`
`"--validation_num_inference_steps": 30`
`"--validation_negative_prompt": "blurry, cropped, ugly"`

`"--lr_scheduler": "cosine"`

`"--lr_warmup_steps": 2400`

ì´ê²ƒë“¤ì€ ë§¤ìš° ìëª…í•©ë‹ˆë‹¤:

`"--validation_prompt"`

ê²€ì¦ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤. ì´ê²ƒì´ ë‹¹ì‹ ì˜ ê¸ì •ì ì¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤.

`"--validation_negative_prompt"`

ë¶€ì •ì ì¸ í”„ë¡¬í”„íŠ¸.

`"--validation_guidance"`

Classifier free guidance (CFG) scale.

`"--validation_num_inference_steps"`

ì‚¬ìš©í•  ìƒ˜í”Œë§ ë‹¨ê³„ ìˆ˜ì…ë‹ˆë‹¤.

`"--validation_seed"`

ê²€ì¦ ì´ë¯¸ì§€ ìƒì„± ì‹œ ì‹œë“œ ê°’ì…ë‹ˆë‹¤.

`"--lr_warmup_steps"`

'SimpleTuner'ëŠ” ì„¤ì •í•˜ì§€ ì•Šì„ ê²½ìš° ê¸°ë³¸ ì›Œë°ì—…ì„ ì „ì²´ í›ˆë ¨ ë‹¨ê³„ì˜ '10%'ë¡œ ì„¤ì •í–ˆëŠ”ë°, ì´ëŠ” ì œê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” ê°’ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ (`24,000` * `0.1` = `2,400`)ì— í•˜ë“œì½”ë”©í–ˆìŠµë‹ˆë‹¤. ììœ ë¡­ê²Œ ë³€ê²½í•´ ë³´ì„¸ìš”.

`"--validation_steps"`

ê²€ì¦ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë ¤ëŠ” ë¹ˆë„ëŠ” `"--validation_steps"`ë¡œ ì„¤ì •ë©ë‹ˆë‹¤. ì €ëŠ” 400ì˜ 1/2ì¸ 200ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤(íŒíƒ€ì§€ ì•„íŠ¸ ì˜ˆì œ ë°ì´í„°ì„¸íŠ¸ì— ëŒ€í•œ í•œ ì‹œëŒ€ì˜ ë‹¨ê³„ ìˆ˜). ì´ëŠ” ì—í¬í¬ì˜ 1/2ë§ˆë‹¤ ê²€ì¦ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì˜¨ì „í•œ í™•ì¸ì„ ìœ„í•´ ìµœì†Œí•œ ë°˜ê¸°ì ë§ˆë‹¤ ê²€ì¦ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ìµœëŒ€í•œ ë¹¨ë¦¬ ì˜¤ë¥˜ë¥¼ í¬ì°©í•˜ì§€ ëª»í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.


ë§ˆì§€ë§‰ìœ¼ë¡œ `"--lr_scheduler"`ì™€ `"--lr_warmup_steps"`ì…ë‹ˆë‹¤.

ì €ëŠ” 'ì½”ì‚¬ì¸' ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ëª¨ìŠµì…ë‹ˆë‹¤.


### What happened to the low-level `config.env` ?

ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ `SimpleTuner`ëŠ” ë‚®ì€ ìˆ˜ì¤€ì˜ `config.env` í˜•ì‹ì—ì„œ ë²—ì–´ë‚˜ ì‚¬ìš© í¸ì˜ì„±ì„ ìœ„í•´ `json`ì„ ì„ íƒí•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ë‹¤ë¥¸ êµìœ¡ ë¦¬í¬ì§€í† ë¦¬ë„ `json`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ [loader.py](https://github.com/bghira/SimpleTuner/blob/main/helpers/configuration/loader.py#L17)ì˜ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ìœ„ ìˆ˜ì¤€ `config.env`ëŠ” ê³„ì† ì§€ì›ë©ë‹ˆë‹¤. . ë˜í•œ ì´ì „ì˜ ë‚®ì€ ìˆ˜ì¤€ `config.env` íŒŒì¼ì´ ì´ë¯¸ ìˆëŠ” `SimpleTuner`ì˜ ì´ì „ ì‚¬ìš©ìëŠ” íŒŒì¼ í˜•ì‹ì„ ì „í™˜í•˜ì§€ ì•Šê³ ë„ ì¼ë¶€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¡°ì •í•˜ì—¬ ì‹ ì†í•˜ê²Œ ì†ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤(í•´ë‹¹ [OPTIONS.MD](https //github.com/bghira/SimpleTuner/blob/main/OPTIONS.md#environment-configuration-variables)).

ì´ëŠ” ìœ„ì˜ `config.json`ê³¼ ë™ì¼í•œ ë²„ì „ì´ì§€ë§Œ `.env` í˜•ì‹ì…ë‹ˆë‹¤.
- 
- Custom SD3.5 Large `LoRA` `config.env`

    ```bash
    export MODEL_TYPE='lora'
    export MODEL_FAMILY='sd3'
    export CONTROLNET=false
    export USE_DORA=false
    # Restart where we left off. Change this to "checkpoint-1234" to start from a specific checkpoint.
    export RESUME_CHECKPOINT="latest"
    export CHECKPOINTING_STEPS=400
    # This is how many checkpoints we will keep. Two is safe, but three is safer.
    export CHECKPOINTING_LIMIT=60
    
    # This is decided as a relatively conservative 'constant' learning rate.
    # Adjust higher or lower depending on how burnt your model becomes.
    export LEARNING_RATE=1.05e-3
    
    # Using a Huggingface Hub model:
    export MODEL_NAME="stabilityai/stable-diffusion-3.5-large"
    
    # Make DEBUG_EXTRA_ARGS empty to disable wandb.
    export DEBUG_EXTRA_ARGS="--report_to=wandb"
    export TRACKER_PROJECT_NAME="sd35-training"
    export TRACKER_RUN_NAME="simpletuner-fantasy-art-lora-01"
    
    # Max number of steps OR epochs can be used. Not both.
    export MAX_NUM_STEPS=24000
    export NUM_EPOCHS=0
    
    # A convenient prefix for all of your training paths.
    export DATALOADER_CONFIG="/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json"
    export OUTPUT_DIR="/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models"
    # Set this to "true" to push your model to Hugging Face Hub.
    export PUSH_TO_HUB="false"
    # If PUSH_TO_HUB and PUSH_CHECKPOINTS are both enabled, every saved checkpoint will be pushed to Hugging Face Hub.
    export PUSH_CHECKPOINTS="true"
    # This will be the model name for your final hub upload, eg. "yourusername/yourmodelname"
    # It defaults to the wandb project name, but you can override this here.
    # export HUB_MODEL_NAME=$TRACKER_PROJECT_NAME
    
    # By default, images will be resized so their SMALLER EDGE is 1024 pixels, maintaining aspect ratio.
    # Setting this value to 768px might result in more reasonable training data sizes for SDXL.
    export RESOLUTION=1024
    # If you want to have the training data resized by pixel area (Megapixels) rather than edge length,
    #  set this value to "area" instead of "pixel", and uncomment the next RESOLUTION declaration.
    export RESOLUTION_TYPE="pixel"
    #export RESOLUTION=1          # 1.0 Megapixel training sizes
    # If RESOLUTION_TYPE="pixel", the minimum resolution specifies the smaller edge length, measured in pixels. Recommended: 1024.
    # If RESOLUTION_TYPE="area", the minimum resolution specifies the total image area, measured in megapixels. Recommended: 1.
    export MINIMUM_RESOLUTION=1024
    
    # How many decimals to round aspect buckets to.
    #export ASPECT_BUCKET_ROUNDING=2
    
    # Use this to append an instance prompt to each caption, used for adding trigger words.
    # This has not been tested in SDXL.
    export INSTANCE_PROMPT="k4s4 "
    # If you also supply a user prompt library or `--use_prompt_library`, this will be added to those lists.
    export VALIDATION_PROMPT="k4s4, a waist up view of a beautiful blonde woman, green eyes"
    export VALIDATION_GUIDANCE=7.5
    # You'll want to set this to 0.7 if you are training a terminal SNR model.
    export VALIDATION_GUIDANCE_RESCALE=0.0
    # How frequently we will save and run a pipeline for validations.
    # export VALIDATION_STEPS=200
    export VALIDATION_STEPS=70
    export VALIDATION_NUM_INFERENCE_STEPS=30
    
    export VALIDATION_NEGATIVE_PROMPT="blurry, cropped, ugly"
    export VALIDATION_SEED=42
    export VALIDATION_RESOLUTION=1024
    
    # Adjust this for your GPU memory size. This, and resolution, are the biggest VRAM killers.
    export TRAIN_BATCH_SIZE=6
    # Accumulate your update gradient over many steps, to save VRAM while still having higher effective batch size:
    # effective batch size = ($TRAIN_BATCH_SIZE * $GRADIENT_ACCUMULATION_STEPS).
    export GRADIENT_ACCUMULATION_STEPS=1
    
    # Use any standard scheduler type. constant, polynomial, constant_with_warmup
    export LR_SCHEDULE="cosine"
    # A warmup period allows the model and the EMA weights more importantly to familiarise itself with the current quanta.
    # For the cosine or sine type schedules, the warmup period defines the interval between peaks or valleys.
    # Use a sine schedule to simulate a warmup period, or a Cosine period to simulate a polynomial start.
    # export LR_WARMUP_STEPS=$((MAX_NUM_STEPS / 10))
    export LR_WARMUP_STEPS=2400
    
    # Caption dropout probability. Set to 0.1 for 10% of captions dropped out. Set to 0 to disable.
    # You may wish to disable dropout if you want to limit your changes strictly to the prompts you show the model.
    # You may wish to increase the rate of dropout if you want to more broadly adopt your changes across the model.
    export CAPTION_DROPOUT_PROBABILITY=0
    
    export METADATA_UPDATE_INTERVAL=65
    export VAE_BATCH_SIZE=12
    
    # If this is set, any images that fail to open will be DELETED to avoid re-checking them every time.
    export DELETE_ERRORED_IMAGES=0
    # If this is set, any images that are too small for the minimum resolution size will be DELETED.
    export DELETE_SMALL_IMAGES=0
    
    # Bytedance recommends these be set to "trailing" so that inference and training behave in a more congruent manner.
    # To follow the original SDXL training strategy, use "leading" instead, though results are generally worse.
    export TRAINING_SCHEDULER_TIMESTEP_SPACING="trailing"
    export INFERENCE_SCHEDULER_TIMESTEP_SPACING="trailing"
    
    # Removing this option or unsetting it uses vanilla training. Setting it reweights the loss by the position of the timestep in the noise schedule.
    # A value "5" is recommended by the researchers. A value of "20" is the least impact, and "1" is the most impact.
    export MIN_SNR_GAMMA=5
    
    # Set this to an explicit value of "false" to disable Xformers. Probably required for AMD users.
    export USE_XFORMERS=true
    
    # There's basically no reason to unset this. However, to disable it, use an explicit value of "false".
    # This will save a lot of memory consumption when enabled.
    export USE_GRADIENT_CHECKPOINTING=true
    
    ##
    # Options below here may require a bit more complicated configuration, so they are not simple variables.
    ##
    
    # TF32 is great on Ampere or Ada, not sure about earlier generations.
    export ALLOW_TF32=true
    # AdamW 8Bit is a robust and lightweight choice. Adafactor might reduce memory consumption, and Dadaptation is slow and experimental.
    # AdamW is the default optimizer, but it uses a lot of memory and is slower than AdamW8Bit or Adafactor.
    # Choices: adamw, adamw8bit, adafactor, dadaptation
    # export OPTIMIZER="adamw_bf16"
    export OPTIMIZER="adamw_bf16"
    
    # EMA is a strong regularisation method that uses a lot of extra VRAM to hold two copies of the weights.
    # This is worthwhile on large training runs, but not so much for smaller training runs.
    export USE_EMA=false
    export EMA_DECAY=0.999
    
    export TRAINER_EXTRA_ARGS="--lora_rank=768 --lora_alpha=768"
    
    # Reproducible training. Set to -1 to disable.
    export TRAINING_SEED=42
    
    # Mixed precision is the best. You honestly might need to YOLO it in fp16 mode for Google Colab type setups.
    export MIXED_PRECISION="bf16"
    export PURE_BF16=true
    
    # This has to be changed if you're training with multiple GPUs.
    export TRAINING_NUM_PROCESSES=1
    export TRAINING_NUM_MACHINES=1
    export ACCELERATE_EXTRA_ARGS=""
    
    # With Pytorch 2.1, you might have pretty good luck here.
    # If you're using aspect bucketing however, each resolution change will recompile. Seriously, just don't do it.
    # Well, then again... Pytorch 2.2 has support for dynamic shapes. Why not?
    export TRAINING_DYNAMO_BACKEND='no'
    
    export TOKENIZERS_PARALLELISM=false
    ```


[**â˜ï¸](https://emojipedia.org/index-pointing-up)** `LoRA` ìˆœìœ„/ì•ŒíŒŒëŠ” `TRAINER_EXTRA_ARGS` ë³€ìˆ˜ ë‚´ì—ì„œ ë³€ê²½ë  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ì§€ì í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.

```bash
export TRAINER_EXTRA_ARGS="--lora_rank=768 --lora_alpha=768"
```

[**âš ï¸](https://emojipedia.org/warning)** `.env` í˜•ì‹ì„ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì •í•œ ê²½ìš° ì¸ë¼ì¸ ì£¼ì„, ì°¸ì¡° ë³€ìˆ˜ ë˜ëŠ” ê³„ì‚°ì´ ì—†ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì´ê²ƒì€ ìƒˆë¡œìš´ `SimpleTuner` [env ë„ìš°ë¯¸](https://github.com/bghira/SimpleTuner/blob/main/helpers/configuration/env_file.py#L94)ê°€ ì‘ë™í•˜ëŠ” ë°©ì‹ì´ë¯€ë¡œ ëª¨ë“  ê²ƒì„ í•˜ë“œ ì½”ë”©í•´ì•¼ í•©ë‹ˆë‹¤. . ****ì˜ˆë¥¼ ë“¤ì–´:

**Failure case 1 (inline comments):**

```bash
export LEARNING_RATE=1.05e-3 #@param {type:"number"}
```

**Failure case 2 (reference variable with `TRAINER_EXTRA_ARGS`):**

```bash
export TRAINER_EXTRA_ARGS="${TRAINER_EXTRA_ARGS} --offset_noise --noise_offset=0.02"
```

**Failure case 3 (calculations)**:

```bash
export LR_WARMUP_STEPS=$((MAX_NUM_STEPS / 10))
```

ì›í•˜ëŠ” ê²½ìš° ìœ„ì˜ í•˜ìœ„ ìˆ˜ì¤€ `config.env`ë¥¼ ê¸°ë³¸ ì°¸ì¡°ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ìœ„ ìˆ˜ì¤€ `env` íŒŒì¼ì„ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì •í•œ ê²½ìš° ìƒìœ„ ìˆ˜ì¤€ `config.env`ì—ì„œ `CONFIG_BACKEND`ë¥¼ `env`ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”.

```bash
TRAINING_NUM_PROCESSES=1
TRAINING_NUM_MACHINES=1
TRAINING_DYNAMO_BACKEND='no'
MIXED_PRECISION='bf16'
export CONFIG_BACKEND="env"
export ENV="sd35_fantasy_art_lora"
```

## Training process

ë§ˆì§€ë§‰ìœ¼ë¡œ í›ˆë ¨ ê³¼ì •ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì°¸ê³ ìš©ìœ¼ë¡œ í•„ìš”í•œ ëª¨ë“  íŒŒì¼ì„ ì—¬ê¸°ì— ê°€ì ¸ì˜¤ê² ìŠµë‹ˆë‹¤.

- High-level `config.env`

    ```bash
    TRAINING_NUM_PROCESSES=1
    TRAINING_NUM_MACHINES=1
    TRAINING_DYNAMO_BACKEND='no'
    MIXED_PRECISION='bf16'
    export CONFIG_BACKEND="json"
    export ENV="sd35_fantasy_art_lora"
    ```

- Custom SD3.5 Large `config.json` for LoRA training

    ```json
    {
      "--model_type": "lora",
      "--model_family": "sd3",
      "--resume_from_checkpoint": "latest",
      "--checkpointing_steps": 400,
      "--checkpoints_total_limit": 60,
      "--learning_rate": 1.05e-3,
      "--pretrained_model_name_or_path": "stabilityai/stable-diffusion-3.5-large",
      "--report_to": "wandb",
      "--tracker_project_name": "sd35-training",
      "--tracker_run_name": "simpletuner-fantasy-art-lora-01",
      "--max_train_steps": 24000,
      "--num_train_epochs": 0,
      "--data_backend_config": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json",
      "--output_dir": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models",
      "--push_to_hub": false,
      "--push_checkpoints_to_hub": true,
      "--hub_model_id": "sd35-training",
      "--resolution": 1024,
      "--resolution_type": "pixel",
      "--minimum_image_size": 1024,
      "--instance_prompt": "k4s4 ",
      "--validation_prompt": "k4s4, a waist up view of a beautiful blonde woman, green eyes",
      "--validation_guidance": 7.5,
      "--validation_guidance_rescale": 0.0,
      "--validation_steps": 200,
      "--validation_num_inference_steps": 30,
      "--validation_negative_prompt": "blurry, cropped, ugly",
      "--validation_seed": 42,
      "--validation_resolution": 1024,
      "--train_batch_size": 6,
      "--gradient_accumulation_steps": 1,
      "--lr_scheduler": "cosine",
      "--lr_warmup_steps": 2400,
      "--caption_dropout_probability": 0,
      "--metadata_update_interval": 65,
      "--vae_batch_size": 12,
      "--delete_unwanted_images": false,
      "--delete_problematic_images": false,
      "--training_scheduler_timestep_spacing": "trailing",
      "--inference_scheduler_timestep_spacing": "trailing",
      "--snr_gamma": 5,
      "--enable_xformers_memory_efficient_attention": true,
      "--gradient_checkpointing": true,
      "--allow_tf32": true,
      "--optimizer": "adamw_bf16",
      "--use_ema": false,
      "--ema_decay": 0.999,
      "--seed": 42,
      "--mixed_precision": "bf16",
      "--lora_rank": 768,
      "--lora_alpha": 768,
      "--lora_type": "standard"
    }
    ```

- Custom SD3.5 Large  `config.env` for LoRA training

    ```bash
     
    export MODEL_TYPE='lora'
    export MODEL_FAMILY='sd3'
    export CONTROLNET=false
    export USE_DORA=false
    # Restart where we left off. Change this to "checkpoint-1234" to start from a specific checkpoint.
    export RESUME_CHECKPOINT="latest"
    export CHECKPOINTING_STEPS=400
    # This is how many checkpoints we will keep. Two is safe, but three is safer.
    export CHECKPOINTING_LIMIT=60
    
    # This is decided as a relatively conservative 'constant' learning rate.
    # Adjust higher or lower depending on how burnt your model becomes.
    export LEARNING_RATE=1.05e-3
    
    # Using a Huggingface Hub model:
    export MODEL_NAME="stabilityai/stable-diffusion-3.5-large"
    
    # Make DEBUG_EXTRA_ARGS empty to disable wandb.
    export DEBUG_EXTRA_ARGS="--report_to=wandb"
    export TRACKER_PROJECT_NAME="sd35-training"
    export TRACKER_RUN_NAME="simpletuner-fantasy-art-lora-01"
    
    # Max number of steps OR epochs can be used. Not both.
    export MAX_NUM_STEPS=24000
    export NUM_EPOCHS=0
    
    # A convenient prefix for all of your training paths.
    export DATALOADER_CONFIG="/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json"
    export OUTPUT_DIR="/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models"
    # Set this to "true" to push your model to Hugging Face Hub.
    export PUSH_TO_HUB="false"
    # If PUSH_TO_HUB and PUSH_CHECKPOINTS are both enabled, every saved checkpoint will be pushed to Hugging Face Hub.
    export PUSH_CHECKPOINTS="true"
    # This will be the model name for your final hub upload, eg. "yourusername/yourmodelname"
    # It defaults to the wandb project name, but you can override this here.
    # export HUB_MODEL_NAME=$TRACKER_PROJECT_NAME
    
    # By default, images will be resized so their SMALLER EDGE is 1024 pixels, maintaining aspect ratio.
    # Setting this value to 768px might result in more reasonable training data sizes for SDXL.
    export RESOLUTION=1024
    # If you want to have the training data resized by pixel area (Megapixels) rather than edge length,
    #  set this value to "area" instead of "pixel", and uncomment the next RESOLUTION declaration.
    export RESOLUTION_TYPE="pixel"
    #export RESOLUTION=1          # 1.0 Megapixel training sizes
    # If RESOLUTION_TYPE="pixel", the minimum resolution specifies the smaller edge length, measured in pixels. Recommended: 1024.
    # If RESOLUTION_TYPE="area", the minimum resolution specifies the total image area, measured in megapixels. Recommended: 1.
    export MINIMUM_RESOLUTION=1024
    
    # How many decimals to round aspect buckets to.
    #export ASPECT_BUCKET_ROUNDING=2
    
    # Use this to append an instance prompt to each caption, used for adding trigger words.
    # This has not been tested in SDXL.
    export INSTANCE_PROMPT="k4s4 "
    # If you also supply a user prompt library or `--use_prompt_library`, this will be added to those lists.
    export VALIDATION_PROMPT="k4s4, a waist up view of a beautiful blonde woman, green eyes"
    export VALIDATION_GUIDANCE=7.5
    # You'll want to set this to 0.7 if you are training a terminal SNR model.
    export VALIDATION_GUIDANCE_RESCALE=0.0
    # How frequently we will save and run a pipeline for validations.
    # export VALIDATION_STEPS=200
    export VALIDATION_STEPS=70
    export VALIDATION_NUM_INFERENCE_STEPS=30
    
    export VALIDATION_NEGATIVE_PROMPT="blurry, cropped, ugly"
    export VALIDATION_SEED=42
    export VALIDATION_RESOLUTION=1024
    
    # Adjust this for your GPU memory size. This, and resolution, are the biggest VRAM killers.
    export TRAIN_BATCH_SIZE=6
    # Accumulate your update gradient over many steps, to save VRAM while still having higher effective batch size:
    # effective batch size = ($TRAIN_BATCH_SIZE * $GRADIENT_ACCUMULATION_STEPS).
    export GRADIENT_ACCUMULATION_STEPS=1
    
    # Use any standard scheduler type. constant, polynomial, constant_with_warmup
    export LR_SCHEDULE="cosine"
    # A warmup period allows the model and the EMA weights more importantly to familiarise itself with the current quanta.
    # For the cosine or sine type schedules, the warmup period defines the interval between peaks or valleys.
    # Use a sine schedule to simulate a warmup period, or a Cosine period to simulate a polynomial start.
    # export LR_WARMUP_STEPS=$((MAX_NUM_STEPS / 10))
    export LR_WARMUP_STEPS=2400
    
    # Caption dropout probability. Set to 0.1 for 10% of captions dropped out. Set to 0 to disable.
    # You may wish to disable dropout if you want to limit your changes strictly to the prompts you show the model.
    # You may wish to increase the rate of dropout if you want to more broadly adopt your changes across the model.
    export CAPTION_DROPOUT_PROBABILITY=0
    
    export METADATA_UPDATE_INTERVAL=65
    export VAE_BATCH_SIZE=12
    
    # If this is set, any images that fail to open will be DELETED to avoid re-checking them every time.
    export DELETE_ERRORED_IMAGES=0
    # If this is set, any images that are too small for the minimum resolution size will be DELETED.
    export DELETE_SMALL_IMAGES=0
    
    # Bytedance recommends these be set to "trailing" so that inference and training behave in a more congruent manner.
    # To follow the original SDXL training strategy, use "leading" instead, though results are generally worse.
    export TRAINING_SCHEDULER_TIMESTEP_SPACING="trailing"
    export INFERENCE_SCHEDULER_TIMESTEP_SPACING="trailing"
    
    # Removing this option or unsetting it uses vanilla training. Setting it reweights the loss by the position of the timestep in the noise schedule.
    # A value "5" is recommended by the researchers. A value of "20" is the least impact, and "1" is the most impact.
    export MIN_SNR_GAMMA=5
    
    # Set this to an explicit value of "false" to disable Xformers. Probably required for AMD users.
    export USE_XFORMERS=true
    
    # There's basically no reason to unset this. However, to disable it, use an explicit value of "false".
    # This will save a lot of memory consumption when enabled.
    export USE_GRADIENT_CHECKPOINTING=true
    
    ##
    # Options below here may require a bit more complicated configuration, so they are not simple variables.
    ##
    
    # TF32 is great on Ampere or Ada, not sure about earlier generations.
    export ALLOW_TF32=true
    # AdamW 8Bit is a robust and lightweight choice. Adafactor might reduce memory consumption, and Dadaptation is slow and experimental.
    # AdamW is the default optimizer, but it uses a lot of memory and is slower than AdamW8Bit or Adafactor.
    # Choices: adamw, adamw8bit, adafactor, dadaptation
    # export OPTIMIZER="adamw_bf16"
    export OPTIMIZER="adamw_bf16"
    
    # EMA is a strong regularisation method that uses a lot of extra VRAM to hold two copies of the weights.
    # This is worthwhile on large training runs, but not so much for smaller training runs.
    export USE_EMA=false
    export EMA_DECAY=0.999
    
    export TRAINER_EXTRA_ARGS="--lora_rank=768 --lora_alpha=768"
    
    # Reproducible training. Set to -1 to disable.
    export TRAINING_SEED=42
    
    # Mixed precision is the best. You honestly might need to YOLO it in fp16 mode for Google Colab type setups.
    export MIXED_PRECISION="bf16"
    export PURE_BF16=true
    
    # This has to be changed if you're training with multiple GPUs.
    export TRAINING_NUM_PROCESSES=1
    export TRAINING_NUM_MACHINES=1
    export ACCELERATE_EXTRA_ARGS=""
    
    # With Pytorch 2.1, you might have pretty good luck here.
    # If you're using aspect bucketing however, each resolution change will recompile. Seriously, just don't do it.
    # Well, then again... Pytorch 2.2 has support for dynamic shapes. Why not?
    export TRAINING_DYNAMO_BACKEND='no'
    
    export TOKENIZERS_PARALLELISM=false
    ```

- Default [train.sh](http://train.sh)

    ```bash
    #!/usr/bin/env bash
    
    # Pull config from config.env
    [ -f "config/config.env" ] && source config/config.env
    
    # If the user has not provided VENV_PATH, we will assume $(pwd)/.venv
    if [ -z "${VENV_PATH}" ]; then
        # what if we have VIRTUAL_ENV? use that instead
        if [ -n "${VIRTUAL_ENV}" ]; then
            export VENV_PATH="${VIRTUAL_ENV}"
        else
            export VENV_PATH="$(pwd)/.venv"
        fi
    fi
    if [ -z "${DISABLE_LD_OVERRIDE}" ]; then
        export NVJITLINK_PATH="$(find "${VENV_PATH}" -name nvjitlink -type d)/lib"
        # if it's not empty, we will add it to LD_LIBRARY_PATH at the front:
        if [ -n "${NVJITLINK_PATH}" ]; then
            export LD_LIBRARY_PATH="${NVJITLINK_PATH}:${LD_LIBRARY_PATH}"
        fi
    fi
    
    export TOKENIZERS_PARALLELISM=false
    export PLATFORM
    PLATFORM=$(uname -s)
    if [[ "$PLATFORM" == "Darwin" ]]; then
        export MIXED_PRECISION="no"
    fi
    
    if [ -z "${ACCELERATE_EXTRA_ARGS}" ]; then
        ACCELERATE_EXTRA_ARGS=""
    fi
    
    if [ -z "${TRAINING_NUM_PROCESSES}" ]; then
        echo "Set custom env vars permanently in config/config.env:"
        printf "TRAINING_NUM_PROCESSES not set, defaulting to 1.\n"
        TRAINING_NUM_PROCESSES=1
    fi
    
    if [ -z "${TRAINING_NUM_MACHINES}" ]; then
        printf "TRAINING_NUM_MACHINES not set, defaulting to 1.\n"
        TRAINING_NUM_MACHINES=1
    fi
    
    if [ -z "${MIXED_PRECISION}" ]; then
        printf "MIXED_PRECISION not set, defaulting to bf16.\n"
        MIXED_PRECISION=bf16
    fi
    
    if [ -z "${TRAINING_DYNAMO_BACKEND}" ]; then
        printf "TRAINING_DYNAMO_BACKEND not set, defaulting to no.\n"
        TRAINING_DYNAMO_BACKEND="no"
    fi
    
    if [ -z "${ENV}" ]; then
        printf "ENV not set, defaulting to default.\n"
        export ENV="default"
    fi
    export ENV_PATH=""
    if [[ "$ENV" != "default" ]]; then
        export ENV_PATH="${ENV}/"
    fi
    
    if [ -z "${CONFIG_BACKEND}" ]; then
        if [ -n "${CONFIG_TYPE}" ]; then
            export CONFIG_BACKEND="${CONFIG_TYPE}"
        fi
    fi
    
    if [ -z "${CONFIG_BACKEND}" ]; then
        export CONFIG_BACKEND="env"
        export CONFIG_PATH="config/${ENV_PATH}config"
        if [ -f "${CONFIG_PATH}.json" ]; then
            export CONFIG_BACKEND="json"
        elif [ -f "${CONFIG_PATH}.toml" ]; then
            export CONFIG_BACKEND="toml"
        elif [ -f "${CONFIG_PATH}.env" ]; then
            export CONFIG_BACKEND="env"
        fi
        echo "Using ${CONFIG_BACKEND} backend: ${CONFIG_PATH}.${CONFIG_BACKEND}"
    fi
    
    # Update dependencies
    if [ -z "${DISABLE_UPDATES}" ]; then
        echo 'Updating dependencies. Set DISABLE_UPDATES to prevent this.'
        if [ -f "pyproject.toml" ] && [ -f "poetry.lock" ]; then
            nvidia-smi 2> /dev/null && poetry install
            uname -s | grep -q Darwin && poetry install -C install/apple
            rocm-smi 2> /dev/null && poetry install -C install/rocm
        fi
    fi
    # Run the training script.
    if [[ -z "${ACCELERATE_CONFIG_PATH}" ]]; then
        ACCELERATE_CONFIG_PATH="${HOME}/.cache/huggingface/accelerate/default_config.yaml"
    fi
    if [ -f "${ACCELERATE_CONFIG_PATH}" ]; then
        echo "Using Accelerate config file: ${ACCELERATE_CONFIG_PATH}"
        accelerate launch --config_file="${ACCELERATE_CONFIG_PATH}" train.py
    else
        echo "Accelerate config file not found: ${ACCELERATE_CONFIG_PATH}. Using values from config.env."
        accelerate launch ${ACCELERATE_EXTRA_ARGS} --mixed_precision="${MIXED_PRECISION}" --num_processes="${TRAINING_NUM_PROCESSES}" --num_machines="${TRAINING_NUM_MACHINES}" --dynamo_backend="${TRAINING_DYNAMO_BACKEND}" train.py
    
    fi
    
    exit 0
    ```


### Possible `accelerate` issues

ì—¬ê¸°ì„œëŠ” í›ˆë ¨ì„ ì‹œì‘í•˜ëŠ” ë° ë°©í•´ê°€ ë  ìˆ˜ ìˆëŠ” í•œ ê°€ì§€ ì‘ì€ ì‚¬í•­ì„ ì–¸ê¸‰í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ë ë¶€ë¶„ì— ìˆëŠ” ê¸°ë³¸ `train.sh` ì•ˆì—ëŠ” í›ˆë ¨ì„ ì‹¤í–‰í•˜ëŠ” ëª…ë ¹ì´ ìˆìŠµë‹ˆë‹¤.

```bash
accelerate launch --config_file="${ACCELERATE_CONFIG_PATH}" train.py
```

```python
# Run the training script.
if [[ -z "${ACCELERATE_CONFIG_PATH}" ]]; then
    ACCELERATE_CONFIG_PATH="${HOME}/.cache/huggingface/accelerate/default_config.yaml"
fi
if [ -f "${ACCELERATE_CONFIG_PATH}" ]; then
    echo "Using Accelerate config file: ${ACCELERATE_CONFIG_PATH}"
    accelerate launch --config_file="${ACCELERATE_CONFIG_PATH}" train.py
else
    echo "Accelerate config file not found: ${ACCELERATE_CONFIG_PATH}. Using values from config.env."
    accelerate launch ${ACCELERATE_EXTRA_ARGS} --mixed_precision="${MIXED_PRECISION}" --num_processes="${TRAINING_NUM_PROCESSES}" --num_machines="${TRAINING_NUM_MACHINES}" --dynamo_backend="${TRAINING_DYNAMO_BACKEND}" train.py

fi
```


ì´ê²ƒì´ ì²˜ìŒìœ¼ë¡œ í›ˆë ¨ ì €ì¥ì†Œë¥¼ ì„¤ì¹˜í•˜ëŠ” ê²ƒì´ë¼ë©´ ì•„ë§ˆë„ ì˜¤ë¥˜ ì—†ì´ ì‹¤í–‰ë  ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¤ë¥¸ ì €ì¥ì†Œì—ì„œ `accelerate`ë¥¼ ì‚¬ìš©í•œ ê²½ìš° `default_config.yaml`ì„ ì´ë¯¸ êµ¬ì„±í–ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. í›ˆë ¨ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²½ìš° ì¼ë°˜ í›ˆë ¨ì„ ìœ„í•´ ì—¬ê¸°ì— ìì²´ `config.yaml`ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤. ë˜í•œ 'LoRA' êµìœ¡ì´ ì•„ë‹Œ ì™„ì „í•œ ë¯¸ì„¸ ì¡°ì •ì„ ì‹œë„í•˜ë ¤ëŠ” ê²½ìš° 'DeepSpeed' 'config.yaml'ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.


'DeepSpeed'ëŠ” GPU VRAMì´ ì¶©ë¶„í•˜ì§€ ì•Šì„ ë•Œ ë‚´ë¶€ì˜ íŠ¹ìˆ˜ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ìµœì í™” ìƒíƒœ, ê·¸ë¼ë°ì´ì…˜ ë° ê¸°íƒ€ ë§¤ê°œë³€ìˆ˜ë¥¼ CPU ë©”ëª¨ë¦¬(RAM)ë¡œ ì˜¤í”„ë¡œë“œí•©ë‹ˆë‹¤. '80GB' VRAMê³¼ '128GB' CPU RAMì„ ê°–ì¶˜ ë‹¨ì¼ 'H100' GPUì—ì„œëŠ” 'SD3.5 Large'ë¡œ ì™„ì „í•œ ë¯¸ì„¸ ì¡°ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. VRAMì´ ë¶€ì¡±í•˜ê³  CPU RAMìœ¼ë¡œ ì˜¤í”„ë¡œë“œí•´ì•¼ í•  ë•Œë§ˆë‹¤ ì´ `config.yaml`ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

- Custom general use `base_config.yaml`

    ```yaml
    compute_environment: LOCAL_MACHINE
    debug: false
    distributed_type: 'NO'
    downcast_bf16: 'no'
    enable_cpu_affinity: false
    gpu_ids: all
    machine_rank: 0
    main_training_function: main
    mixed_precision: bf16
    num_machines: 1
    num_processes: 1
    rdzv_backend: static
    same_network: true
    tpu_env: []
    tpu_use_cluster: false
    tpu_use_sudo: false
    use_cpu: false
    ```

- Custom `DeepSpeed 2` `deepspeed_config.yaml`

    ```yaml
    compute_environment: LOCAL_MACHINE
    debug: false
    deepspeed_config:
      gradient_accumulation_steps: 8
      gradient_clipping: 1.0
      offload_optimizer_device: cpu
      offload_param_device: cpu
      zero3_init_flag: false
      zero_stage: 2
    distributed_type: DEEPSPEED
    downcast_bf16: 'no'
    enable_cpu_affinity: true
    machine_rank: 0
    main_training_function: main
    mixed_precision: bf16
    num_machines: 1
    num_processes: 1
    rdzv_backend: static
    same_network: true
    tpu_env: []
    tpu_use_cluster: false
    tpu_use_sudo: false
    use_cpu: false
    ```


ì´ëŸ¬í•œ `yaml` íŒŒì¼ì„ ë°°ì¹˜í•  ìœ„ì¹˜ë¥¼ ì„ íƒí•  ë•Œë§ˆë‹¤ `train.sh` ì½”ë“œì—ì„œ í•´ë‹¹ íŒŒì¼ì„ ì˜¬ë°”ë¥´ê²Œ ì°¸ì¡°í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì €ëŠ” `SimpleTuner` ë””ë ‰í† ë¦¬ì˜ ë£¨íŠ¸ì— íŒŒì¼ì„ ë°°ì¹˜í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì½”ë“œì˜ 'ACCELERATE_CONFIG_PATH' ë¶€ë¶„ì´ ê·¸ì— ë”°ë¼ ìˆ˜ì •ë©ë‹ˆë‹¤.

```bash
# Run the training script with base config.
if [[ -z "${ACCELERATE_CONFIG_PATH}" ]]; then
    ACCELERATE_CONFIG_PATH="${HOME}/SimpleTuner/base_config.yaml"
fi
```

or

```bash
# Run the training script with DeepSpeed config.
if [[ -z "${ACCELERATE_CONFIG_PATH}" ]]; then
    ACCELERATE_CONFIG_PATH="${HOME}/SimpleTuner/deepspeed_config.yaml"
fi
```

ê²°êµ­ `DeepSpeed` ì§€ì› í›ˆë ¨ì„ ì‹œë„í•˜ê²Œ ëœë‹¤ë©´, ì´ì— ë”°ë¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•˜ìœ„ ìˆ˜ì¤€ `config.env` ìƒ˜í”Œì´ ìˆìŠµë‹ˆë‹¤.

- Custom SD3.5 Large `full` fine-tune`config.json`

    ```json
    {
      "--model_type": "full",
      "--model_family": "sd3",
      "--resume_from_checkpoint": "latest",
      "--checkpointing_steps": 100,
      "--checkpoints_total_limit": 100,
      "--learning_rate": 5e-5,
      "--pretrained_model_name_or_path": "stabilityai/stable-diffusion-3.5-large",
      "--report_to": "wandb",
      "--tracker_project_name": "sd35-training",
      "--tracker_run_name": "simpletuner-fantasy-art-full-01",
      "--max_train_steps": 24000,
      "--num_train_epochs": 0,
      "--data_backend_config": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/multidatabackend.json",
      "--output_dir": "/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models",
      "--push_to_hub": false,
      "--push_checkpoints_to_hub": true,
      "--hub_model_id": "sd35-training",
      "--resolution": 1024,
      "--resolution_type": "pixel",
      "--minimum_image_size": 1024,
      "--instance_prompt": "k4s4 ",
      "--validation_prompt": "k4s4, a waist up view of a beautiful blonde woman, green eyes",
      "--validation_guidance": 7.5,
      "--validation_guidance_rescale": 0.0,
      "--validation_steps": 25,
      "--validation_num_inference_steps": 30,
      "--validation_negative_prompt": "blurry, cropped, ugly",
      "--validation_seed": 42,
      "--validation_resolution": 1024,
      "--train_batch_size": 6,
      "--gradient_accumulation_steps": 1,
      "--lr_scheduler": "cosine",
      "--lr_warmup_steps": 2400,
      "--caption_dropout_probability": 0,
      "--metadata_update_interval": 65,
      "--vae_batch_size": 12,
      "--delete_unwanted_images": false,
      "--delete_problematic_images": false,
      "--training_scheduler_timestep_spacing": "trailing",
      "--inference_scheduler_timestep_spacing": "trailing",
      "--snr_gamma": 5,
      "--enable_xformers_memory_efficient_attention": true,
      "--gradient_checkpointing": true,
      "--allow_tf32": true,
      "--optimizer": "adamw_bf16",
      "--use_ema": false,
      "--ema_decay": 0.999,
      "--seed": 42,
      "--mixed_precision": "bf16",
    }
    ```

- Custom SD3.5 Large `full` fine-tune`config.env`

    ```bash
     
    export MODEL_TYPE='full'
    export MODEL_FAMILY='sd3'
    export CONTROLNET=false
    export USE_DORA=false
    # Restart where we left off. Change this to "checkpoint-1234" to start from a specific checkpoint.
    export RESUME_CHECKPOINT="latest"
    export CHECKPOINTING_STEPS=100
    # This is how many checkpoints we will keep. Two is safe, but three is safer.
    export CHECKPOINTING_LIMIT=100
    
    # This is decided as a relatively conservative 'constant' learning rate.
    # Adjust higher or lower depending on how burnt your model becomes.
    export LEARNING_RATE=5e-5
    
    # Using a Huggingface Hub model:
    export MODEL_NAME="stabilityai/stable-diffusion-3.5-large"
    
    # Make DEBUG_EXTRA_ARGS empty to disable wandb.
    export DEBUG_EXTRA_ARGS="--report_to=wandb"
    export TRACKER_PROJECT_NAME="sd35-training"
    export TRACKER_RUN_NAME="simpletuner-fantasy-art-full-01"
    
    # Max number of steps OR epochs can be used. Not both.
    export MAX_NUM_STEPS=24000
    export NUM_EPOCHS=0
    
    # A convenient prefix for all of your training paths.
    export DATALOADER_CONFIG="/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_full_L_01/datasets/multidatabackend.json"
    export OUTPUT_DIR="/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_full_L_01/datasets/models"
    # Set this to "true" to push your model to Hugging Face Hub.
    export PUSH_TO_HUB="false"
    # If PUSH_TO_HUB and PUSH_CHECKPOINTS are both enabled, every saved checkpoint will be pushed to Hugging Face Hub.
    export PUSH_CHECKPOINTS="true"
    # This will be the model name for your final hub upload, eg. "yourusername/yourmodelname"
    # It defaults to the wandb project name, but you can override this here.
    # export HUB_MODEL_NAME=$TRACKER_PROJECT_NAME
    
    # By default, images will be resized so their SMALLER EDGE is 1024 pixels, maintaining aspect ratio.
    # Setting this value to 768px might result in more reasonable training data sizes for SDXL.
    export RESOLUTION=1024
    # If you want to have the training data resized by pixel area (Megapixels) rather than edge length,
    #  set this value to "area" instead of "pixel", and uncomment the next RESOLUTION declaration.
    export RESOLUTION_TYPE="pixel"
    #export RESOLUTION=1          # 1.0 Megapixel training sizes
    # If RESOLUTION_TYPE="pixel", the minimum resolution specifies the smaller edge length, measured in pixels. Recommended: 1024.
    # If RESOLUTION_TYPE="area", the minimum resolution specifies the total image area, measured in megapixels. Recommended: 1.
    export MINIMUM_RESOLUTION=1024
    
    # How many decimals to round aspect buckets to.
    #export ASPECT_BUCKET_ROUNDING=2
    
    # Use this to append an instance prompt to each caption, used for adding trigger words.
    # This has not been tested in SDXL.
    export INSTANCE_PROMPT="k4s4 "
    # If you also supply a user prompt library or `--use_prompt_library`, this will be added to those lists.
    export VALIDATION_PROMPT="k4s4, a waist up view of a beautiful blonde woman, green eyes"
    export VALIDATION_GUIDANCE=7.5
    # You'll want to set this to 0.7 if you are training a terminal SNR model.
    export VALIDATION_GUIDANCE_RESCALE=0.0
    # How frequently we will save and run a pipeline for validations.
    # export VALIDATION_STEPS=200
    export VALIDATION_STEPS=25
    export VALIDATION_NUM_INFERENCE_STEPS=30
    
    export VALIDATION_NEGATIVE_PROMPT="blurry, cropped, ugly"
    export VALIDATION_SEED=42
    export VALIDATION_RESOLUTION=1024
    
    # Adjust this for your GPU memory size. This, and resolution, are the biggest VRAM killers.
    export TRAIN_BATCH_SIZE=6
    # Accumulate your update gradient over many steps, to save VRAM while still having higher effective batch size:
    # effective batch size = ($TRAIN_BATCH_SIZE * $GRADIENT_ACCUMULATION_STEPS).
    export GRADIENT_ACCUMULATION_STEPS=1
    
    # Use any standard scheduler type. constant, polynomial, constant_with_warmup
    export LR_SCHEDULE="cosine"
    # A warmup period allows the model and the EMA weights more importantly to familiarise itself with the current quanta.
    # For the cosine or sine type schedules, the warmup period defines the interval between peaks or valleys.
    # Use a sine schedule to simulate a warmup period, or a Cosine period to simulate a polynomial start.
    # export LR_WARMUP_STEPS=$((MAX_NUM_STEPS / 10))
    export LR_WARMUP_STEPS=2400
    
    # Caption dropout probability. Set to 0.1 for 10% of captions dropped out. Set to 0 to disable.
    # You may wish to disable dropout if you want to limit your changes strictly to the prompts you show the model.
    # You may wish to increase the rate of dropout if you want to more broadly adopt your changes across the model.
    export CAPTION_DROPOUT_PROBABILITY=0
    
    export METADATA_UPDATE_INTERVAL=65
    export VAE_BATCH_SIZE=12
    
    # If this is set, any images that fail to open will be DELETED to avoid re-checking them every time.
    export DELETE_ERRORED_IMAGES=0
    # If this is set, any images that are too small for the minimum resolution size will be DELETED.
    export DELETE_SMALL_IMAGES=0
    
    # Bytedance recommends these be set to "trailing" so that inference and training behave in a more congruent manner.
    # To follow the original SDXL training strategy, use "leading" instead, though results are generally worse.
    export TRAINING_SCHEDULER_TIMESTEP_SPACING="trailing"
    export INFERENCE_SCHEDULER_TIMESTEP_SPACING="trailing"
    
    # Removing this option or unsetting it uses vanilla training. Setting it reweights the loss by the position of the timestep in the noise schedule.
    # A value "5" is recommended by the researchers. A value of "20" is the least impact, and "1" is the most impact.
    export MIN_SNR_GAMMA=5
    
    # Set this to an explicit value of "false" to disable Xformers. Probably required for AMD users.
    export USE_XFORMERS=true
    
    # There's basically no reason to unset this. However, to disable it, use an explicit value of "false".
    # This will save a lot of memory consumption when enabled.
    export USE_GRADIENT_CHECKPOINTING=true
    
    ##
    # Options below here may require a bit more complicated configuration, so they are not simple variables.
    ##
    
    # TF32 is great on Ampere or Ada, not sure about earlier generations.
    export ALLOW_TF32=true
    # AdamW 8Bit is a robust and lightweight choice. Adafactor might reduce memory consumption, and Dadaptation is slow and experimental.
    # AdamW is the default optimizer, but it uses a lot of memory and is slower than AdamW8Bit or Adafactor.
    # Choices: adamw, adamw8bit, adafactor, dadaptation
    # export OPTIMIZER="adamw_bf16"
    export OPTIMIZER="adamw_bf16"
    
    # EMA is a strong regularisation method that uses a lot of extra VRAM to hold two copies of the weights.
    # This is worthwhile on large training runs, but not so much for smaller training runs.
    export USE_EMA=false
    export EMA_DECAY=0.999
    
    export TRAINER_EXTRA_ARGS=""
    
    # Reproducible training. Set to -1 to disable.
    export TRAINING_SEED=42
    
    # Mixed precision is the best. You honestly might need to YOLO it in fp16 mode for Google Colab type setups.
    export MIXED_PRECISION="bf16"
    export PURE_BF16=true
    
    # This has to be changed if you're training with multiple GPUs.
    export TRAINING_NUM_PROCESSES=1
    export TRAINING_NUM_MACHINES=1
    export ACCELERATE_EXTRA_ARGS=""
    
    # With Pytorch 2.1, you might have pretty good luck here.
    # If you're using aspect bucketing however, each resolution change will recompile. Seriously, just don't do it.
    # Well, then again... Pytorch 2.2 has support for dynamic shapes. Why not?
    export TRAINING_DYNAMO_BACKEND='no'
    
    export TOKENIZERS_PARALLELISM=false
    ```


Changed parameters

```json
{
	"--model_type": "full",
	"--checkpointing_steps": 100,
  "--checkpoints_total_limit": 100,
  "--learning_rate": 5e-5,
  "--tracker_run_name": "simpletuner-fantasy-art-full-01"
}

```

íŠ¹íˆ í•™ìŠµë¥ ì´ '5e-5'ë¡œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤.

ëª¨ë“  ê²ƒì´ ì •ìƒì´ë©´ ê³„ì†í•´ì„œ í›ˆë ¨ì„ ì‹œì‘í•˜ì‹­ì‹œì˜¤.

```bash
bash train.sh
```

### Memory usage

í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ í›ˆë ¨í•˜ì§€ ì•ŠëŠ” ê²½ìš°(ìš°ë¦¬ëŠ” ê·¸ë ‡ì§€ ì•ŠìŠµë‹ˆë‹¤) 'SimpleTuner'ë¥¼ ì‚¬ìš©í•˜ë©´ ì•½ '10.4GB'ì˜ VRAMì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

'ë°°ì¹˜ í¬ê¸°'ë¥¼ '6'ìœ¼ë¡œ ì„¤ì •í•˜ê³  'lora ìˆœìœ„/ì•ŒíŒŒ'ë¥¼ '768'ë¡œ ì„¤ì •í•˜ë©´ í›ˆë ¨ì—ì„œ ì•½ '32GB'ì˜ VRAMì„ ì†Œë¹„í•©ë‹ˆë‹¤.

ë‹¹ì—°íˆ ì´ëŠ” ì†Œë¹„ì '24GB' VRAM GPUì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤. ê·¸ë˜ì„œ `batch size`ë¥¼ `1`, `lora Rank/alpha`ë¥¼ `128`ë¡œ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ë¹„ìš©ì„ ì¤„ì´ë ¤ê³  í–ˆìŠµë‹ˆë‹¤.

ì ì •ì ìœ¼ë¡œ VRAM ë¹„ìš©ì„ ì•½ '19.65GB' VRAMìœ¼ë¡œ ë‚®ì¶œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ìœ íš¨ì„± ê²€ì‚¬ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì¶”ë¡ ì„ ì‹¤í–‰í•˜ë©´ VRAMì´ ìµœëŒ€ '23.37GB'ê¹Œì§€ ê¸‰ì¦í•©ë‹ˆë‹¤.

ì•ˆì „ì„ ìœ„í•´ 'lora ìˆœìœ„/ì•ŒíŒŒ'ë¥¼ '64'ë¡œ ë”ìš± ì¤„ì—¬ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ í›ˆë ¨ ì¤‘ì— ì•½ '18.83GB'ì˜ VRAMì„ ì†Œë¹„í•˜ê²Œ ë©ë‹ˆë‹¤.

ê²€ì¦ ì¶”ë¡  ì¤‘ì—ëŠ” ìµœëŒ€ ì•½ '21.50GB'ì˜ VRAMì´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ì •ë„ë©´ ì¶©ë¶„íˆ ì•ˆì „í•´ ë³´ì…ë‹ˆë‹¤.

'ë°°ì¹˜ í¬ê¸°' '6' ë° 'lora ìˆœìœ„/ì•ŒíŒŒ' '768'ì˜ ë” ë†’ì€ ì‚¬ì–‘ êµìœ¡ì„ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì •í•œ ê²½ìš° [ìœ„](https:// www.notion.so/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6?pvs=21) GPU VRAMì´ ë¶€ì¡±í•˜ê³  CPU RAMì´ ì¶©ë¶„í•œ ê²½ìš°.

### Monitoring the training


í›ˆë ¨ ê³¼ì •ì—ì„œ ê²€ì¦ ì´ë¯¸ì§€ê°€ í”½ì…€í™”ë˜ê±°ë‚˜ ê²€ê²Œ ë³€í•˜ëŠ” ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” '1.05e-3'ì´ë¼ëŠ” ë§¤ìš° ê³µê²©ì ì¸ í•™ìŠµë¥ ì„ ì‚¬ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë” ì•ˆì „í•˜ê²Œ í”Œë ˆì´í•˜ê³  ì‹¶ë‹¤ë©´ '9.5e-4'ë¥¼ ì‚¬ìš©í•˜ë©´ í”½ì…€í™” ë¬¸ì œê°€ ê±°ì˜ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ë‘ ì†ì‹¤ ê³¡ì„ ì€ ê²°êµ­ í›Œë¥­í•˜ê²Œ ìˆ˜ë ´í–ˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ìš°ë ¤ì‚¬í•­ì„ í•´ì†Œí•˜ê¸° ìœ„í•´ ì–´ë–¤ ëª¨ìŠµì¼ì§€ ëª‡ ê°€ì§€ ì˜ˆë¥¼ ë³´ì—¬ë“œë¦¬ê³  ì‹¶ìŠµë‹ˆë‹¤.


### Observing training loss

### `LoRA`

íŒíƒ€ì§€ ì•„íŠ¸ 'LoRA' ìˆ˜ë ¨ì„ í†µí•´ ì–»ì€ í”¼ê·œì–´ë“¤ì…ë‹ˆë‹¤. ì†ì‹¤ì´ ê°ì†Œí•˜ê³  ìˆìœ¼ë©° ì•„ì§ ìˆ˜ë ´ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í™•ì‚° ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•œ ê²½í—˜ì´ ìˆëŠ” ê²½ìš° ì†ì‹¤ ìµœì†Œí™”ëŠ” ë¯¸ì  ê·¹ëŒ€í™”ì™€ ê±°ì˜ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤. ë˜í•œ ë†’ì€ í•™ìŠµë¥ ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì†ì‹¤ ê³¡ì„ ì˜ ìµœê³ ì  ê·¼ì²˜ì—ì„œ ê²€ì¦ ì´ë¯¸ì§€ì˜ í”½ì…€í™” ë˜ëŠ” í’ˆì§ˆ ì €í•˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. í›ˆë ¨ì´ ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šì€ í•™ìŠµ ì†ë„ì— ë„ë‹¬í•˜ë©´ ì´ëŠ” ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤.

í•™ìŠµë¥ ì´ ë†’ìœ¼ë©´ ì—´ì°¨ ì†ì‹¤ë„ ìµœê³ ì ì— ë‹¬í•©ë‹ˆë‹¤.

## Evaluating the results

### How to actually get the LoRA models into ComfyUI

ì´ì œ ëª¨ë¸ì´ ëª¨ë‘ í›ˆë ¨ë˜ì—ˆìœ¼ë¯€ë¡œ `ComfyUI`ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•  ì°¨ë¡€ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ SimpleTunerê°€ ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¸í•´ 'ComfyUI/models/loras' ë””ë ‰í„°ë¦¬ë¡œ ê°€ì ¸ì˜¤ê¸°ê°€ ì•½ê°„ ì–´ë µìŠµë‹ˆë‹¤.

ëª¨ë¸ì„ ì €ì¥í•œ ë””ë ‰í„°ë¦¬ë¡œ ì´ë™í•˜ë©´ í•´ë‹¹ í˜•ì‹ì´ ì´ í˜•ì‹ì¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê° ë””ë ‰í† ë¦¬ì—ì„œ ì›í•˜ëŠ” íŒŒì¼ì€ `pytorch_lora_weights.safetensors` íŒŒì¼ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ íŒŒì¼ì„ `ComfyUI`ë¡œ ê°€ì ¸ì˜¤ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ê°„ì†Œí™”í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

- `create_symlinks_lora.sh`

    ```bash
    #!/bin/bash
    
    # Source directory where the models are stored
    SOURCE_DIR="/weka2/home-yeo/simpletuner_models/sd3_large/full_finetune/fantasy_art_L_01/datasets/models"
    
    # Target directory for symlinks
    TARGET_DIR="/weka2/home-yeo/ComfyUI/models/loras/sd35_large/fantasy_art"
    
    # Ensure target directory exists or create it
    mkdir -p "${TARGET_DIR}"
    
    # Iterate over each checkpoint directory
    for CHECKPOINT_DIR in ${SOURCE_DIR}/checkpoint-*; do
        # Check if it's indeed a directory
        if [ -d "${CHECKPOINT_DIR}" ]; then
            # Extract the checkpoint number from the directory name
            CHECKPOINT_NAME=$(basename ${CHECKPOINT_DIR})
            
            # Define the source file path
            SOURCE_FILE="${CHECKPOINT_DIR}/pytorch_lora_weights.safetensors"
            
            # Define the symlink name with 'lora' added before 'safetensors'
            LINK_NAME="${TARGET_DIR}/${CHECKPOINT_NAME}_lora.safetensors"
            
            # Check if the source file exists
            if [ -f "${SOURCE_FILE}" ]; then
                # Create a symlink in the target directory
                echo "Creating symlink from ${SOURCE_FILE} to ${LINK_NAME}"
                ln -s "${SOURCE_FILE}" "${LINK_NAME}"
                echo "Symlink created for ${CHECKPOINT_NAME}"
            else
                echo "File not found: ${SOURCE_FILE}"
            fi
        else
            echo "Not a directory: ${CHECKPOINT_DIR}"
        fi
    done
    
    echo "Symlinking complete."
    ```


ìœ„ì˜ ì‰˜ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆ˜í–‰í•  ì‘ì—…ì€ `SimpleTuner`ì—ì„œ `SOURCE_DIR`ì„ ë°˜ë³µí•œ ë‹¤ìŒ ***ë§Œ*** `pytorch_lora_weights.safetensors` íŒŒì¼ì„ `TARGET_DIR`ì— ì‹¬ë³¼ë¦­ ë§í¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ íŒŒì¼ì€ `ComfyUI ë‚´ë¶€ ë””ë ‰í† ë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤. /ëª¨ë¸/ë¡œë¼ìŠ¤`. íŒŒì¼ì„ ì¶”ì í•˜ê¸° ìœ„í•´ íŒŒì¼ ì´ë¦„ ì•ˆì— í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ ë²ˆí˜¸ê°€ í¬í•¨ë˜ë„ë¡ ì´ë¦„ë„ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.

### Determining the best checkpoint


ì œê°€ ì‚¬ìš©í•˜ê³  ìˆëŠ” ê¸°ë³¸ì ì¸ 'SD3.5 Large' ì›Œí¬í”Œë¡œëŠ” ì´ê²ƒì´ì—ˆìŠµë‹ˆë‹¤.

ê°€ì¥ ì¢‹ì€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ê²°ì •í•˜ëŠ” ë°©ë²•ì€ íŠ¹ì • í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ xì¶•ì— ì²´í¬í¬ì¸íŠ¸ ë²ˆí˜¸ë¥¼ í‘œì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ì €ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ì¼ ìŠ¤íŠ¸ë¦½ì„ ì–»ìŠµë‹ˆë‹¤.

íŒíƒ€ì§€ ì•„íŠ¸ 'LoRA'

Prompt

```bash
a three fourth perspective waist up portrait view of a young woman with messy long blonde hair and light purple eyes, looking at viewer with a closed mouth smile, wearing tight black dress, a faded pink simple background during golden hour
```



ì´ë¥¼ ìœ„í•´ `ComfyUI` ì›Œí¬í”Œë¡œì˜ `api` ë²„ì „ì— ë¡œë“œë˜ëŠ” ì‚¬ìš©ì ì •ì˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì €ì¥(API í˜•ì‹) ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ëª¨ë“  ì›Œí¬í”Œë¡œìš°ë¥¼ 'API' í˜•ì‹ìœ¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·€í•˜ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì´ë¯¸ ìœ„ ë²„ì „ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤. 'ComfyUI' API ì‚¬ìš©ì— ëŒ€í•œ ë” ì‹¬ì¸µì ì¸ ë¹„ë””ì˜¤ ê°€ì´ë“œë¥¼ ì›í•˜ì‹œë©´ ì œê°€ ì‘ë…„ì— [ì—¬ê¸°](https://youtu.be/WwsJ_QIgsG8)ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.


`ComfyUI`ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•œ í›„ ì•„ë˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”. ë˜í•œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” ë™ì¼í•œ ìœ„ì¹˜ì— `.env` íŒŒì¼ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

- `API script`

  This is my custom `python` script:

    ```
    import os
    import json
    import random
    from urllib import request
    import datetime
    from PIL import Image, ImageDraw, ImageFont
    import time
    import re
    import urllib.error
    
    from dotenv import load_dotenv
    load_dotenv()
    
    # Configuration
    api_workflow_dir = os.getenv("API_WORKFLOW_DIR")
    lora_dir = os.getenv("LORA_DIR")
    
    api_workflow_file = os.getenv("API_WORKFLOW_FILE")
    api_endpoint = os.getenv("API_ENDPOINT")
    image_output_dir = os.getenv("IMAGE_OUTPUT_DIR")
    font_ttf_path = os.getenv("FONT_TTF_PATH")
    
    comfyui_output_dir = os.getenv("COMFYUI_OUTPUT_DIR")
    
    api_endpoint = f"http://{api_endpoint}/prompt"
    
    workflow_file_path = os.path.join(api_workflow_dir, api_workflow_file)
    workflow = json.load(open(workflow_file_path))
    
    current_datetime = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    relative_output_path = current_datetime
    
    directory_creation_timeout = 3000  # Timeout for directory creation in seconds
    image_generation_timeout = 30000  # Timeout for image generation in seconds
    
    def get_checkpoint_number(filename):
        match = re.search(r'checkpoint-(\d+)', filename)
        if match:
            return int(match.group(1))
        match = re.search(r'/checkpoint-(\d+)/', filename)
        if match:
            return int(match.group(1))
        return None
    
    def get_most_recent_output_folder(base_dir):
        folders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]
        if not folders:
            return None
        return max(folders, key=lambda f: os.path.getctime(os.path.join(base_dir, f)))
    
    def process_loras(lora_dir, workflow):
        print(f"Scanning directory: {lora_dir}")
        
        # Extract the last two directories from LORA_DIR
        lora_path_parts = lora_dir.split('/')
        dynamic_lora_path = '/'.join(lora_path_parts[-2:])
        
        all_items = os.listdir(lora_dir)
        
        lora_items = [f for f in all_items if f.endswith('_lora.safetensors')]
        
        lora_items.sort(key=lambda x: int(x.split('-')[1].split('_')[0]))
        
        print(f"Found items: {lora_items}")
        
        for item in lora_items:
            checkpoint_num = item.split('-')[1].split('_')[0]
            
            print(f"Processing: {item}")
    
            # Update the LoRA loader node
            lora_loader_node = workflow["276"]
            lora_loader_node["inputs"]["lora_name"] = f"{dynamic_lora_path}/{item}"
    
            save_image = workflow["314"]
            filename_prefix = f"checkpoint-{checkpoint_num}"
            save_image["inputs"]["output_path"] = relative_output_path
            save_image["inputs"]["filename_prefix"] = filename_prefix
    
            success = queue_prompt(workflow)
            if not success:
                print(f"Failed to queue prompt for checkpoint {checkpoint_num}")
            else:
                print(f"Successfully queued prompt for checkpoint {checkpoint_num}")
    
        if not lora_items:
            print("No LoRA files found in the directory.")
        
        return len(lora_items)
    
    def create_image_strip(lora_dir, image_folder, output_filename):
        lora_files = [f for f in os.listdir(lora_dir) if f.endswith('_lora.safetensors')]
        lora_files.sort(key=get_checkpoint_number)
        checkpoints = [get_checkpoint_number(f) for f in lora_files if get_checkpoint_number(f) is not None]
    
        images = []
        for checkpoint in checkpoints:
            filename = f"checkpoint-{checkpoint}_0001.png"
            filepath = os.path.join(image_folder, filename)
            if os.path.exists(filepath):
                try:
                    img = Image.open(filepath)
                    images.append(img)
                except IOError as e:
                    print(f"Cannot open image: {filepath}")
                    print(f"Error: {e}")
    
        if not images:
            print("No valid images found.")
            return
    
        img_width, img_height = images[0].size
        strip_width = img_width * len(images)
        label_height = 50  # Space for labels
        strip_height = img_height + label_height
    
        strip_image = Image.new('RGB', (strip_width, strip_height), 'white')
        draw = ImageDraw.Draw(strip_image)
        font = ImageFont.truetype(font_ttf_path, 20)
    
        for i, (img, checkpoint) in enumerate(zip(images, checkpoints)):
            strip_image.paste(img, (i * img_width, label_height))
            
            label = f"checkpoint-{checkpoint}"
            label_width = draw.textlength(label, font=font)
            label_x = i * img_width + (img_width - label_width) // 2
            draw.text((label_x, 10), label, fill="black", font=font)
    
        strip_image.save(output_filename)
        print(f"Image strip saved to: {output_filename}")
    
    def queue_prompt(workflow):
        p = {"prompt": workflow}
        data = json.dumps(p).encode('utf-8')
        req = request.Request(api_endpoint, data=data, headers={'Content-Type': 'application/json'})
        try:
            with request.urlopen(req) as response:
                print(f"API request successful. Status code: {response.getcode()}")
                return True
        except urllib.error.URLError as e:
            if hasattr(e, 'reason'):
                print(f"Failed to reach the server. Reason: {e.reason}")
            elif hasattr(e, 'code'):
                print(f"The server couldn't fulfill the request. Error code: {e.code}")
            print(f"API endpoint: {api_endpoint}")
        except Exception as e:
            print(f"An error occurred: {str(e)}")
        return False
    
    def wait_for_directory_creation(directory, timeout):
        print(f"Waiting for directory {directory} to be created...")
        start_time = time.time()
        while time.time() - start_time < timeout:
            if os.path.exists(directory):
                print(f"Directory {directory} found.")
                return True
            time.sleep(5)  # Check every 5 seconds
        print(f"Timeout waiting for directory {directory} to be created.")
        return False
    
    def wait_for_images(image_folder, expected_count, timeout):
        print("Waiting for images to be generated...")
        start_time = time.time()
        while time.time() - start_time < timeout:
            if os.path.exists(image_folder):
                image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]
                if len(image_files) >= expected_count:
                    print(f"Found all {expected_count} images.")
                    return True
            time.sleep(5)  # Check every 5 seconds
        print("Timeout waiting for images to be generated.")
        return False
    
    if __name__ == "__main__":
        print(f"LoRA directory: {lora_dir}")
    
        # Generate images
        expected_image_count = process_loras(lora_dir, workflow)
    
        absolute_output_path = os.path.join(comfyui_output_dir, current_datetime)
        print(f"Absolute output path: {absolute_output_path}")
    
        # Create the image strip
        if wait_for_directory_creation(absolute_output_path, directory_creation_timeout):
            print(f"Expected image count: {expected_image_count}")
            if wait_for_images(absolute_output_path, expected_image_count, image_generation_timeout):
                output_strip_filename = os.path.join(absolute_output_path, "output_image_strip.png")
                create_image_strip(lora_dir, absolute_output_path, output_strip_filename)
            else:
                print("Failed to generate all images in time.")
        else:
            print("Output directory was not created.")
    ```

- sample `.env` file

    ```bash
    API_WORKFLOW_DIR=/weka2/home-yeo/workflows
    COMFYUI_OUTPUT_DIR = /weka2/home-yeo/ComfyUI/output/
    LORA_DIR=/admin/home-yeo/workspace/ComfyUI/models/loras/sd35_large/fantasy_art_01
    API_WORKFLOW_FILE=sd35_fantasy_art_02_api.json
    API_ENDPOINT=127.0.0.1:8188
    FONT_TTF_PATH=/weka2/home-yeo/fonts/arial.ttf
    BOLD_FONT_TTF_PATH=/weka2/home-yeo/fonts/arialbd.ttf
    ```


Fantasy Art `LoRA`

Prompt

```markdown
a three fourth perspective waist up portrait view of a young woman with messy long blonde hair and light purple eyes, looking at viewer with a closed mouth smile, wearing tight black dress, a faded pink simple background during golden hour
```

ê²°êµ­ '24,000' ë‹¨ê³„ì—ì„œ ê±°ì˜ ë§ˆì§€ë§‰ì— ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„ íƒí•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

ë‚˜ëŠ” ë˜í•œ ê±´ì „ì„± í™•ì¸ì„ ìœ„í•´ ìˆ˜í–‰í•œ ë‹¤ë¥¸ ëª¨ë“  í›ˆë ¨ì— ëŒ€í•´ ë™ì¼í•œ ì‹¤í—˜ì„ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤.

Cinema Photo `LoRA`

Prompt

```markdown
a few hooded figures walking on an empty road in the rain, desolate, high skyscrapers
```

John Singer Sargent `LoRA`

Prompt

```markdown
an abandoned beach with a lighthouse
```

Underexposed Photography `LoRA`

Prompt

```markdown
waist up view of a woman posing on a runway, streetwear in the style of alexander mcqueen
```


ì „ë¬¸ì ì¸ ì´ìœ ë¡œ ì›ë˜ ê·¸ë¦¬ë“œì˜ íŠ¹ì • ë¶€ë¶„ì´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤. ì „ì²´ ê·¸ë¦¬ë“œì—ëŠ” ëª¨ë“  ì²­ì¤‘ì—ê²Œ ì í•©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆëŠ” ì½˜í…ì¸ ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê¸°ìˆ ì ì¸ ì¸¡ë©´ì— ì´ˆì ì„ ë§ì¶”ê¸° ìœ„í•´ ì˜ë¦° ë²„ì „ì´ í‘œì‹œë©ë‹ˆë‹¤.

Pixel Art `LoRA`

Prompt

```bash
a plush chibi mythical creature
```

Ethnic Paint `LoRA`

Prompt

```bash
a skyline view of a futuristic maritime village floating above ground, in the clouds, towering skyscrapers, golden hour, day time lighting
```

## A/B evaluation

### Improving/tuning generations with APG scaling

ìµœê³ ì˜ ë¯¸ì  ê²°ê³¼ë¥¼ ì œê³µí•˜ëŠ” 'LoRA' ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì•˜ìœ¼ë©´ 'APG' ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ ì´ë¥¼ ë”ìš± í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'APG' ìŠ¤ì¼€ì¼ë§ì€ ì ì‘í˜• ì˜ˆì¸¡ ì§€ì¹¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

[APG ë…¼ë¬¸](https://arxiv.org/abs/2410.02416) ì´ˆë¡ì˜ í•µì‹¬ ë¶€ë¶„

```markdown
Our approach, termed adaptive projected guidance (APG), retains the quality-boosting advantages of CFG while enabling the use of higher guidance scales without oversaturation. APG is easy to implement and introduces practically no additional computational overhead to the sampling process.
```

ì´ê²ƒì´ ì´ ìƒ˜í”Œ ì›Œí¬í”Œë¡œì— í¬í•¨ëœ [ComfyUI ë…¸ë“œ](https://github.com/logtd/ComfyUI-APGScaling)ì…ë‹ˆë‹¤. ì„¸ ê°€ì§€ ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í•˜ë‚˜ëŠ” ê¸°ë³¸ ì´ë¯¸ì§€, í•˜ë‚˜ëŠ” ***`APG` ìŠ¤ì¼€ì¼ë§ ì—†ì´ ***`LoRA` ì ìš©, ì„¸ ë²ˆì§¸ ì´ë¯¸ì§€ëŠ” ***` ì‚¬ìš©*** ` LoRA` ì ìš© APG` ìŠ¤ì¼€ì¼ë§.

The parameters for APG are:

```markdown
eta
norm_threshold
use_momentum
momentum
```

ì´ ë…¸ë“œì— ëŒ€í•´ ê·¸ë ‡ê²Œ ë§ì´ ì‹¬ì¸µ ë¶„ì„í•˜ì§€ëŠ” ì•Šì•˜ì§€ë§Œ ì´ë¯¸ì§€ í’ˆì§ˆì´ ì¢‹ë“  ë‚˜ì˜ë“  ë³€ê²½ë©ë‹ˆë‹¤.

### Before and after comparison

Fantasy Art

Prompt

```markdown
a three fourth perspective waist up portrait view of a young woman with messy long blonde hair and light purple eyes, perfect face, looking at viewer with a closed mouth smile, wearing loose black dress, a faded pink simple background during golden hour
```

`Base model`


`LoRA`


`LoRA` + `APG`


Cinema Photo

Prompt

```markdown
a wide view of a figure looking up at a meteor breaking apart
```

`Base model`

`LoRA`


`LoRA` + `APG`


John Singer Sargent

Prompt

```markdown
an abandoned beach with a lighthouse
```

`Base model`

`LoRA`

`LoRA` + `APG`

Underexposed Photography

Prompt

```markdown
waist up view of a woman posing on a runway, streetwear in the style of alexander mcqueen
```

`Base model`

`LoRA`

`LoRA` + `APG`

Pixel Art

Prompt

```markdown
a sci-fi venetian town near the water
```

`Base model`

`LoRA`

`LoRA` + `APG`

Ethnic Paint

Prompt

```markdown
a man in his late 30s to early 40s, rendered in a dark, moody style, The subject is depicted from the shoulders up, facing the viewer directly, He has a full, thick beard and mustache, which is dark and well-groomed, with a few strands of gray, His hair is short and neatly combed, with a few strands falling over his forehead, His eyes are dark and piercing, with a slight hint of sadness or introspection, 
```

`Base model`

`LoRA`

`LoRA` + `APG`


`APG` ëŠ” ê·¸ ë§ì— ì¶©ì‹¤í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì±„ë„ë¥¼ ì¤„ì—¬ì¤ë‹ˆë‹¤. ê°œì¸ì ìœ¼ë¡œ ë‚˜ëŠ” ë°”ëœ ìƒ‰ìƒì„ ì„ í˜¸í•˜ì§€ ì•Šì§€ë§Œ ë°‹ë°‹í•œ "RAW" ê°™ì€ ì´ë¯¸ì§€ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ì¢‹ì€ ë°©ë²•ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


## Other fine-tuning tools/libraries for SD3.5

Hugging Faceì˜ [ì´ ìŠ¤í¬ë¦½íŠ¸ ë° êµ¬ì„±](https://huggingface.co/blog/sd3-5#training-loras-with-sd35-large-with-Quantization)ì„ ì°¸ì¡°í•˜ì„¸ìš”. ì´ëŠ” ì‚¬ìš©í•˜ê¸°ê°€ ë” ê°„ë‹¨í•˜ì§€ë§Œ ê²°ê³¼ëŠ” ì•½ê°„ ë” ë‚˜ì  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Conclusion & Feedback


ì—¬ê¸° ìˆëŠ” ëª¨ë“  ì •ë³´ê°€ ì¶œì‹œì¼ì— SD3.5 Largeë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë° ë„ì›€ì´ ë˜ê¸°ë¥¼ ë°”ëë‹ˆë‹¤. 'DiT' ì•„í‚¤í…ì²˜ëŠ” ì—¬ì „íˆ ìƒëŒ€ì ìœ¼ë¡œ ìƒˆë¡œìš´ ê²ƒì´ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” êµ¬ì„±, ì§ˆê° ë° ì „ì²´ì ì¸ ë¯¸í•™ ì¸¡ë©´ì—ì„œ ìµœê³ ì˜ ì´ë¯¸ì§€ í’ˆì§ˆì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë°©ë²•ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤. ìµœìƒì˜ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í•˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ê²½ìš° í›ˆë ¨ ì¤‘ì— ë³´ë‹¤ ì„¸ë¶€ì ì¸ ë ˆì´ì–´ ì¡°ì‘ì„ ì ê·¹ ê¶Œì¥í•©ë‹ˆë‹¤.



## Two cents from Dango

ë”°ë¼ì„œ SD3.5 ì‹œë¦¬ì¦ˆì˜ ì£¼ìš” ì„¤ê³„ì ì¤‘ í•˜ë‚˜ì¸ Dangoì˜ ì¶”ê°€ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

[Dango's Hugging Face profile](https://huggingface.co/Dango233)

### Diving into SD3.5 Large Architecture


SD 3.5 Largeì˜ í° ê·¸ë¦¼ì„ ì´í•´í•˜ê¸° ìœ„í•´ ë¨¼ì € ì•„í‚¤í…ì²˜ë¥¼ ì¸ì‡„í•´ ë³´ê² ìŠµë‹ˆë‹¤.

ëª¨ë¸ì„ ë¡œì»¬ ë””ë ‰í„°ë¦¬ì— ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²½ìš° `stable-diffusion-3-medium-diffusers`ì™€ ìœ ì‚¬í•œ íŒŒì¼ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.


SD3.5 Largeì˜ ê²½ìš° ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

í‚¤ë¥¼ ë‚˜ì—´í•˜ë ¤ê³  í•˜ë©´ ìƒ¤ë”©ëœ ë””í“¨ì € í˜•ì‹ì˜ ê¸°ë³¸ ëª¨ë¸ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë¯€ë¡œ ì´ë¥¼ ë‹¨ì¼ ëª¨ë¸ë¡œ ë³‘í•©í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤. ì´ ì‹œì ì—ì„œ ë‚˜ëŠ” ëª¨ë¸ì˜ ë¡œì»¬ ë²„ì „ìœ¼ë¡œ ì‘ì—…í•˜ê³  ìˆì—ˆì§€ë§Œ `.cache`ì— ë‹¤ìš´ë¡œë“œí•œ Hugging Face ë²„ì „ê³¼ ë™ì¼í•©ë‹ˆë‹¤.

Example path:

```markdown
/home-kasukanra/.cache/huggingface/hub/models--stabilityai--stable-diffusion-3.5-large/snapshots/1a43aa3b9bb52ead637f9693a228092aa802a5dd/transformer
```

```

import safetensors.torch

shards = [
    "/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/test_convert/transformer/diffusion_pytorch_model-00001-of-00002.safetensors",
    "/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/test_convert/transformer/diffusion_pytorch_model-00002-of-00002.safetensors"
]

# Initialize an empty state dictionary
combined_state_dict = {}

# Load each shard and merge into combined_state_dict
for shard in shards:
    ckpt = safetensors.torch.load_file(shard)
    combined_state_dict.update(ckpt)

# Specify the output path for the combined model
output_path = "/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/merged/combined_model.safetensors"

# Save the combined state dictionary to a single .safetensors file
safetensors.torch.save_file(combined_state_dict, output_path)
print(f"Combined model saved successfully at {output_path}")

```


ì œ ê²½ìš°ì—ëŠ” ë³‘í•©ëœ ëª¨ë¸(`combined_model.safetensors`)ì´ ìˆìœ¼ë©´ ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ì•„í‚¤í…ì²˜ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ì— ì €ì¥í•˜ì„¸ìš”. ìŠ¤í¬ë¦½íŠ¸ëŠ” ë³€í™˜ê¸° ëª¨ë¸ì˜ ì¼ë°˜ì ì¸ ìˆœì°¨ íë¦„ì„ ì¶œë ¥í•©ë‹ˆë‹¤.

```

import safetensors.torch
import re
import json
from collections import defaultdict

def group_keys(keys):
    groups = defaultdict(list)
    for key in keys:
        if 'transformer_blocks' in key:
            block_num = int(re.search(r'transformer_blocks\.(\d+)', key).group(1))
            groups[f'transformer_block_{block_num}'].append(key)
        elif 'embed' in key:
            groups['embedding'].append(key)
        elif 'pos_embed' in key:
            groups['positional_embedding'].append(key)
        elif 'time_text_embed' in key:
            groups['time_text_embedding'].append(key)
        elif 'norm_out' in key:
            groups['output_normalization'].append(key)
        elif 'proj_out' in key:
            groups['output_projection'].append(key)
        else:
            groups['other'].append(key)
    return groups

def order_groups(groups):
    order = [
        'embedding',
        'positional_embedding',
        'time_text_embedding',
    ] + [f'transformer_block_{i}' for i in range(38)] + [
        'output_normalization',
        'output_projection',
        'other'
    ]
    return {k: groups[k] for k in order if k in groups}

def pretty_print_and_save(ckpt, output_file):
    keys_list = list(ckpt.keys())
    grouped_keys = group_keys(keys_list)
    ordered_groups = order_groups(grouped_keys)
    
    output = []
    for group, keys in ordered_groups.items():
        output.append(f"\n{group.upper()}:")
        output.extend(sorted(keys))
    
    pretty_output = '\n'.join(output)
    
    with open(output_file, 'w') as f:
        f.write(pretty_output)
    
    print(f"Grouped keys have been saved to {output_file}")

# Load the checkpoint
checkpoint_path = "/weka2/home-yeo/sd3_diffusers/ckpts/35L_1024_rc6b/merged/combined_model.safetensors"
ckpt = safetensors.torch.load_file(checkpoint_path)

# Pretty-print and save the grouped keys to a file
output_file = "ckpt_keys_grouped_output.txt"
pretty_print_and_save(ckpt, output_file)

```

# ì°¸ì¡°
-----

* [Stable Diffusion 3.5 Large Fine-tuning Tutorial](https://stabilityai.notion.site/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6)
